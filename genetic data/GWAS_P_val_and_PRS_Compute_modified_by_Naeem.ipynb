{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to rerun any part of this code\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_gwas_output_as_tsv_file(inVCF_File, writeFile=None):\n",
    "    root = './tabular_format_gwas_data/'\n",
    "    t0 = time.time()\n",
    "    if writeFile is None:\n",
    "        writeFile = root + inVCF_File.split('/')[-1].split('.')[0] + '.tsv'\n",
    "    \n",
    "    print(\"Processing:\", inVCF_File)\n",
    "    if \".gz\" == inVCF_File[-3:]:\n",
    "        with gzip.open(inVCF_File, 'rb') as f_in:\n",
    "            with open(root+'temp_vcf.vcf', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        inVCF_File = root+'temp_vcf.vcf'\n",
    "    print(\"gzip open done :\", inVCF_File)\n",
    "    \"\"\"\n",
    "    Convert all the VCF into simplified tsv format\n",
    "    Source: https://github.com/everestial/VCF-simplify#table-of-contents\n",
    "    python3 ./VCF-Simplify/VCF-Simplify/VcfSimplify.py SimplifyVCF -toType table -inVCF ./ukb-b-6134.vcf -outFile ./simple_table.txt\n",
    "    \"\"\"\n",
    "    vcf_simplify_path = \"./VCF-Simplify/VCF-Simplify/VcfSimplify.py\"\n",
    "    out_File = root+'temp_table.tsv'\n",
    "    os.system(f\"python3 {vcf_simplify_path} SimplifyVCF -toType table -inVCF {inVCF_File} -outFile {out_File}\")\n",
    "    print(writeFile)\n",
    "        \n",
    "    f_read = open(out_File, 'r')\n",
    "    f_write = open(writeFile, 'w')\n",
    "    cnt = 0\n",
    "    for line in f_read:\n",
    "#         print(line)\n",
    "        newliner = ''\n",
    "        if '\\n' == line[-1]:\n",
    "            newliner = '\\n'\n",
    "            line = line.strip()\n",
    "        if 'CHROM' in line:\n",
    "            line = line.upper() + '\\tPVAL_generated_from_LP'\n",
    "        else:\n",
    "            LP = float(line.split('\\t')[10])\n",
    "#             LP = float(line.split('\\t')[14])\n",
    "            #find out LP column from the tsv file\n",
    "            \n",
    "            p_val = str(10**(-1 * LP))\n",
    "            line = line + '\\t' + p_val\n",
    "        line = line + newliner\n",
    "    #     print(line)\n",
    "        f_write.write(line)\n",
    "        cnt += 1\n",
    "    #     if cnt == 20: break\n",
    "\n",
    "    f_read.close()\n",
    "    f_write.close()\n",
    "    print(f'Total SNPs: {cnt} | Total exec time: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('GWAS_datasets_to_consider.xlsx', engine='openpyxl')[['GWAS_ID', 'Trait', 'Category']]\n",
    "ALL_GWAS_IDS = list(df['GWAS_ID'].to_numpy().tolist()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./tabular_format_gwas_data/ieu-b-5067.vcf.gz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tabular_format_gwas_data/ieu-b-5067.vcf.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ukb-b-4171\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ukb-b-4000\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ukb-d-20544_1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-4171.vcf.gz', writeFile=None)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgenerate_gwas_output_as_tsv_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minVCF_File\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tabular_format_gwas_data/ieu-b-5067.vcf.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriteFile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-18162.vcf.gz', writeFile=None)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-14624.vcf.gz', writeFile=None)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# ukb-b-6358\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# next update in the 'GWAS_datasets_to_consider.xlsx'\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [2], line 9\u001b[0m, in \u001b[0;36mgenerate_gwas_output_as_tsv_file\u001b[0;34m(inVCF_File, writeFile)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inVCF_File)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m inVCF_File[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minVCF_File\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(root\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_vcf.vcf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[1;32m     11\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(f_in, f_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/ad_venv_2/lib/python3.10/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m~/anaconda3/envs/ad_venv_2/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tabular_format_gwas_data/ieu-b-5067.vcf.gz'"
     ]
    }
   ],
   "source": [
    "# ukb-b-4171\n",
    "# ukb-b-4000\n",
    "# ukb-d-20544_1\n",
    "# ukb-d-20544_15\n",
    "# ukb-d-20441\n",
    "# ukb-b-18162\n",
    "# ukb-b-14624\n",
    "# ukb-b-9667\n",
    "\n",
    "\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-4171.vcf.gz', writeFile=None)\n",
    "generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ieu-b-5067.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-18162.vcf.gz', writeFile=None)\n",
    "\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-14624.vcf.gz', writeFile=None)\n",
    "\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-9667.vcf.gz', writeFile=None)\n",
    "\n",
    "\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-d-20544_1.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-d-20544_15.vcf.gz', writeFile=None)\n",
    "\n",
    "\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-12417.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-11495.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-12493.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-19732.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-17006.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-5779.vcf.gz', writeFile=None)\n",
    "# generate_gwas_output_as_tsv_file(inVCF_File='./tabular_format_gwas_data/ukb-b-15541.vcf.gz', writeFile=None)\n",
    "# need to just specify the file path here\n",
    "# ebi-a-GCST005920.vcf.gz\n",
    "# ebi-a-GCST005923.vcf.gz\n",
    "# ukb-b-14699.vcf.gz\n",
    "# ukb-b-323.vcf.gz\n",
    "# ukb-b-6358\n",
    "# next update in the 'GWAS_datasets_to_consider.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "category_map = {\n",
    "    'Continuous' : 'beta', \n",
    "    'Categorical Ordered (assumed continuous)': 'beta',\n",
    "    'Binary': 'or',\n",
    "    'NA (Possibly binary)': 'or'\n",
    "}\n",
    "\n",
    "# always replace Binary traits category with 'or' and continuous category with 'beta'\n",
    "\n",
    "df = pd.read_excel('GWAS_datasets_to_consider.xlsx', engine='openpyxl')[['GWAS_ID', 'Trait', 'Category']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# df = pd.read_excel('GWAS_datasets_to_consider.xlsx', engine='openpyxl')[['GWAS_ID', 'Trait', 'Category']]\n",
    "# all_gwas_vcf_files = glob('/mnt/c/Users/HP/Downloads/Compressed/GWAS/*')\n",
    "# ROOT = '/mnt/c/Users/HP/Downloads/Compressed/GWAS/'\n",
    "# ALL_GWAS_IDS = list(set(df['GWAS_ID'].to_numpy().tolist()) - {'bbj-a-46', 'bbj-a-78', 'ieu-a-1283', 'ukb-a-257','ukb-b-2209','ukb-b-17627','ieub-109','ieu-b-110','ieu-b-111','met-d-Total_C'})\n",
    "# bbj-a-78,ukb-a-257,bbj-a-46\n",
    "# ALL_GWAS_IDS = list(set(df['GWAS_ID'].to_numpy().tolist()) - { 'ieu-a-1283','bbj-a-46', 'bbj-a-78'})\n",
    "# ALL_GWAS_IDS = list(set(df['GWAS_ID'].to_numpy().tolist()) - {  'ieu-b-109','ieu-b-110','ieu-b-111','met-d-Total_C','ukb-b-2209','ukb-b-17627','ukb-a-257','ieu-a-1283','bbj-a-46', 'bbj-a-78','ukb-b-14180','ukb-b-13806','ukb-b-12064','ukb-b-323','ukb-b-14699','ukb-b-14699','ukb-b-5779'})\n",
    "ALL_GWAS_IDS = list(df['GWAS_ID'].to_numpy().tolist()) \n",
    "\n",
    "# print(len(ALL_GWAS_IDS))\n",
    "# print(ALL_GWAS_IDS)\n",
    "\n",
    "base_files = {}\n",
    "# take all except for AD PRS\n",
    "\n",
    "for GWAS_ID in ALL_GWAS_IDS:\n",
    "#     if GWAS_ID=='ieu-b-5067':\n",
    "#         continue\n",
    "#     file = ROOT + GWAS_ID + '.vcf.gz'\n",
    "#     base_files[GWAS_ID] = category_map[df[df['GWAS_ID'] == GWAS_ID].to_numpy()[0, 2]]\n",
    "    base_files[GWAS_ID] = df[df['GWAS_ID'] == GWAS_ID].to_numpy()[0, 2]\n",
    "#     base_files[GWAS_ID]=category_map[base_files[GWAS_ID]]\n",
    "    \n",
    "print(base_files.__len__())\n",
    "#     generate_gwas_output_as_tsv_file(inVCF_File=file, writeFile=None)\n",
    "# print(name_mapping)\n",
    "# print(name_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ukb-b-13806': 'or', 'ukb-d-20405_0': 'or', 'ukb-d-20405_1': 'or', 'ukb-d-20405_2': 'or', 'met-d-Total_C': 'beta', 'ieu-b-109': 'beta', 'ieu-b-110': 'beta', 'ieu-b-111': 'beta', 'ieu-b-25': 'beta', 'ieu-b-38': 'beta', 'ieu-b-39': 'beta', 'ukb-a-257': 'or', 'ukb-b-12064': 'or', 'ukb-b-17627': 'beta', 'ukb-b-18275': 'or', 'ukb-b-19953': 'beta', 'ukb-b-2209': 'beta', 'ukb-b-3957': 'beta', 'ukb-b-4424': 'beta', 'ukb-b-6134': 'beta', 'ukb-b-6324': 'beta', 'ukb-b-7663': 'or', 'ukb-b-770': 'beta', 'ukb-b-8476': 'or', 'ukb-b-323': 'or', 'ukb-b-14699': 'or', 'ukb-b-14180': 'or', 'ukb-b-17243': 'or', 'ukb-b-6358': 'or', 'ukb-b-17006': 'or', 'ukb-b-5779': 'beta', 'ukb-b-15541': 'or', 'ukb-b-19732': 'or', 'ukb-b-20289': 'or', 'ukb-b-14057': 'or', 'ukb-b-12963': 'beta', 'ukb-b-12417': 'beta', 'ukb-b-11495': 'or', 'ukb-b-12493': 'or', 'ukb-b-4282': 'beta', 'ukb-b-4171': 'or', 'ukb-b-4000': 'or', 'ukb-d-20544_1': 'or', 'ukb-d-20544_15': 'or', 'ukb-b-20556': 'or', 'ukb-d-20441': 'or', 'ukb-b-18162': 'or', 'ukb-b-14624': 'or', 'ukb-b-9667': 'or', 'ieu-a-1183': 'or', 'ukb-d-30500_irnt': 'beta', 'ieu-b-7': 'or', 'ieu-b-5067': 'beta'}\n",
      "ukb-b-13806\n",
      "ukb-d-20405_0\n",
      "ukb-d-20405_1\n",
      "ukb-d-20405_2\n",
      "met-d-Total_C\n",
      "ieu-b-109\n",
      "ieu-b-110\n",
      "ieu-b-111\n",
      "ieu-b-25\n",
      "ieu-b-38\n",
      "ieu-b-39\n",
      "ukb-a-257\n",
      "ukb-b-12064\n",
      "ukb-b-17627\n",
      "ukb-b-18275\n",
      "ukb-b-19953\n",
      "ukb-b-2209\n",
      "ukb-b-3957\n",
      "ukb-b-4424\n",
      "ukb-b-6134\n",
      "ukb-b-6324\n",
      "ukb-b-7663\n",
      "ukb-b-770\n",
      "ukb-b-8476\n",
      "ukb-b-323\n",
      "ukb-b-14699\n",
      "ukb-b-14180\n",
      "ukb-b-17243\n",
      "ukb-b-6358\n",
      "ukb-b-17006\n",
      "ukb-b-5779\n",
      "ukb-b-15541\n",
      "ukb-b-19732\n",
      "ukb-b-20289\n",
      "ukb-b-14057\n",
      "ukb-b-12963\n",
      "ukb-b-12417\n",
      "ukb-b-11495\n",
      "ukb-b-12493\n",
      "ukb-b-4282\n",
      "ukb-b-4171\n",
      "ukb-b-4000\n",
      "ukb-d-20544_1\n",
      "ukb-d-20544_15\n",
      "ukb-b-20556\n",
      "ukb-d-20441\n",
      "ukb-b-18162\n",
      "ukb-b-14624\n",
      "ukb-b-9667\n",
      "ieu-a-1183\n",
      "ukb-d-30500_irnt\n",
      "ieu-b-7\n",
      "ieu-b-5067\n"
     ]
    }
   ],
   "source": [
    "# base_files\n",
    "print(base_files)\n",
    "for GWAS_ID in base_files:\n",
    "    print(GWAS_ID)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28105/276037619.py:2: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    }
   ],
   "source": [
    "gender_map = {'Female': 0,'Male': 1}\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "def get_gender_and_age(PTID):\n",
    "    gender = ADNIMERGE[ADNIMERGE['PTID'] == PTID]['PTGENDER'].dropna().unique()[0]\n",
    "    age = ADNIMERGE[ADNIMERGE['PTID'] == PTID]['AGE'].dropna().unique()[0]\n",
    "    return str(gender_map[gender]) + ' ' + str(age)\n",
    "\n",
    "# NUM_TRAINING_SAMPLES = int(830 * .7)\n",
    "# print('NUM_TRAINING_SAMPLES:', NUM_TRAINING_SAMPLES)\n",
    "# f_writable = open('./COVAR_FILE.txt', 'w')\n",
    "# f_TRAINING_SAMPLES = open('./TRAINING_SAMPLES.txt', 'w')\n",
    "# with open('/mnt/c/Users/HP/Documents/GWA_tutorial/2_Population_stratification/covar_mds.txt') as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if 'FID IID' in line:\n",
    "#             line = line[:-1] + ' GENDER AGE\\n'\n",
    "#         else:\n",
    "#             line = line[:-1] + ' ' + get_gender_and_age(PTID=line.split(' ')[1]) + '\\n'\n",
    "#         if i < NUM_TRAINING_SAMPLES:\n",
    "#             f_TRAINING_SAMPLES.write(' '.join(line.split(' ')[:2])+'\\n')\n",
    "#         f_writable.write(line)\n",
    "# #         print(line)\n",
    "# f_writable.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_prsice(GWAS_ID, redo=False):\n",
    "#     TARGET_DATA = '/mnt/c/Users/HP/Documents/GWA_tutorial/2_Population_stratification/ADNI' # old and smaller dataset\n",
    "    #jei data diye ultimate train kora hocche model e\n",
    "    #larger dataset er protita data er jnno protita trait er prscore ber kora hocche\n",
    "    TARGET_DATA = '../larger_dataset/larger_dataset' # new and larger dataset\n",
    "    '''\n",
    "    return_status --> 0: success | 256: Error. Execusion Halted | 2: \n",
    "    '''\n",
    "#     need to alter between these two if needed\n",
    "    if os.path.isfile(f'./PRSice_output/{GWAS_ID}/'):\n",
    "#     if True:\n",
    "        return_status = os.system(f'mkdir ./PRSice_output/{GWAS_ID}/')\n",
    "        print('0:', return_status)\n",
    "        if return_status == 256 and not redo:\n",
    "            return return_status\n",
    "    print('stat',{base_files[GWAS_ID]})\n",
    "#     prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "#         --dir ./PRSice_output \\\n",
    "#         --prsice ./PRSice_linux/PRSice_linux \\\n",
    "#         --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "#         --target {TARGET_DATA} \\\n",
    "#         --thread 2 \\\n",
    "#         --stat {base_files[GWAS_ID].upper()} \\\n",
    "#         --{base_files[GWAS_ID]} \\\n",
    "#         --binary-target F \\\n",
    "#         --quantile 10 \\\n",
    "#         --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "#         --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "#         --score std \\\n",
    "#         --cov ../larger_dataset/COVAR_FILE_bigger_dataset.txt \\\n",
    "#         --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "    if os.path.isfile(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'):\n",
    "        prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "            --dir ./PRSice_output \\\n",
    "            --prsice ./PRSice_linux/PRSice_linux \\\n",
    "            --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "            --target {TARGET_DATA} \\\n",
    "            --thread 2 \\\n",
    "            --stat {base_files[GWAS_ID].upper()} \\\n",
    "            --{base_files[GWAS_ID]} \\\n",
    "            --binary-target F \\\n",
    "            --quantile 10 \\\n",
    "            --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "            --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "            --score std \\\n",
    "            --cov ../larger_dataset/COVAR_FILE_bigger_dataset.txt \\\n",
    "            --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "    else :\n",
    "        prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "            --dir ./PRSice_output \\\n",
    "            --prsice ./PRSice_linux/PRSice_linux \\\n",
    "            --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "            --target {TARGET_DATA} \\\n",
    "            --thread 2 \\\n",
    "            --stat {base_files[GWAS_ID].upper()} \\\n",
    "            --{base_files[GWAS_ID]} \\\n",
    "            --binary-target F \\\n",
    "            --quantile 10 \\\n",
    "            --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "            --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "            --score std \\\n",
    "            --cov ../larger_dataset/COVAR_FILE_bigger_dataset.txt '\n",
    "#         print(prsice_command)\n",
    "#         print('stat',{base_files[GWAS_ID]})\n",
    "#         return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.valid\n",
    "#         print('1:', return_status)\n",
    "#         return_status = os.system('echo \"===== Done =====\"')\n",
    "#         print('2:', return_status) \n",
    "\n",
    "    #             running second time with .valid extraction\n",
    "#         prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "#             --dir ./PRSice_output \\\n",
    "#             --prsice ./PRSice_linux/PRSice_linux \\\n",
    "#             --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "#             --target {TARGET_DATA} \\\n",
    "#             --thread 2 \\\n",
    "#             --stat {base_files[GWAS_ID].upper()} \\\n",
    "#             --{base_files[GWAS_ID]} \\\n",
    "#             --binary-target F \\\n",
    "#             --quantile 10 \\\n",
    "#             --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "#             --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "#             --score std \\\n",
    "#             --cov ../larger_dataset/COVAR_FILE_bigger_dataset.txt \\\n",
    "#             --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "        \n",
    "\n",
    "# #     return_status = os.system(prsice_command + '  --keep ./TRAINING_SAMPLES.txt') # generate the file named {GWAS_ID}.valid\n",
    "# #     return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.validukb-b-12963\n",
    "# #     print('1a:', return_status)\n",
    "# #     return\n",
    "# #     if os.path.isfile(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'): \n",
    "# #         prsice_command += f' --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid' \n",
    "# #         return_status = os.system(prsice_command + '  --keep ./TRAINING_SAMPLES.txt') # perform the regression analysis on the training samples only and choose the best p-value-threshold\n",
    "# #         return_status = os.system(prsice_command) # perform the regression analysis on the training samples only and choose the best p-value-threshold\n",
    "# #         print('1b:', return_status)\n",
    "# #         if return_status != 0:\n",
    "# # #             return_status = os.system(prsice_command) # perform the regression analysis on the training samples only and choose the best p-value-threshold\n",
    "# #             print('1c:', return_status)\n",
    "# #     return_status = os.system(prsice_command + '  --all-score --no-regress') # Do not perform the regression analysis and simply output all PRS.\n",
    "# #     return_status = os.system(prsice_command + '  --all-score') # Do not perform the regression analysis and simply output all PRS.\n",
    "# #     print('2:', return_status)\n",
    "#     # in later steps, select the PRS values for the selected p-value-threshold for all the samples (both training and testing)\n",
    "    print(prsice_command)\n",
    "    print('stat',{base_files[GWAS_ID]})\n",
    "    return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.valid\n",
    "    print('1:', return_status)\n",
    "    return_status = os.system('echo \"===== Done =====\"')\n",
    "    print('2:', return_status) \n",
    "#     os.system(f'mv ./prsice_output/{GWAS_ID}.* ./prsice_output/{GWAS_ID}/')\n",
    "# https://github.com/adiamb/Remove-duplicate-snps-plink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ADNI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prsice_for_adni(GWAS_ID, redo=False):\n",
    "#     TARGET_DATA = '/mnt/c/Users/HP/Documents/GWA_tutorial/2_Population_stratification/ADNI' # old and smaller dataset\n",
    "\n",
    "#     TARGET_DATA = '../larger_dataset/larger_dataset' # new and larger dataset\n",
    "\n",
    "#     TARGET_DATA = \"../../../GWA_tutorial-master/1_QC_GWAS/HapMap_3_r3_12\" \n",
    "    TARGET_DATA = \"../../../GWA_tutorial-master/2_Population_stratification/HapMap_3_r3_13\" \n",
    "    \n",
    "#     this dataset may need to be changed\n",
    "\n",
    "    '''\n",
    "    return_status --> 0: success | 256: Error. Execusion Halted | 2: \n",
    "    '''\n",
    "#     need to alter between these two if needed\n",
    "    if os.path.isfile(f'./PRSice_output/{GWAS_ID}/'):\n",
    "#     if True:\n",
    "        return_status = os.system(f'mkdir ./PRSice_output/{GWAS_ID}/')\n",
    "        print('0:', return_status)\n",
    "        if return_status == 256 and not redo:\n",
    "            return return_status\n",
    "    print('stat',{base_files[GWAS_ID]})\n",
    "#     prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "#         --dir ./PRSice_output \\\n",
    "#         --prsice ./PRSice_linux/PRSice_linux \\\n",
    "#         --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "#         --target {TARGET_DATA} \\\n",
    "#         --thread 2 \\\n",
    "#         --stat {base_files[GWAS_ID].upper()} \\\n",
    "#         --{base_files[GWAS_ID]} \\\n",
    "#         --binary-target F \\\n",
    "#         --quantile 10 \\\n",
    "#         --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "#         --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "#         --score std \\\n",
    "#         --cov ../larger_dataset/COVAR_FILE_bigger_dataset.txt \\\n",
    "#         --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "    if os.path.isfile(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'):\n",
    "        prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "            --dir ./PRSice_output \\\n",
    "            --prsice ./PRSice_linux/PRSice_linux \\\n",
    "            --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "            --target {TARGET_DATA} \\\n",
    "            --thread 2 \\\n",
    "            --stat {base_files[GWAS_ID].upper()} \\\n",
    "            --{base_files[GWAS_ID]} \\\n",
    "            --binary-target F \\\n",
    "            --quantile 10 \\\n",
    "            --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "            --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "            --score std \\\n",
    "            --cov \"../../../GWA_tutorial-master/2_Population_stratification/covar_mds.txt\" \\\n",
    "            --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "    else :\n",
    "        prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "            --dir ./PRSice_output \\\n",
    "            --prsice ./PRSice_linux/PRSice_linux \\\n",
    "            --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "            --target {TARGET_DATA} \\\n",
    "            --thread 2 \\\n",
    "            --stat {base_files[GWAS_ID].upper()} \\\n",
    "            --{base_files[GWAS_ID]} \\\n",
    "            --binary-target F \\\n",
    "            --quantile 10 \\\n",
    "            --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "            --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "            --score std \\\n",
    "            --cov \"../../../GWA_tutorial-master/2_Population_stratification/covar_mds.txt\" '\n",
    "#         print(prsice_command)\n",
    "#         print('stat',{base_files[GWAS_ID]})\n",
    "#         return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.valid\n",
    "#         print('1:', return_status)\n",
    "#         return_status = os.system('echo \"===== Done =====\"')\n",
    "#         print('2:', return_status) \n",
    "\n",
    "    #             running second time with .valid extraction\n",
    "#         prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "#             --dir ./PRSice_output \\\n",
    "#             --prsice ./PRSice_linux/PRSice_linux \\\n",
    "#             --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "#             --target {TARGET_DATA} \\\n",
    "#             --thread 2 \\\n",
    "#             --stat {base_files[GWAS_ID].upper()} \\\n",
    "#             --{base_files[GWAS_ID]} \\\n",
    "#             --binary-target F \\\n",
    "#             --quantile 10 \\\n",
    "#             --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "#             --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "#             --score std \\\n",
    "#             --cov ../larger_dataset/COVAR_FILE_bigger_dataset.txt \\\n",
    "#             --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "        \n",
    "\n",
    "# #     return_status = os.system(prsice_command + '  --keep ./TRAINING_SAMPLES.txt') # generate the file named {GWAS_ID}.valid\n",
    "# #     return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.validukb-b-12963\n",
    "# #     print('1a:', return_status)\n",
    "# #     return\n",
    "# #     if os.path.isfile(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'): \n",
    "# #         prsice_command += f' --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid' \n",
    "# #         return_status = os.system(prsice_command + '  --keep ./TRAINING_SAMPLES.txt') # perform the regression analysis on the training samples only and choose the best p-value-threshold\n",
    "# #         return_status = os.system(prsice_command) # perform the regression analysis on the training samples only and choose the best p-value-threshold\n",
    "# #         print('1b:', return_status)\n",
    "# #         if return_status != 0:\n",
    "# # #             return_status = os.system(prsice_command) # perform the regression analysis on the training samples only and choose the best p-value-threshold\n",
    "# #             print('1c:', return_status)\n",
    "# #     return_status = os.system(prsice_command + '  --all-score --no-regress') # Do not perform the regression analysis and simply output all PRS.\n",
    "# #     return_status = os.system(prsice_command + '  --all-score') # Do not perform the regression analysis and simply output all PRS.\n",
    "# #     print('2:', return_status)\n",
    "#     # in later steps, select the PRS values for the selected p-value-threshold for all the samples (both training and testing)\n",
    "    print(prsice_command)\n",
    "    print('stat',{base_files[GWAS_ID]})\n",
    "    return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.valid\n",
    "    print('1:', return_status)\n",
    "    return_status = os.system('echo \"===== Done =====\"')\n",
    "    print('2:', return_status) \n",
    "#     os.system(f'mv ./prsice_output/{GWAS_ID}.* ./prsice_output/{GWAS_ID}/')\n",
    "# https://github.com/adiamb/Remove-duplicate-snps-plink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For larger dataset generated by us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Change the TARGET_DATA folder and also in the script change the cov source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prsice_for_larger_by_us(GWAS_ID, redo=False):\n",
    "#     TARGET_DATA = '/mnt/c/Users/HP/Documents/GWA_tutorial/2_Population_stratification/ADNI' # old and smaller dataset\n",
    "    #jei data diye ultimate train kora hocche model e\n",
    "    #larger dataset er protita data er jnno protita trait er prscore ber kora hocche\n",
    "    TARGET_DATA = '../larger_dataset_by_naeem2/larger_dataset' # new and larger dataset\n",
    "    '''\n",
    "    return_status --> 0: success | 256: Error. Execusion Halted | 2: \n",
    "    '''\n",
    "#     need to alter between these two if needed\n",
    "    if not os.path.isdir(f'./PRSice_output/{GWAS_ID}/'):\n",
    "#     if True:\n",
    "        print(\"creating directory\")\n",
    "        return_status = os.system(f'mkdir ./PRSice_output/{GWAS_ID}/')\n",
    "        print('0:', return_status)\n",
    "        if return_status == 256 and not redo:\n",
    "            return return_status\n",
    "    print('stat',{base_files[GWAS_ID]})\n",
    "    if os.path.isfile(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'):\n",
    "        prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "            --dir ./PRSice_output \\\n",
    "            --prsice ./PRSice_linux/PRSice_linux \\\n",
    "            --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "            --target {TARGET_DATA} \\\n",
    "            --thread 8 \\\n",
    "            --stat {base_files[GWAS_ID].upper()} \\\n",
    "            --{base_files[GWAS_ID]} \\\n",
    "            --binary-target F \\\n",
    "            --quantile 10 \\\n",
    "            --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "            --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "            --score std \\\n",
    "            --cov ../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt \\\n",
    "            --extract ./PRSice_output/{GWAS_ID}/{GWAS_ID}.valid'\n",
    "    else :\n",
    "        prsice_command = f'Rscript ./PRSice_linux/PRSice.R \\\n",
    "            --dir ./PRSice_output \\\n",
    "            --prsice ./PRSice_linux/PRSice_linux \\\n",
    "            --base ./tabular_format_gwas_data/{GWAS_ID}.tsv \\\n",
    "            --target {TARGET_DATA} \\\n",
    "            --thread 8 \\\n",
    "            --stat {base_files[GWAS_ID].upper()} \\\n",
    "            --{base_files[GWAS_ID]} \\\n",
    "            --binary-target F \\\n",
    "            --quantile 10 \\\n",
    "            --out ./PRSice_output/{GWAS_ID}/{GWAS_ID} \\\n",
    "            --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat {GWAS_ID.upper()}:ES --pvalue PVAL_generated_from_LP \\\n",
    "            --score std \\\n",
    "            --cov ../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt '\n",
    "\n",
    "#     # in later steps, select the PRS values for the selected p-value-threshold for all the samples (both training and testing)\n",
    "    print(prsice_command)\n",
    "    print('stat',{base_files[GWAS_ID]})\n",
    "    return_status = os.system(prsice_command) # generate the file named {GWAS_ID}.valid\n",
    "    print('1:', return_status)\n",
    "    return_status = os.system('echo \"===== Done =====\"')\n",
    "    print('2:', return_status) \n",
    "#     os.system(f'mv ./prsice_output/{GWAS_ID}.* ./prsice_output/{GWAS_ID}/')\n",
    "# https://github.com/adiamb/Remove-duplicate-snps-plink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on run_prsice\n",
    "\n",
    "for each of the data ( individual ) that we have in target dataset, we use our built psrice framework to find out the prs value. so we now know the PRS for target dataset IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat {'or'}\n",
      "Rscript ./PRSice_linux/PRSice.R             --dir ./PRSice_output             --prsice ./PRSice_linux/PRSice_linux             --base ./tabular_format_gwas_data/ukb-b-11495.tsv             --target ../larger_dataset_by_naeem2/larger_dataset             --thread 8             --stat OR             --or             --binary-target F             --quantile 10             --out ./PRSice_output/temp/ukb-b-11495/ukb-b-11495             --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat UKB-B-11495:ES --pvalue PVAL_generated_from_LP             --score std             --cov ../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt             --extract ./PRSice_output/ukb-b-11495/ukb-b-11495.valid\n",
      "stat {'or'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRSice 2.3.3 (2020-08-05) \n",
      "https://github.com/choishingwan/PRSice\n",
      "(C) 2016-2020 Shing Wan (Sam) Choi and Paul F. O'Reilly\n",
      "GNU General Public License v3\n",
      "If you use PRSice in any published work, please cite:\n",
      "Choi SW, O'Reilly PF.\n",
      "PRSice-2: Polygenic Risk Score Software for Biobank-Scale Data.\n",
      "GigaScience 8, no. 7 (July 1, 2019)\n",
      "2023-11-07 13:52:19\n",
      "./PRSice_linux/PRSice_linux \\\n",
      "    --a1 ALT \\\n",
      "    --a2 REF \\\n",
      "    --bar-levels 0.001,0.05,0.1,0.2,0.3,0.4,0.5,1 \\\n",
      "    --base ./tabular_format_gwas_data/ukb-b-11495.tsv \\\n",
      "    --binary-target F \\\n",
      "    --bp POS \\\n",
      "    --chr CHROM \\\n",
      "    --clump-kb 250kb \\\n",
      "    --clump-p 1.000000 \\\n",
      "    --clump-r2 0.100000 \\\n",
      "    --cov ../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt \\\n",
      "    --extract ./PRSice_output/ukb-b-11495/ukb-b-11495.valid \\\n",
      "    --interval 5e-05 \\\n",
      "    --lower 5e-08 \\\n",
      "    --num-auto 22 \\\n",
      "    --or  \\\n",
      "    --out ./PRSice_output/temp/ukb-b-11495/ukb-b-11495 \\\n",
      "    --pvalue PVAL_generated_from_LP \\\n",
      "    --score std \\\n",
      "    --seed 1710595952 \\\n",
      "    --snp ID \\\n",
      "    --stat UKB-B-11495:ES \\\n",
      "    --target ../larger_dataset_by_naeem2/larger_dataset \\\n",
      "    --thread 8 \\\n",
      "    --upper 0.5\n",
      "\n",
      "Initializing Genotype file: \n",
      "../larger_dataset_by_naeem2/larger_dataset (bed) \n",
      "\n",
      "Start processing ukb-b-11495 \n",
      "================================================== \n",
      "\n",
      "SNP extraction/exclusion list contains 5 columns, will \n",
      "assume first column contains the SNP ID \n",
      "\n",
      "Base file: ./tabular_format_gwas_data/ukb-b-11495.tsv \n",
      "Header of file is: \n",
      "\n",
      "CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO:AF\tUKB-B-11495:ES\tUKB-B-11495:SE\tUKB-B-11495:LP\tUKB-B-11495:AF\tUKB-B-11495:SS\tUKB-B-11495:EZ\tUKB-B-11495:SI\tUKB-B-11495:NC\tUKB-B-11495:ID\tPVAL_generated_from_LP \n",
      "\n",
      "Reading 100.00%\n",
      "9606172 variant(s) observed in base file, with: \n",
      "6370208 variant(s) excluded based on user input \n",
      "3235964 total variant(s) included from base file \n",
      "\n",
      "Loading Genotype info from target \n",
      "================================================== \n",
      "\n",
      "1816 people (1000 male(s), 816 female(s)) observed \n",
      "1816 founder(s) included \n",
      "\n",
      "4592254 variant(s) not found in previous data \n",
      "15 variant(s) with mismatch information \n",
      "3235964 variant(s) included \n",
      "\n",
      "There are a total of 1 phenotype to process \n",
      "\n",
      "Start performing clumping \n",
      "\n",
      "Clumping Progress: 100.00%\n",
      "Number of variant(s) after clumping : 255065 \n",
      "\n",
      "Processing the 1 th phenotype \n",
      "\n",
      "Phenotype is a continuous phenotype \n",
      "1816 sample(s) with valid phenotype \n",
      "\n",
      "Processing the covariate file: \n",
      "../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt \n",
      "============================== \n",
      "\n",
      "Include Covariates: \n",
      "Name\tMissing\tNumber of levels \n",
      "PC1\t0\t- \n",
      "PC2\t0\t- \n",
      "PC3\t0\t- \n",
      "PC4\t0\t- \n",
      "PC5\t0\t- \n",
      "PC6\t0\t- \n",
      "PC7\t0\t- \n",
      "PC8\t0\t- \n",
      "PC9\t0\t- \n",
      "PC10\t0\t- \n",
      "PTGENDER\t0\t- \n",
      "AGE\t0\t- \n",
      "\u001b[1;33mWarning: More than 12.885463% of your samples were removed! \n",
      "         You should check if your covariate file is correct \u001b[0m\n",
      "\n",
      "After reading the covariate file, 1582 sample(s) included \n",
      "in the analysis \n",
      "\n",
      "\n",
      "Start Processing\n",
      "Processing 100.00%\n",
      "There are 1 region(s) with p-value between 0.1 and 1e-5 \n",
      "(\u001b[1;31mmay not be significant\u001b[0m). \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin plotting\n",
      "Current Rscript version = 2.3.3\n",
      "Plotting the quantile plot\n",
      "1: 256\n",
      "===== Done =====\n",
      "2: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in `geom_point()`:\n",
      "! Problem while converting geom to grob.\n",
      "ℹ Error occurred in the 1st layer.\n",
      "Caused by error in `dyn.load()`:\n",
      "! unable to load shared object '/mnt/088683cc-b114-4db2-ab3a-2810c4d5fd6b/1705005-1705014/Github Repo/AD_Project_Thesis/genetic data/PRSice_output/lib/farver/libs/farver.so':\n",
      "  /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /mnt/088683cc-b114-4db2-ab3a-2810c4d5fd6b/1705005-1705014/Github Repo/AD_Project_Thesis/genetic data/PRSice_output/lib/farver/libs/farver.so)\n",
      "Backtrace:\n",
      "     ▆\n",
      "  1. ├─global process_plot(...)\n",
      "  2. │ └─global quantile_plot(...)\n",
      "  3. │   └─global call_quantile(...)\n",
      "  4. │     └─global plot.quant(...)\n",
      "  5. │       └─ggplot2::ggsave(...)\n",
      "  6. │         ├─grid::grid.draw(plot)\n",
      "  7. │         └─ggplot2:::grid.draw.ggplot(plot)\n",
      "  8. │           ├─base::print(x)\n",
      "  9. │           └─ggplot2:::print.ggplot(x)\n",
      " 10. │             ├─ggplot2::ggplot_gtable(data)\n",
      " 11. │             └─ggplot2:::ggplot_gtable.ggplot_built(data)\n",
      " 12. │               └─ggplot2:::by_layer(...)\n",
      " 13. │                 ├─rlang::try_fetch(...)\n",
      " 14. │                 │ ├─base::tryCatch(...)\n",
      " 15. │                 │ │ └─base (local) tryCatchList(expr, classes, parentenv, handlers)\n",
      " 16. │                 │ │   └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]])\n",
      " 17. │                 │ │     └─base (local) doTryCatch(return(expr), name, parentenv, handler)\n",
      " 18. │                 │ └─base::withCallingHandlers(...)\n",
      " 19. │                 └─ggplot2 (local) f(l = layers[[i]], d = data[[i]])\n",
      " 20. │                   └─l$draw_geom(d, layout)\n",
      " 21. │                     └─ggplot2 (local) draw_geom(..., self = self)\n",
      " 22. │                       └─self$geom$draw_layer(...)\n",
      " 23. │                         └─ggplot2 (local) draw_layer(..., self = self)\n",
      " 24. │                           └─base::lapply(...)\n",
      " 25. │                             └─ggplot2 (local) FUN(X[[i]], ...)\n",
      " 26. │                               ├─rlang::inject(self$draw_panel(data, panel_params, coord, !!!params))\n",
      " 27. │                               └─self$draw_panel(data, panel_params, coord, na.rm = FALSE)\n",
      " 28. │                                 └─ggplot2 (local) draw_panel(..., self = self)\n",
      " 29. │                                   ├─ggplot2:::ggname(...)\n",
      " 30. │                                   │ └─grid::grobName(grob, prefix)\n",
      " 31. │                                   ├─grid::pointsGrob(...)\n",
      " 32. │                                   │ └─grid::grob(...)\n",
      " 33. │                                   ├─grid::gpar(...)\n",
      " 34. │                                   │ └─grid:::validGP(list(...))\n",
      " 35. │                                   │   └─grid (local) numnotnull(\"fontsize\")\n",
      " 36. │                                   └─scales::alpha(coords$colour, coords$alpha)\n",
      " 37. ├─base::loadNamespace(x)\n",
      " 38. │ └─base::library.dynam(lib, package, package.lib)\n",
      " 39. │   └─base::dyn.load(file, DLLpath = DLLpath, ...)\n",
      " 40. └─base::.handleSimpleError(...)\n",
      " 41.   └─rlang (local) h(simpleError(msg, call))\n",
      " 42.     └─handlers[[1L]](cnd)\n",
      " 43.       └─cli::cli_abort(...)\n",
      " 44.         └─rlang::abort(...)\n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "# for individual run\n",
    "run_prsice_for_larger_by_us(GWAS_ID=\"ukb-b-11495\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------counter  52  ------------------------------------\n",
      "*****GWAS ID IS***** ieu-b-7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_prsice_for_larger_by_us' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*****GWAS ID IS*****\u001b[39m\u001b[38;5;124m'\u001b[39m,GWAS_ID)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#         run_prsice(GWAS_ID=GWAS_ID)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mrun_prsice_for_larger_by_us\u001b[49m(GWAS_ID\u001b[38;5;241m=\u001b[39mGWAS_ID)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         problem -> they do not have corresponding entries in prsice_output folder->create those folders manually\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_prsice_for_larger_by_us' is not defined"
     ]
    }
   ],
   "source": [
    "# here for some files a weird error occurs: \"reporter not initialized\"\n",
    "# soln : create corresponding directory\n",
    "# bbj-a-78\n",
    "# ukb-a-257\n",
    "# bbj-a-46\n",
    "\n",
    "# these are not included due to not being in the EUR ancestry\n",
    "counter = 0\n",
    "for GWAS_ID in base_files:\n",
    "#     if GWAS_ID == 'ukb-b-4282':\n",
    "    counter += 1\n",
    "    if counter > 51:\n",
    "        print('-----------------------counter ',counter,' ------------------------------------')\n",
    "        print('*****GWAS ID IS*****',GWAS_ID)\n",
    "    #         run_prsice(GWAS_ID=GWAS_ID)\n",
    "        run_prsice_for_larger_by_us(GWAS_ID=GWAS_ID)\n",
    "\n",
    "#         problem -> they do not have corresponding entries in prsice_output folder->create those folders manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat {'or'}\n",
      "Rscript ./PRSice_linux/PRSice.R             --dir ./PRSice_output             --prsice ./PRSice_linux/PRSice_linux             --base ./tabular_format_gwas_data/ieu-b-5067.tsv             --target ../larger_dataset_by_naeem2/larger_dataset             --thread 8             --stat OR             --or             --binary-target F             --quantile 10             --out ./PRSice_output/ieu-b-5067/ieu-b-5067             --snp ID --chr CHROM --bp POS --A2 REF --A1 ALT --stat IEU-B-5067:ES --pvalue PVAL_generated_from_LP             --score std             --cov ../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt             --extract ./PRSice_output/ieu-b-5067/ieu-b-5067.valid\n",
      "stat {'or'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRSice 2.3.3 (2020-08-05) \n",
      "https://github.com/choishingwan/PRSice\n",
      "(C) 2016-2020 Shing Wan (Sam) Choi and Paul F. O'Reilly\n",
      "GNU General Public License v3\n",
      "If you use PRSice in any published work, please cite:\n",
      "Choi SW, O'Reilly PF.\n",
      "PRSice-2: Polygenic Risk Score Software for Biobank-Scale Data.\n",
      "GigaScience 8, no. 7 (July 1, 2019)\n",
      "2023-01-14 21:05:38\n",
      "./PRSice_linux/PRSice_linux \\\n",
      "    --a1 ALT \\\n",
      "    --a2 REF \\\n",
      "    --bar-levels 0.001,0.05,0.1,0.2,0.3,0.4,0.5,1 \\\n",
      "    --base ./tabular_format_gwas_data/ieu-b-5067.tsv \\\n",
      "    --binary-target F \\\n",
      "    --bp POS \\\n",
      "    --chr CHROM \\\n",
      "    --clump-kb 250kb \\\n",
      "    --clump-p 1.000000 \\\n",
      "    --clump-r2 0.100000 \\\n",
      "    --cov ../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt \\\n",
      "    --extract ./PRSice_output/ieu-b-5067/ieu-b-5067.valid \\\n",
      "    --interval 5e-05 \\\n",
      "    --lower 5e-08 \\\n",
      "    --num-auto 22 \\\n",
      "    --or  \\\n",
      "    --out ./PRSice_output/ieu-b-5067/ieu-b-5067 \\\n",
      "    --pvalue PVAL_generated_from_LP \\\n",
      "    --score std \\\n",
      "    --seed 3773676530 \\\n",
      "    --snp ID \\\n",
      "    --stat IEU-B-5067:ES \\\n",
      "    --target ../larger_dataset_by_naeem2/larger_dataset \\\n",
      "    --thread 8 \\\n",
      "    --upper 0.5\n",
      "\n",
      "Initializing Genotype file: \n",
      "../larger_dataset_by_naeem2/larger_dataset (bed) \n",
      "\n",
      "Start processing ieu-b-5067 \n",
      "================================================== \n",
      "\n",
      "SNP extraction/exclusion list contains 5 columns, will \n",
      "assume first column contains the SNP ID \n",
      "\n",
      "Base file: ./tabular_format_gwas_data/ieu-b-5067.tsv \n",
      "Header of file is: \n",
      "\n",
      "CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO:AF\tIEU-B-5067:ES\tIEU-B-5067:SE\tIEU-B-5067:LP\tIEU-B-5067:AF\tIEU-B-5067:SS\tIEU-B-5067:EZ\tIEU-B-5067:SI\tIEU-B-5067:NC\tIEU-B-5067:ID\tPVAL_generated_from_LP \n",
      "\n",
      "Reading 100.00%\n",
      "12321875 variant(s) observed in base file, with: \n",
      "9299261 variant(s) excluded based on user input \n",
      "3022614 total variant(s) included from base file \n",
      "\n",
      "Loading Genotype info from target \n",
      "================================================== \n",
      "\n",
      "1816 people (1000 male(s), 816 female(s)) observed \n",
      "1816 founder(s) included \n",
      "\n",
      "4805605 variant(s) not found in previous data \n",
      "14 variant(s) with mismatch information \n",
      "3022614 variant(s) included \n",
      "\n",
      "There are a total of 1 phenotype to process \n",
      "\n",
      "Start performing clumping \n",
      "\n",
      "Clumping Progress: 100.00%\n",
      "Number of variant(s) after clumping : 246152 \n",
      "\n",
      "Processing the 1 th phenotype \n",
      "\n",
      "Phenotype is a continuous phenotype \n",
      "1816 sample(s) with valid phenotype \n",
      "\n",
      "Processing the covariate file: \n",
      "../larger_dataset_by_naeem2/COVAR_FILE_bigger_dataset.txt \n",
      "============================== \n",
      "\n",
      "Include Covariates: \n",
      "Name\tMissing\tNumber of levels \n",
      "PC1\t0\t- \n",
      "PC2\t0\t- \n",
      "PC3\t0\t- \n",
      "PC4\t0\t- \n",
      "PC5\t0\t- \n",
      "PC6\t0\t- \n",
      "PC7\t0\t- \n",
      "PC8\t0\t- \n",
      "PC9\t0\t- \n",
      "PC10\t0\t- \n",
      "PTGENDER\t0\t- \n",
      "AGE\t0\t- \n",
      "\u001b[1;33mWarning: More than 12.885463% of your samples were removed! \n",
      "         You should check if your covariate file is correct \u001b[0m\n",
      "\n",
      "After reading the covariate file, 1582 sample(s) included \n",
      "in the analysis \n",
      "\n",
      "\n",
      "Start Processing\n",
      "Processing 100.00%\n",
      "There are 1 region(s) with p-value less than 1e-5. Please \n",
      "note that these results are inflated due to the overfitting \n",
      "inherent in finding the best-fit PRS (but it's still best \n",
      "to find the best-fit PRS!). \n",
      "You can use the --perm option (see manual) to calculate an \n",
      "empirical P-value. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin plotting\n",
      "Current Rscript version = 2.3.3\n",
      "Plotting the quantile plot\n",
      "Plotting Bar Plot\n",
      "Plotting the high resolution plot\n",
      "1: 0\n",
      "===== Done =====\n",
      "2: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run_prsice_for_larger_by_us(GWAS_ID='ieu-b-5067')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_files\n",
    "GWAS_ID = 'ukb-b-14057'\n",
    "\n",
    "#====\n",
    "\n",
    "def get_prs_values(GWAS_ID):\n",
    "    prsice_output = f'./PRSice_output/{GWAS_ID}/'\n",
    "\n",
    "    if False:\n",
    "        with open(prsice_output+f'{GWAS_ID}.all_score', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = ['\\t'.join(x.split()) for x in lines]\n",
    "            lines = '\\n'.join(lines)\n",
    "\n",
    "        with open(prsice_output+f'{GWAS_ID}.all_score.tsv', 'w') as f:\n",
    "            f.write(lines)\n",
    "\n",
    "        # best_p_val_threshold = '0.00025005'\n",
    "        best_p_val_threshold = str(open(prsice_output+f'{GWAS_ID}.summary').readlines()[1].split('\\t')[2]) \n",
    "    #     print(best_p_val_threshold) \n",
    "    prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy() \n",
    "    return prs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17813/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1816"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prs_values(GWAS_ID=GWAS_ID).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base files {'ukb-b-13806': 'or', 'ukb-d-20405_0': 'or', 'ukb-d-20405_1': 'or', 'ukb-d-20405_2': 'or', 'met-d-Total_C': 'beta', 'ieu-b-109': 'beta', 'ieu-b-110': 'beta', 'ieu-b-111': 'beta', 'ieu-b-25': 'beta', 'ieu-b-38': 'beta', 'ieu-b-39': 'beta', 'ukb-a-257': 'or', 'ukb-b-12064': 'or', 'ukb-b-17627': 'beta', 'ukb-b-18275': 'or', 'ukb-b-19953': 'beta', 'ukb-b-2209': 'beta', 'ukb-b-3957': 'beta', 'ukb-b-4424': 'beta', 'ukb-b-6134': 'beta', 'ukb-b-6324': 'beta', 'ukb-b-7663': 'or', 'ukb-b-770': 'beta', 'ukb-b-8476': 'or', 'ukb-b-323': 'or', 'ukb-b-14699': 'or', 'ukb-b-14180': 'or', 'ukb-b-17243': 'or', 'ukb-b-6358': 'or', 'ukb-b-17006': 'or', 'ukb-b-5779': 'beta', 'ukb-b-15541': 'or', 'ukb-b-19732': 'or', 'ukb-b-20289': 'or', 'ukb-b-14057': 'or', 'ukb-b-12963': 'beta', 'ukb-b-12417': 'beta', 'ukb-b-11495': 'or', 'ukb-b-12493': 'or', 'ukb-b-4282': 'beta', 'ukb-b-4171': 'or', 'ukb-b-4000': 'or', 'ukb-d-20544_1': 'or', 'ukb-d-20544_15': 'or', 'ukb-b-20556': 'or', 'ukb-d-20441': 'or', 'ukb-b-18162': 'or', 'ukb-b-14624': 'or', 'ukb-b-9667': 'or', 'ieu-a-1183': 'or', 'ukb-d-30500_irnt': 'beta', 'ieu-b-7': 'or', 'ieu-b-5067': 'beta'}\n",
      "new base files ['ukb-b-13806', 'ukb-d-20405_0', 'ukb-d-20405_1', 'ukb-d-20405_2', 'met-d-Total_C', 'ieu-b-109', 'ieu-b-110', 'ieu-b-111', 'ieu-b-25', 'ieu-b-38', 'ieu-b-39', 'ukb-a-257', 'ukb-b-12064', 'ukb-b-17627', 'ukb-b-18275', 'ukb-b-19953', 'ukb-b-2209', 'ukb-b-3957', 'ukb-b-4424', 'ukb-b-6134', 'ukb-b-6324', 'ukb-b-7663', 'ukb-b-770', 'ukb-b-8476', 'ukb-b-323', 'ukb-b-14699', 'ukb-b-14180', 'ukb-b-17243', 'ukb-b-6358', 'ukb-b-17006', 'ukb-b-5779', 'ukb-b-15541', 'ukb-b-19732', 'ukb-b-20289', 'ukb-b-14057', 'ukb-b-12963', 'ukb-b-12417', 'ukb-b-11495', 'ukb-b-12493', 'ukb-b-4282', 'ukb-b-4171', 'ukb-b-4000', 'ukb-d-20544_1', 'ukb-d-20544_15', 'ukb-b-20556', 'ukb-d-20441', 'ukb-b-18162', 'ukb-b-14624', 'ukb-b-9667', 'ieu-a-1183', 'ukb-d-30500_irnt', 'ieu-b-7', 'ieu-b-5067']\n",
      "ukb-b-13806\n",
      "ukb-d-20405_0\n",
      "ukb-d-20405_1\n",
      "ukb-d-20405_2\n",
      "met-d-Total_C\n",
      "ieu-b-109\n",
      "ieu-b-110\n",
      "ieu-b-111\n",
      "ieu-b-25\n",
      "ieu-b-38\n",
      "ieu-b-39\n",
      "ukb-a-257\n",
      "ukb-b-12064\n",
      "ukb-b-17627\n",
      "ukb-b-18275\n",
      "ukb-b-19953\n",
      "ukb-b-2209\n",
      "ukb-b-3957\n",
      "ukb-b-4424\n",
      "ukb-b-6134\n",
      "ukb-b-6324\n",
      "ukb-b-7663\n",
      "ukb-b-770\n",
      "ukb-b-8476\n",
      "ukb-b-323\n",
      "ukb-b-14699\n",
      "ukb-b-14180\n",
      "ukb-b-17243\n",
      "ukb-b-6358\n",
      "ukb-b-17006\n",
      "ukb-b-5779\n",
      "ukb-b-15541\n",
      "ukb-b-19732\n",
      "ukb-b-20289\n",
      "ukb-b-14057\n",
      "ukb-b-12963\n",
      "ukb-b-12417\n",
      "ukb-b-11495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukb-b-12493\n",
      "ukb-b-4282\n",
      "ukb-b-4171\n",
      "ukb-b-4000\n",
      "ukb-d-20544_1\n",
      "ukb-d-20544_15\n",
      "ukb-b-20556\n",
      "ukb-d-20441\n",
      "ukb-b-18162\n",
      "ukb-b-14624\n",
      "ukb-b-9667\n",
      "ieu-a-1183\n",
      "ukb-d-30500_irnt\n",
      "ieu-b-7\n",
      "ieu-b-5067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n",
      "/tmp/ipykernel_25522/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import json\n",
    "name_mapping={}\n",
    "# GWAS_ID\n",
    "# ukb-b-13806\n",
    "# ukb-b-12064\n",
    "# ukb-b-323\n",
    "# ukb-b-14699\n",
    "# ukb-b-5779\n",
    "# base_files=base_files-\n",
    "print('base files',base_files)\n",
    "newBaseFiles=[]\n",
    "for GWAS_ID in base_files:\n",
    "    prsice_output = f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best'\n",
    "    if os.path.exists(prsice_output):  \n",
    "        newBaseFiles.append(GWAS_ID)\n",
    "print('new base files',newBaseFiles)\n",
    "\n",
    "PRS_feature_matrix = np.zeros([len(newBaseFiles), 1816])\n",
    "# PRS_feature_matrix = np.zeros([len(newBaseFiles), 846])\n",
    "\n",
    "for i, GWAS_ID in enumerate(newBaseFiles):\n",
    "    print(GWAS_ID)\n",
    "    prsice_output = f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best'\n",
    "    if os.path.exists(prsice_output):  \n",
    "        PRS_feature_matrix[i, :] = get_prs_values(GWAS_ID=GWAS_ID)\n",
    "        name_mapping[GWAS_ID] =df[df['GWAS_ID'] == GWAS_ID].to_numpy()[0, 1]\n",
    "    else :\n",
    "        print(\"no path exists\")\n",
    "        \n",
    "# with open(inputFolder+\"traits_map.json\", \"w\") as outfile:\n",
    "with open(\"traits_map_w_ad.json\", \"w\") as outfile:\n",
    "    \n",
    "    json.dump(name_mapping, outfile)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRS_feature_matrix = PRS_feature_matrix.T\n",
    "np.save('PRS_feature_matrix_with_AD_PRS', PRS_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRS_feature_matrix.shape\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only with AD PRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19669/1739813349.py:21: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  prs_array = pd.read_csv(prsice_output+f'{GWAS_ID}.best', ' ')['PRS'].to_numpy()\n"
     ]
    }
   ],
   "source": [
    "# PRS_feature_matrix_only_ad = np.zeros([1, 1816])\n",
    "PRS_feature_matrix_only_ad = get_prs_values(GWAS_ID='ieu-b-5067')\n",
    "PRS_feature_matrix_only_ad = PRS_feature_matrix_only_ad.T\n",
    "np.save('PRS_feature_matrix_only_ad_2', PRS_feature_matrix_only_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 1266 1816\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SAMPLE_SIZE = 550\n",
    "train_samples = list(range(1816))\n",
    "random.shuffle(train_samples)\n",
    "train_samples, test_samples = train_samples[:TRAIN_SAMPLE_SIZE], train_samples[TRAIN_SAMPLE_SIZE:]\n",
    "# print(test_samples)\n",
    "print(len(train_samples), len(test_samples), len(train_samples) + len(test_samples))\n",
    "# PRS_feature_matrix[train_samples].mean(0), PRS_feature_matrix[train_samples].std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.20063684e-03, -1.16745630e-02, -1.62563695e-03, -1.45995396e-02,\n",
       "         1.81226160e-02, -1.41222658e-02,  2.59397513e-02, -1.08676569e-03,\n",
       "        -1.59275442e-02,  1.06056163e-02,  1.00998294e-03, -1.12513684e-02,\n",
       "         1.19402802e-03,  5.46455829e-03, -1.19946661e-02, -1.64919659e-02,\n",
       "         1.28068050e-02, -9.79679559e-03,  1.47792065e-02,  2.43447114e-04,\n",
       "        -1.99630757e-03, -1.09297553e-02, -1.91106013e-02, -7.10920847e-04,\n",
       "        -1.03662225e-02, -1.91184444e-02, -9.33204446e-03, -2.12532227e-02,\n",
       "        -2.91836350e-02, -6.25583773e-03, -1.99435276e-02,  4.13992541e-04,\n",
       "        -1.07788069e-02,  7.99917727e-03, -1.64618723e-02, -1.66578261e-02,\n",
       "        -1.64490084e-02, -5.95392853e-03, -1.83166027e-02, -2.12051609e-02,\n",
       "        -3.29552667e-03, -1.06555431e-02, -1.63715148e-02, -8.72369661e-03,\n",
       "         6.65254670e-03,  6.00976296e-03, -1.49823886e-02,  3.32215118e-07,\n",
       "         7.59193063e-03, -2.53701504e-04]),\n",
       " array([0.95372266, 0.94400142, 0.97165727, 0.90807297, 0.9794307 ,\n",
       "        1.02487039, 0.9965353 , 0.99606001, 0.99052919, 1.00104849,\n",
       "        1.00524695, 0.9288827 , 0.99299107, 0.99095256, 0.93342815,\n",
       "        0.98910239, 1.01753085, 1.00687874, 0.9022691 , 0.99760172,\n",
       "        1.00030945, 0.91534946, 0.9885595 , 0.97777315, 0.99099556,\n",
       "        0.99082316, 0.97022882, 0.97408027, 0.97823395, 0.99108851,\n",
       "        0.8904033 , 0.99865293, 0.90653442, 0.99919717, 0.93600575,\n",
       "        0.9032774 , 0.91601003, 0.91479259, 0.92699031, 0.94898975,\n",
       "        0.94062901, 0.94073395, 0.90553653, 0.91585274, 0.99220891,\n",
       "        0.99843672, 0.9578753 , 0.98522894, 0.99398665, 1.01698892]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRS_feature_matrix[test_samples].mean(0), PRS_feature_matrix[test_samples].std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.80121253e-11, -7.23843658e-11, -4.03398744e-11, -1.24741255e-11,\n",
       "         1.19856782e-11,  8.02313086e-12, -3.06668484e-11,  6.21563858e-11,\n",
       "        -2.72191628e-11,  7.99063703e-12,  7.19383295e-11, -1.15251910e-12,\n",
       "        -3.74664176e-11,  1.85110097e-11, -5.24669764e-11,  1.11084739e-11,\n",
       "         5.59416361e-11,  3.21943862e-11, -5.24559427e-11, -2.76734093e-12,\n",
       "        -1.95044098e-11,  9.72282264e-12,  5.42070353e-12,  6.10582832e-11,\n",
       "         3.02257739e-11,  1.54763251e-11,  3.52919065e-11, -6.37114248e-12,\n",
       "        -8.16188233e-12,  4.53524247e-11, -1.82294982e-11,  3.02863363e-12,\n",
       "         6.98566804e-12,  3.05832070e-11, -1.58479201e-13,  4.16432195e-11,\n",
       "         2.62714803e-11,  1.76151057e-11, -4.53576677e-12]),\n",
       " array([0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463, 0.99972463,\n",
       "        0.99972463, 0.99972463, 0.99972463, 0.99972463]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRS_feature_matrix[:].mean(0), PRS_feature_matrix[:].std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25522/3893603251.py:4: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  usable_samples_ADNI_unsplitted = pd.read_csv(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "GWAS_ID = 'ukb-b-13806'\n",
    "usable_samples_ADNI_unsplitted = pd.read_csv(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n",
    "usable_samples_ADNI = {'_'.join(sample.split('_')[1:]):sample for sample in (usable_samples_ADNI_unsplitted)}\n",
    "# above line may need to be commented out\n",
    "# json.dump(usable_samples_ADNI, open('./usable_samples_ADNI.json', 'w'))\n",
    "# print(type(usable_samples_ADNI), len(usable_samples_ADNI))\n",
    "# usable_samples_ADNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ieu-b-109  met-d-Total_C  ukb-b-13806  ukb-b-17627  ukb-b-3957\tukb-b-770\r\n",
      "ieu-b-110  ukb-a-257\t  ukb-b-14057  ukb-b-18275  ukb-b-4424\tukb-b-8476\r\n",
      "ieu-b-111  ukb-b-11495\t  ukb-b-14180  ukb-b-19732  ukb-b-5779\tukb-d-20405_0\r\n",
      "ieu-b-25   ukb-b-12064\t  ukb-b-14699  ukb-b-19953  ukb-b-6134\tukb-d-20405_1\r\n",
      "ieu-b-38   ukb-b-12417\t  ukb-b-15541  ukb-b-20289  ukb-b-6324\tukb-d-20405_2\r\n",
      "ieu-b-39   ukb-b-12493\t  ukb-b-17006  ukb-b-2209   ukb-b-6358\r\n",
      "lib\t   ukb-b-12963\t  ukb-b-17243  ukb-b-323    ukb-b-7663\r\n"
     ]
    }
   ],
   "source": [
    "!ls PRSice_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1816, 53) 1816\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# PRS_feature_matrix = np.load('./PRS_feature_matrix.npy')\n",
    "PRS_feature_matrix = np.load('PRS_feature_matrix_with_AD_PRS.npy')\n",
    "# usable_samples_ADNI = json.load(open('./usable_samples_ADNI.json'))\n",
    "print(PRS_feature_matrix.shape, usable_samples_ADNI.__len__())\n",
    "\n",
    "# usable_samples_ADNI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prsice result used in our model\n",
    "usable_samples_adni-> from .best of prsice corresponding to larger dataset we get the IIDs ( ie the individuals that we considered for our studies )\n",
    "now , larger data set is actually a part of ADNIMERGE dataset . So, here we have all the considered patients, but some of the data that are in ADNIMERGE are not present in larger dataset. That leads to the check of \"sample in usable_samples_adni\". \n",
    "The reason behind this is related to the generation of \"larger dataset\" generation from \"ADNI\" dataset.\n",
    "Here our main task is to find out those samples that have corresponding .best file entry, ie included in larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25522/631299173.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 4)\n",
      "        RID        PTID        DX  Month\n",
      "1333    619  002_S_0619  Dementia      0\n",
      "6467    619  002_S_0619  Dementia      6\n",
      "6469    619  002_S_0619  Dementia     12\n",
      "6470    619  002_S_0619  Dementia     24\n",
      "1549    729  002_S_0729  Dementia     12\n",
      "...     ...         ...       ...    ...\n",
      "7928   1311  941_S_1311  Dementia     24\n",
      "8004   1363  941_S_1363  Dementia      6\n",
      "14030  6345  941_S_6345  Dementia     12\n",
      "14947  6345  941_S_6345  Dementia     30\n",
      "14672  6854  941_S_6854  Dementia      0\n",
      "\n",
      "[2338 rows x 4 columns]\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "Final Samples Dementia length :  654\n",
      "Final Samples Non-Dementia length :  1144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(654, 1144, 1798)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples with no restriction\n",
    "THRESHOLD_MONTH = 12*0\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "print(Dementia)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Dementia = set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "        if last_dx == 'Dementia' and sample in usable_samples_ADNI:\n",
    "            \n",
    "            \n",
    "            Final_Samples_Dementia = Final_Samples_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "                \n",
    "\n",
    "Final_Samples_NonDementia = set()\n",
    "for sample in Samples_NonDementia:\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and sample in usable_samples_ADNI:\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_NonDementia = Final_Samples_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Dementia = [[x, 1] for x in Final_Samples_Dementia]\n",
    "Final_Samples_NonDementia = [[x, 0] for x in Final_Samples_NonDementia]\n",
    "print(\"Final Samples Dementia length : \", len(Final_Samples_Dementia) )\n",
    "print(\"Final Samples Non-Dementia length : \", len(Final_Samples_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Dementia + Final_Samples_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_nores.json', 'w'))\n",
    "json.dump(Final_Samples_Dementia, open('Final_Samples_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_NonDementia, open('Final_Samples_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Dementia.__len__(), Final_Samples_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28054/2837539842.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 4)\n",
      "        RID        PTID        DX  Month\n",
      "1333    619  002_S_0619  Dementia      0\n",
      "6467    619  002_S_0619  Dementia      6\n",
      "6469    619  002_S_0619  Dementia     12\n",
      "6470    619  002_S_0619  Dementia     24\n",
      "1549    729  002_S_0729  Dementia     12\n",
      "...     ...         ...       ...    ...\n",
      "7928   1311  941_S_1311  Dementia     24\n",
      "8004   1363  941_S_1363  Dementia      6\n",
      "14030  6345  941_S_6345  Dementia     12\n",
      "14947  6345  941_S_6345  Dementia     30\n",
      "14672  6854  941_S_6854  Dementia      0\n",
      "\n",
      "[2338 rows x 4 columns]\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "Final Samples Dementia length :  0\n",
      "Final Samples Non-Dementia length :  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 years samples\n",
    "THRESHOLD_MONTH = 12*2\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "print(Dementia)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Dementia = set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "        if last_dx == 'Dementia' and sample in usable_samples_ADNI:\n",
    "            \n",
    "            \n",
    "            Final_Samples_Dementia = Final_Samples_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "                \n",
    "\n",
    "Final_Samples_NonDementia = set()\n",
    "for sample in Samples_NonDementia:\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and sample in usable_samples_ADNI:\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_NonDementia = Final_Samples_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Dementia = [[x, 1] for x in Final_Samples_Dementia]\n",
    "Final_Samples_NonDementia = [[x, 0] for x in Final_Samples_NonDementia]\n",
    "print(\"Final Samples Dementia length : \", len(Final_Samples_Dementia) )\n",
    "print(\"Final Samples Non-Dementia length : \", len(Final_Samples_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Dementia + Final_Samples_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_2yrs.json', 'w'))\n",
    "json.dump(Final_Samples_Dementia, open('Final_Samples_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_NonDementia, open('Final_Samples_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Dementia.__len__(), Final_Samples_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28054/142797638.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 4)\n",
      "        RID        PTID        DX  Month\n",
      "1333    619  002_S_0619  Dementia      0\n",
      "6467    619  002_S_0619  Dementia      6\n",
      "6469    619  002_S_0619  Dementia     12\n",
      "6470    619  002_S_0619  Dementia     24\n",
      "1549    729  002_S_0729  Dementia     12\n",
      "...     ...         ...       ...    ...\n",
      "7928   1311  941_S_1311  Dementia     24\n",
      "8004   1363  941_S_1363  Dementia      6\n",
      "14030  6345  941_S_6345  Dementia     12\n",
      "14947  6345  941_S_6345  Dementia     30\n",
      "14672  6854  941_S_6854  Dementia      0\n",
      "\n",
      "[2338 rows x 4 columns]\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "Final Samples Dementia length :  654\n",
      "Final Samples Non-Dementia length :  504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(654, 504, 1158)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 years samples\n",
    "THRESHOLD_MONTH = 12*4\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "print(Dementia)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Dementia = set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "        if last_dx == 'Dementia' and sample in usable_samples_ADNI:\n",
    "            \n",
    "            \n",
    "            Final_Samples_Dementia = Final_Samples_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "                \n",
    "\n",
    "Final_Samples_NonDementia = set()\n",
    "for sample in Samples_NonDementia:\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and sample in usable_samples_ADNI:\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_NonDementia = Final_Samples_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Dementia = [[x, 1] for x in Final_Samples_Dementia]\n",
    "Final_Samples_NonDementia = [[x, 0] for x in Final_Samples_NonDementia]\n",
    "print(\"Final Samples Dementia length : \", len(Final_Samples_Dementia) )\n",
    "print(\"Final Samples Non-Dementia length : \", len(Final_Samples_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Dementia + Final_Samples_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_4yrs.json', 'w'))\n",
    "json.dump(Final_Samples_Dementia, open('Final_Samples_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_NonDementia, open('Final_Samples_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Dementia.__len__(), Final_Samples_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28054/1836803714.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 4)\n",
      "        RID        PTID        DX  Month\n",
      "1333    619  002_S_0619  Dementia      0\n",
      "6467    619  002_S_0619  Dementia      6\n",
      "6469    619  002_S_0619  Dementia     12\n",
      "6470    619  002_S_0619  Dementia     24\n",
      "1549    729  002_S_0729  Dementia     12\n",
      "...     ...         ...       ...    ...\n",
      "7928   1311  941_S_1311  Dementia     24\n",
      "8004   1363  941_S_1363  Dementia      6\n",
      "14030  6345  941_S_6345  Dementia     12\n",
      "14947  6345  941_S_6345  Dementia     30\n",
      "14672  6854  941_S_6854  Dementia      0\n",
      "\n",
      "[2338 rows x 4 columns]\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "usable_samples_adni: 1816\n",
      "Final Samples Dementia length :  654\n",
      "Final Samples Non-Dementia length :  320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(654, 320, 974)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 years samples\n",
    "THRESHOLD_MONTH = 12*6\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "print(Dementia)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "print(\"usable_samples_adni:\",usable_samples_ADNI.__len__())\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Dementia = set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "        if last_dx == 'Dementia' and sample in usable_samples_ADNI:\n",
    "#             print(\"here\")\n",
    "            \n",
    "            \n",
    "            Final_Samples_Dementia = Final_Samples_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "               \n",
    "\n",
    "Final_Samples_NonDementia = set()\n",
    "for sample in Samples_NonDementia:\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "#     print(last_month_of_dx)\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and sample in usable_samples_ADNI:\n",
    "#         print(\"here2\")\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_NonDementia = Final_Samples_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Dementia = [[x, 1] for x in Final_Samples_Dementia]\n",
    "Final_Samples_NonDementia = [[x, 0] for x in Final_Samples_NonDementia]\n",
    "print(\"Final Samples Dementia length : \", len(Final_Samples_Dementia) )\n",
    "print(\"Final Samples Non-Dementia length : \", len(Final_Samples_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Dementia + Final_Samples_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_6yrs.json', 'w'))\n",
    "json.dump(Final_Samples_Dementia, open('Final_Samples_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_NonDementia, open('Final_Samples_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Dementia.__len__(), Final_Samples_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18499/2860556536.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 4)\n",
      "        RID        PTID        DX  Month\n",
      "1333    619  002_S_0619  Dementia      0\n",
      "6467    619  002_S_0619  Dementia      6\n",
      "6469    619  002_S_0619  Dementia     12\n",
      "6470    619  002_S_0619  Dementia     24\n",
      "1549    729  002_S_0729  Dementia     12\n",
      "...     ...         ...       ...    ...\n",
      "7928   1311  941_S_1311  Dementia     24\n",
      "8004   1363  941_S_1363  Dementia      6\n",
      "14030  6345  941_S_6345  Dementia     12\n",
      "14947  6345  941_S_6345  Dementia     30\n",
      "14672  6854  941_S_6854  Dementia      0\n",
      "\n",
      "[2338 rows x 4 columns]\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "usable_samples_adni: 1816\n",
      "Final Samples Dementia length :  654\n",
      "Final Samples Non-Dementia length :  169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(654, 169, 823)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 years samples\n",
    "THRESHOLD_MONTH = 12*8\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "print(Dementia)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "print(\"usable_samples_adni:\",usable_samples_ADNI.__len__())\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Dementia = set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "        if last_dx == 'Dementia' and sample in usable_samples_ADNI:\n",
    "#             print(\"here\")\n",
    "            \n",
    "            \n",
    "            Final_Samples_Dementia = Final_Samples_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "               \n",
    "\n",
    "Final_Samples_NonDementia = set()\n",
    "for sample in Samples_NonDementia:\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "#     print(last_month_of_dx)\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and sample in usable_samples_ADNI:\n",
    "#         print(\"here2\")\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_NonDementia = Final_Samples_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Dementia = [[x, 1] for x in Final_Samples_Dementia]\n",
    "Final_Samples_NonDementia = [[x, 0] for x in Final_Samples_NonDementia]\n",
    "print(\"Final Samples Dementia length : \", len(Final_Samples_Dementia) )\n",
    "print(\"Final Samples Non-Dementia length : \", len(Final_Samples_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Dementia + Final_Samples_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_8yrs.json', 'w'))\n",
    "json.dump(Final_Samples_Dementia, open('Final_Samples_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_NonDementia, open('Final_Samples_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Dementia.__len__(), Final_Samples_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28054/3847962530.py:4: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  usable_samples_ADNI = pd.read_csv(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADNI3_036_S_6231': 0,\n",
       " 'ADNI3_006_S_6277': 1,\n",
       " 'ADNI3_129_S_6146': 2,\n",
       " 'ADNI3_033_S_6352': 3,\n",
       " 'ADNI3_027_S_6183': 4,\n",
       " 'ADNI3_005_S_6427': 5,\n",
       " 'ADNI3_127_S_6147': 6,\n",
       " 'ADNI3_114_S_6251': 7,\n",
       " 'ADNI3_129_S_6228': 8,\n",
       " 'ADNI3_114_S_6309': 9,\n",
       " 'ADNI3_135_S_6110': 10,\n",
       " 'ADNI3_020_S_6358': 11,\n",
       " 'ADNI3_135_S_6411': 12,\n",
       " 'ADNI3_024_S_6202': 13,\n",
       " 'ADNI3_018_S_6414': 14,\n",
       " 'ADNI3_002_S_6103': 15,\n",
       " 'ADNI3_177_S_6408': 16,\n",
       " 'ADNI3_014_S_6148': 17,\n",
       " 'ADNI3_036_S_6466': 18,\n",
       " 'ADNI3_036_S_6134': 19,\n",
       " 'ADNI3_007_S_6455': 20,\n",
       " 'ADNI3_037_S_6271': 21,\n",
       " 'ADNI3_116_S_6100': 22,\n",
       " 'ADNI3_027_S_6327': 23,\n",
       " 'ADNI3_099_S_6097': 24,\n",
       " 'ADNI3_127_S_6330': 25,\n",
       " 'ADNI3_127_S_6168': 26,\n",
       " 'ADNI3_018_S_6351': 27,\n",
       " 'ADNI3_009_S_6212': 28,\n",
       " 'ADNI3_168_S_6180': 29,\n",
       " 'ADNI3_116_S_6119': 30,\n",
       " 'ADNI3_023_S_6346': 31,\n",
       " 'ADNI3_168_S_6065': 32,\n",
       " 'ADNI3_035_S_6200': 33,\n",
       " 'ADNI3_023_S_6399': 34,\n",
       " 'ADNI3_037_S_6125': 35,\n",
       " 'ADNI3_116_S_6428': 36,\n",
       " 'ADNI3_041_S_6192': 37,\n",
       " 'ADNI3_941_S_6333': 38,\n",
       " 'ADNI3_006_S_6209': 39,\n",
       " 'ADNI3_033_S_6497': 40,\n",
       " 'ADNI3_127_S_6436': 41,\n",
       " 'ADNI3_141_S_6178': 42,\n",
       " 'ADNI3_002_S_6456': 43,\n",
       " 'ADNI3_168_S_6062': 44,\n",
       " 'ADNI3_033_S_6298': 45,\n",
       " 'ADNI3_002_S_6053': 46,\n",
       " 'ADNI3_135_S_6359': 47,\n",
       " 'ADNI3_141_S_6015': 48,\n",
       " 'ADNI3_027_S_6317': 49,\n",
       " 'ADNI3_002_S_6009': 50,\n",
       " 'ADNI3_098_S_6343': 51,\n",
       " 'ADNI3_024_S_6033': 52,\n",
       " 'ADNI3_941_S_6384': 53,\n",
       " 'ADNI3_141_S_6008': 54,\n",
       " 'ADNI3_023_S_6400': 55,\n",
       " 'ADNI3_027_S_6034': 56,\n",
       " 'ADNI3_002_S_6404': 57,\n",
       " 'ADNI3_037_S_6031': 58,\n",
       " 'ADNI3_941_S_6017': 59,\n",
       " 'ADNI3_130_S_6329': 60,\n",
       " 'ADNI3_941_S_6044': 61,\n",
       " 'ADNI3_020_S_6470': 62,\n",
       " 'ADNI3_941_S_6052': 63,\n",
       " 'ADNI3_082_S_6197': 64,\n",
       " 'ADNI3_130_S_6019': 65,\n",
       " 'ADNI3_023_S_6356': 66,\n",
       " 'ADNI3_002_S_6030': 67,\n",
       " 'ADNI3_094_S_6419': 68,\n",
       " 'ADNI3_130_S_6037': 69,\n",
       " 'ADNI3_007_S_6310': 70,\n",
       " 'ADNI3_941_S_6054': 71,\n",
       " 'ADNI3_041_S_6354': 72,\n",
       " 'ADNI3_141_S_6041': 73,\n",
       " 'ADNI3_135_S_6360': 74,\n",
       " 'ADNI3_067_S_6045': 75,\n",
       " 'ADNI3_011_S_6367': 76,\n",
       " 'ADNI3_099_S_6016': 77,\n",
       " 'ADNI3_035_S_6306': 78,\n",
       " 'ADNI3_024_S_6005': 79,\n",
       " 'ADNI3_168_S_6318': 80,\n",
       " 'ADNI3_130_S_6047': 81,\n",
       " 'ADNI3_082_S_6287': 82,\n",
       " 'ADNI3_941_S_6068': 83,\n",
       " 'ADNI3_037_S_6222': 84,\n",
       " 'ADNI3_022_S_6013': 85,\n",
       " 'ADNI3_941_S_6422': 86,\n",
       " 'ADNI3_114_S_6039': 87,\n",
       " 'ADNI3_006_S_6375': 88,\n",
       " 'ADNI3_002_S_6066': 89,\n",
       " 'ADNI3_094_S_6250': 90,\n",
       " 'ADNI3_305_S_6188': 91,\n",
       " 'ADNI3_941_S_6471': 92,\n",
       " 'ADNI3_003_S_6258': 93,\n",
       " 'ADNI3_041_S_6159': 94,\n",
       " 'ADNI3_100_S_6349': 95,\n",
       " 'ADNI3_141_S_6240': 96,\n",
       " 'ADNI3_131_S_6170': 97,\n",
       " 'ADNI3_018_S_6207': 98,\n",
       " 'ADNI3_003_S_6256': 99,\n",
       " 'ADNI3_301_S_6224': 100,\n",
       " 'ADNI3_023_S_6334': 101,\n",
       " 'ADNI3_007_S_6255': 102,\n",
       " 'ADNI3_003_S_6260': 103,\n",
       " 'ADNI3_100_S_6164': 104,\n",
       " 'ADNI3_070_S_6394': 105,\n",
       " 'ADNI3_014_S_6145': 106,\n",
       " 'ADNI3_099_S_6396': 107,\n",
       " 'ADNI3_109_S_6213': 108,\n",
       " 'ADNI3_129_S_6452': 109,\n",
       " 'ADNI3_009_S_6163': 110,\n",
       " 'ADNI3_116_S_6543': 111,\n",
       " 'ADNI3_003_S_6158': 112,\n",
       " 'ADNI3_130_S_6161': 113,\n",
       " 'ADNI3_006_S_6252': 114,\n",
       " 'ADNI3_135_S_6446': 115,\n",
       " 'ADNI3_014_S_6199': 116,\n",
       " 'ADNI3_130_S_6319': 117,\n",
       " 'ADNI3_129_S_6244': 118,\n",
       " 'ADNI3_032_S_6279': 119,\n",
       " 'ADNI3_006_S_6243': 120,\n",
       " 'ADNI3_301_S_6326': 121,\n",
       " 'ADNI3_114_S_6113': 122,\n",
       " 'ADNI3_007_S_6323': 123,\n",
       " 'ADNI3_153_S_6274': 124,\n",
       " 'ADNI3_094_S_6269': 125,\n",
       " 'ADNI3_037_S_6141': 126,\n",
       " 'ADNI3_037_S_6187': 127,\n",
       " 'ADNI3_168_S_6049': 128,\n",
       " 'ADNI3_177_S_6335': 129,\n",
       " 'ADNI3_033_S_6266': 130,\n",
       " 'ADNI3_129_S_6459': 131,\n",
       " 'ADNI3_130_S_6372': 132,\n",
       " 'ADNI3_141_S_6253': 133,\n",
       " 'ADNI3_006_S_6500': 134,\n",
       " 'ADNI3_130_S_6072': 135,\n",
       " 'ADNI3_082_S_6283': 136,\n",
       " 'ADNI3_141_S_6061': 137,\n",
       " 'ADNI3_007_S_6341': 138,\n",
       " 'ADNI3_007_S_6120': 139,\n",
       " 'ADNI3_094_S_6275': 140,\n",
       " 'ADNI3_941_S_6080': 141,\n",
       " 'ADNI3_016_S_6381': 142,\n",
       " 'ADNI3_941_S_6094': 143,\n",
       " 'ADNI3_941_S_6345': 144,\n",
       " 'ADNI3_003_S_6067': 145,\n",
       " 'ADNI3_023_S_6374': 146,\n",
       " 'ADNI3_012_S_6073': 147,\n",
       " 'ADNI3_037_S_6216': 148,\n",
       " 'ADNI3_135_S_6104': 149,\n",
       " 'ADNI3_168_S_6320': 150,\n",
       " 'ADNI3_067_S_6117': 151,\n",
       " 'ADNI3_041_S_6401': 152,\n",
       " 'ADNI3_014_S_6076': 153,\n",
       " 'ADNI3_011_S_6418': 154,\n",
       " 'ADNI3_036_S_6088': 155,\n",
       " 'ADNI3_003_S_6432': 156,\n",
       " 'ADNI3_037_S_6083': 157,\n",
       " 'ADNI3_116_S_6550': 158,\n",
       " 'ADNI3_130_S_6043': 159,\n",
       " 'ADNI3_070_S_6386': 160,\n",
       " 'ADNI3_127_S_6024': 161,\n",
       " 'ADNI3_027_S_6370': 162,\n",
       " 'ADNI3_023_S_6369': 163,\n",
       " 'ADNI3_130_S_6035': 164,\n",
       " 'ADNI3_168_S_6371': 165,\n",
       " 'ADNI3_099_S_6038': 166,\n",
       " 'ADNI3_168_S_6350': 167,\n",
       " 'ADNI3_301_S_6056': 168,\n",
       " 'ADNI3_099_S_6025': 169,\n",
       " 'ADNI3_135_S_6389': 170,\n",
       " 'ADNI3_130_S_6105': 171,\n",
       " 'ADNI3_127_S_6241': 172,\n",
       " 'ADNI3_022_S_6069': 173,\n",
       " 'ADNI3_009_S_6402': 174,\n",
       " 'ADNI3_941_S_6058': 175,\n",
       " 'ADNI3_035_S_6488': 176,\n",
       " 'ADNI3_141_S_6116': 177,\n",
       " 'ADNI3_116_S_6439': 178,\n",
       " 'ADNI3_116_S_6133': 179,\n",
       " 'ADNI3_037_S_6204': 180,\n",
       " 'ADNI3_114_S_6429': 181,\n",
       " 'ADNI3_041_S_6292': 182,\n",
       " 'ADNI3_013_S_6206': 183,\n",
       " 'ADNI3_153_S_6336': 184,\n",
       " 'ADNI3_014_S_6437': 185,\n",
       " 'ADNI3_100_S_6273': 186,\n",
       " 'ADNI3_027_S_6577': 187,\n",
       " 'ADNI3_941_S_6575': 188,\n",
       " 'ADNI3_036_S_6189': 189,\n",
       " 'ADNI3_135_S_6545': 190,\n",
       " 'ADNI3_036_S_6179': 191,\n",
       " 'ADNI3_135_S_6473': 192,\n",
       " 'ADNI3_029_S_6289': 193,\n",
       " 'ADNI3_014_S_6522': 194,\n",
       " 'ADNI3_019_S_6186': 195,\n",
       " 'ADNI3_014_S_6502': 196,\n",
       " 'ADNI3_070_S_6236': 197,\n",
       " 'ADNI3_027_S_6582': 198,\n",
       " 'ADNI3_168_S_6085': 199,\n",
       " 'ADNI3_019_S_6573': 200,\n",
       " 'ADNI3_003_S_6259': 201,\n",
       " 'ADNI3_305_S_6498': 202,\n",
       " 'ADNI3_003_S_6014': 203,\n",
       " 'ADNI3_100_S_6578': 204,\n",
       " 'ADNI3_021_S_6312': 205,\n",
       " 'ADNI3_135_S_6544': 206,\n",
       " 'ADNI3_014_S_6424': 207,\n",
       " 'ADNI3_305_S_6313': 208,\n",
       " 'ADNI3_127_S_6549': 209,\n",
       " 'ADNI3_131_S_6143': 210,\n",
       " 'ADNI3_127_S_6512': 211,\n",
       " 'ADNI3_127_S_6232': 212,\n",
       " 'ADNI3_033_S_6572': 213,\n",
       " 'ADNI3_041_S_6226': 214,\n",
       " 'ADNI3_003_S_6264': 215,\n",
       " 'ADNI3_032_S_6211': 216,\n",
       " 'ADNI3_023_S_6547': 217,\n",
       " 'ADNI3_130_S_6111': 218,\n",
       " 'ADNI3_141_S_6423': 219,\n",
       " 'ADNI3_020_S_6282': 220,\n",
       " 'ADNI3_067_S_6529': 221,\n",
       " 'ADNI3_011_S_6303': 222,\n",
       " 'ADNI3_020_S_6566': 223,\n",
       " 'ADNI3_099_S_6175': 224,\n",
       " 'ADNI3_037_S_6377': 225,\n",
       " 'ADNI3_002_S_6007': 226,\n",
       " 'ADNI3_003_S_6307': 227,\n",
       " 'ADNI3_037_S_6046': 228,\n",
       " 'ADNI3_003_S_6479': 229,\n",
       " 'ADNI3_305_S_6157': 230,\n",
       " 'ADNI3_007_S_6515': 231,\n",
       " 'ADNI3_941_S_6581': 232,\n",
       " 'ADNI3_127_S_6173': 233,\n",
       " 'ADNI3_094_S_6417': 234,\n",
       " 'ADNI3_024_S_6385': 235,\n",
       " 'ADNI3_020_S_6227': 236,\n",
       " 'ADNI3_168_S_6413': 237,\n",
       " 'ADNI3_135_S_6284': 238,\n",
       " 'ADNI3_024_S_6472': 239,\n",
       " 'ADNI3_020_S_6185': 240,\n",
       " 'ADNI3_123_S_6118': 241,\n",
       " 'ADNI3_941_S_6514': 242,\n",
       " 'ADNI3_019_S_6315': 243,\n",
       " 'ADNI3_141_S_6075': 244,\n",
       " 'ADNI3_012_S_6503': 245,\n",
       " 'ADNI3_100_S_6308': 246,\n",
       " 'ADNI3_135_S_6586': 247,\n",
       " 'ADNI3_005_S_6084': 248,\n",
       " 'ADNI3_027_S_6001': 249,\n",
       " 'ADNI3_177_S_6448': 250,\n",
       " 'ADNI3_053_S_6598': 251,\n",
       " 'ADNI3_027_S_6002': 252,\n",
       " 'ADNI3_168_S_6467': 253,\n",
       " 'ADNI3_006_S_6234': 254,\n",
       " 'ADNI3_003_S_6268': 255,\n",
       " 'ADNI3_168_S_6426': 256,\n",
       " 'ADNI3_067_S_6442': 257,\n",
       " 'ADNI3_127_S_6348': 258,\n",
       " 'ADNI3_020_S_6513': 259,\n",
       " 'ADNI3_067_S_6138': 260,\n",
       " 'ADNI3_135_S_6510': 261,\n",
       " 'ADNI3_168_S_6151': 262,\n",
       " 'ADNI3_011_S_6465': 263,\n",
       " 'ADNI3_019_S_6483': 264,\n",
       " 'ADNI3_941_S_6454': 265,\n",
       " 'ADNI3_094_S_6485': 266,\n",
       " 'ADNI3_168_S_6321': 267,\n",
       " 'ADNI3_067_S_6474': 268,\n",
       " 'ADNI3_007_S_6421': 269,\n",
       " 'ADNI3_114_S_6347': 270,\n",
       " 'ADNI3_141_S_6416': 271,\n",
       " 'ADNI3_129_S_6457': 272,\n",
       " 'ADNI3_027_S_6463': 273,\n",
       " 'ADNI3_094_S_6468': 274,\n",
       " 'ADNI3_177_S_6409': 275,\n",
       " 'ADNI3_032_S_6294': 276,\n",
       " 'ADNI3_341_S_6494': 277,\n",
       " 'ADNI3_036_S_6316': 278,\n",
       " 'ADNI3_035_S_6480': 279,\n",
       " 'ADNI3_006_S_6441': 280,\n",
       " 'ADNI3_127_S_6357': 281,\n",
       " 'ADNI3_027_S_6516': 282,\n",
       " 'ADNI3_941_S_6580': 283,\n",
       " 'ADNI3_941_S_6546': 284,\n",
       " 'ADNI3_127_S_6433': 285,\n",
       " 'ADNI3_100_S_6493': 286,\n",
       " 'ADNI3_019_S_6533': 287,\n",
       " 'ADNI3_168_S_6233': 288,\n",
       " 'ADNI3_126_S_6559': 289,\n",
       " 'ADNI3_168_S_6492': 290,\n",
       " 'ADNI3_131_S_6519': 291,\n",
       " 'ADNI3_099_S_6476': 292,\n",
       " 'ADNI3_020_S_6504': 293,\n",
       " 'ADNI3_098_S_6534': 294,\n",
       " 'ADNI3_041_S_6314': 295,\n",
       " 'ADNI3_941_S_6499': 296,\n",
       " 'ADNI3_023_S_6535': 297,\n",
       " 'ADNI3_305_S_6378': 298,\n",
       " 'ADNI3_082_S_6415': 299,\n",
       " '4_024_S_0985': 300,\n",
       " '6_131_S_0123': 301,\n",
       " '7_098_S_0160': 302,\n",
       " '8_027_S_0256': 303,\n",
       " '9_116_S_1243': 304,\n",
       " '17_011_S_0002': 305,\n",
       " '18_003_S_0907': 306,\n",
       " '26_052_S_1346': 307,\n",
       " '27_012_S_4026': 308,\n",
       " '28_037_S_4030': 309,\n",
       " '29_073_S_2182': 310,\n",
       " '30_116_S_4167': 311,\n",
       " '32_073_S_0089': 312,\n",
       " '33_082_S_2099': 313,\n",
       " '36_021_S_2100': 314,\n",
       " '37_127_S_1427': 315,\n",
       " '38_023_S_0926': 316,\n",
       " '39_137_S_4672': 317,\n",
       " '41_033_S_0920': 318,\n",
       " '43_137_S_1414': 319,\n",
       " '45_128_S_1408': 320,\n",
       " '47_072_S_2027': 321,\n",
       " '48_128_S_0545': 322,\n",
       " '50_021_S_0626': 323,\n",
       " '51_016_S_0702': 324,\n",
       " '52_136_S_0695': 325,\n",
       " '53_051_S_1072': 326,\n",
       " '54_014_S_0558': 327,\n",
       " '55_136_S_0873': 328,\n",
       " '57_002_S_0729': 329,\n",
       " '58_131_S_0384': 330,\n",
       " '61_014_S_0563': 331,\n",
       " '62_029_S_0845': 332,\n",
       " '63_007_S_0068': 333,\n",
       " '64_027_S_2219': 334,\n",
       " '65_021_S_2142': 335,\n",
       " '67_011_S_1080': 336,\n",
       " '69_127_S_1032': 337,\n",
       " '70_014_S_0169': 338,\n",
       " '71_027_S_1045': 339,\n",
       " '72_082_S_0832': 340,\n",
       " '74_082_S_4208': 341,\n",
       " '75_068_S_4174': 342,\n",
       " '77_051_S_1331': 343,\n",
       " '78_029_S_2376': 344,\n",
       " '79_031_S_4149': 345,\n",
       " '80_153_S_4151': 346,\n",
       " '81_098_S_2052': 347,\n",
       " '82_007_S_2058': 348,\n",
       " '83_007_S_0128': 349,\n",
       " '84_100_S_1226': 350,\n",
       " '85_127_S_0259': 351,\n",
       " '86_100_S_0296': 352,\n",
       " '87_031_S_4029': 353,\n",
       " '88_041_S_4051': 354,\n",
       " '90_098_S_4059': 355,\n",
       " '91_016_S_2031': 356,\n",
       " '92_941_S_4036': 357,\n",
       " '93_068_S_4134': 358,\n",
       " '94_003_S_4119': 359,\n",
       " '95_141_S_4160': 360,\n",
       " '96_041_S_1418': 361,\n",
       " '97_033_S_4176': 362,\n",
       " '98_941_S_4100': 363,\n",
       " '99_153_S_2148': 364,\n",
       " '134_014_S_4058': 365,\n",
       " '135_099_S_4076': 366,\n",
       " '137_027_S_0644': 367,\n",
       " '153_002_S_2043': 368,\n",
       " '174_037_S_0303': 369,\n",
       " '176_041_S_4138': 370,\n",
       " '189_023_S_4448': 371,\n",
       " '190_128_S_4571': 372,\n",
       " '191_012_S_4545': 373,\n",
       " '192_035_S_4414': 374,\n",
       " '194_109_S_4499': 375,\n",
       " '195_126_S_4514': 376,\n",
       " '200_014_S_0548': 377,\n",
       " '205_153_S_4159': 378,\n",
       " '217_051_S_1123': 379,\n",
       " '219_098_S_4095': 380,\n",
       " '222_141_S_0915': 381,\n",
       " '223_126_S_2407': 382,\n",
       " '226_037_S_4410': 383,\n",
       " '227_006_S_4357': 384,\n",
       " '229_018_S_2155': 385,\n",
       " '230_013_S_4268': 386,\n",
       " '231_021_S_4402': 387,\n",
       " '232_141_S_4438': 388,\n",
       " '233_019_S_4477': 389,\n",
       " '234_068_S_4424': 390,\n",
       " '235_141_S_4456': 391,\n",
       " '238_130_S_4417': 392,\n",
       " '239_153_S_4372': 393,\n",
       " '240_130_S_4294': 394,\n",
       " '242_021_S_4335': 395,\n",
       " '243_073_S_4360': 396,\n",
       " '244_021_S_4421': 397,\n",
       " '245_053_S_2396': 398,\n",
       " '247_032_S_0479': 399,\n",
       " '249_018_S_4313': 400,\n",
       " '250_018_S_2133': 401,\n",
       " '254_006_S_4449': 402,\n",
       " '259_072_S_4465': 403,\n",
       " '260_011_S_4547': 404,\n",
       " '262_053_S_4578': 405,\n",
       " '264_037_S_4381': 406,\n",
       " '265_041_S_4513': 407,\n",
       " '266_109_S_4455': 408,\n",
       " '267_007_S_4611': 409,\n",
       " '268_100_S_4469': 410,\n",
       " '269_022_S_4291': 411,\n",
       " '270_036_S_4562': 412,\n",
       " '271_130_S_4605': 413,\n",
       " '272_153_S_4621': 414,\n",
       " '273_130_S_4641': 415,\n",
       " '275_023_S_4502': 416,\n",
       " '282_067_S_4072': 417,\n",
       " '283_005_S_2390': 418,\n",
       " '284_023_S_4034': 419,\n",
       " '285_022_S_4196': 420,\n",
       " '286_099_S_4157': 421,\n",
       " '287_002_S_2073': 422,\n",
       " '289_128_S_2036': 423,\n",
       " '291_014_S_4263': 424,\n",
       " '292_009_S_4564': 425,\n",
       " '294_127_S_1419': 426,\n",
       " '300_073_S_2191': 427,\n",
       " '302_127_S_0260': 428,\n",
       " '307_136_S_0186': 429,\n",
       " '309_023_S_4164': 430,\n",
       " '313_068_S_4340': 431,\n",
       " '314_011_S_4278': 432,\n",
       " '315_129_S_4396': 433,\n",
       " '317_116_S_4338': 434,\n",
       " '320_130_S_4343': 435,\n",
       " '324_129_S_4371': 436,\n",
       " '326_127_S_0112': 437,\n",
       " '329_099_S_4104': 438,\n",
       " '330_014_S_4039': 439,\n",
       " '333_016_S_4121': 440,\n",
       " '337_003_S_4350': 441,\n",
       " '338_013_S_4580': 442,\n",
       " '339_126_S_4494': 443,\n",
       " '341_141_S_2210': 444,\n",
       " '343_032_S_4429': 445,\n",
       " '344_009_S_4530': 446,\n",
       " '346_002_S_4473': 447,\n",
       " '347_007_S_1206': 448,\n",
       " '348_073_S_4540': 449,\n",
       " '350_053_S_4557': 450,\n",
       " '351_036_S_4491': 451,\n",
       " '352_007_S_0101': 452,\n",
       " '353_019_S_4548': 453,\n",
       " '354_137_S_4482': 454,\n",
       " '355_082_S_4428': 455,\n",
       " '356_006_S_4363': 456,\n",
       " '357_135_S_4446': 457,\n",
       " '358_009_S_4359': 458,\n",
       " '362_018_S_4597': 459,\n",
       " '365_130_S_4589': 460,\n",
       " '366_009_S_4612': 461,\n",
       " '369_003_S_0981': 462,\n",
       " '370_007_S_4568': 463,\n",
       " '371_135_S_4598': 464,\n",
       " '372_137_S_4520': 465,\n",
       " '374_072_S_4539': 466,\n",
       " '375_073_S_4552': 467,\n",
       " '376_128_S_4603': 468,\n",
       " '377_135_S_4566': 469,\n",
       " '378_014_S_4615': 470,\n",
       " '379_037_S_4146': 471,\n",
       " '380_016_S_2007': 472,\n",
       " '381_035_S_4582': 473,\n",
       " '382_135_S_4489': 474,\n",
       " '383_007_S_4467': 475,\n",
       " '387_130_S_4405': 476,\n",
       " '388_126_S_4507': 477,\n",
       " '389_941_S_4365': 478,\n",
       " '392_130_S_4468': 479,\n",
       " '393_036_S_4430': 480,\n",
       " '394_005_S_4707': 481,\n",
       " '397_022_S_1097': 482,\n",
       " '398_041_S_4037': 483,\n",
       " '399_141_S_4053': 484,\n",
       " '401_023_S_4122': 485,\n",
       " '402_052_S_4626': 486,\n",
       " '403_023_S_4501': 487,\n",
       " '404_037_S_4071': 488,\n",
       " '405_094_S_4234': 489,\n",
       " '406_137_S_4331': 490,\n",
       " '409_032_S_4386': 491,\n",
       " '410_022_S_4266': 492,\n",
       " '411_094_S_4434': 493,\n",
       " '414_068_S_4431': 494,\n",
       " '415_007_S_4516': 495,\n",
       " '416_016_S_4584': 496,\n",
       " '417_006_S_4546': 497,\n",
       " '418_114_S_4379': 498,\n",
       " '420_116_S_4625': 499,\n",
       " '421_022_S_4444': 500,\n",
       " '422_041_S_4510': 501,\n",
       " '423_099_S_4475': 502,\n",
       " '424_037_S_4308': 503,\n",
       " '425_029_S_4385': 504,\n",
       " '426_072_S_4462': 505,\n",
       " '427_007_S_4488': 506,\n",
       " '428_002_S_4654': 507,\n",
       " '429_037_S_4432': 508,\n",
       " '430_073_S_4614': 509,\n",
       " '431_073_S_4559': 510,\n",
       " '432_136_S_4408': 511,\n",
       " '434_014_S_4668': 512,\n",
       " '435_013_S_4595': 513,\n",
       " '436_041_S_4629': 514,\n",
       " '437_128_S_4653': 515,\n",
       " '440_020_S_1288': 516,\n",
       " '442_116_S_0752': 517,\n",
       " '444_012_S_4188': 518,\n",
       " '445_021_S_4254': 519,\n",
       " '446_137_S_4299': 520,\n",
       " '447_031_S_4590': 521,\n",
       " '449_006_S_0498': 522,\n",
       " '451_072_S_2083': 523,\n",
       " '453_068_S_2184': 524,\n",
       " '455_094_S_2238': 525,\n",
       " '457_127_S_2234': 526,\n",
       " '458_031_S_2233': 527,\n",
       " '459_022_S_2263': 528,\n",
       " '461_022_S_0130': 529,\n",
       " '462_098_S_4018': 530,\n",
       " '463_072_S_4007': 531,\n",
       " '464_037_S_4015': 532,\n",
       " '465_037_S_4001': 533,\n",
       " '466_130_S_2373': 534,\n",
       " '468_002_S_4251': 535,\n",
       " '469_023_S_2068': 536,\n",
       " '470_072_S_4206': 537,\n",
       " '472_099_S_4202': 538,\n",
       " '474_129_S_4422': 539,\n",
       " '476_029_S_4327': 540,\n",
       " '477_082_S_1256': 541,\n",
       " '478_099_S_4498': 542,\n",
       " '479_006_S_4153': 543,\n",
       " '480_098_S_2079': 544,\n",
       " '481_137_S_0800': 545,\n",
       " '483_098_S_2047': 546,\n",
       " '484_941_S_4187': 547,\n",
       " '485_002_S_4225': 548,\n",
       " '487_002_S_4237': 549,\n",
       " '488_070_S_4692': 550,\n",
       " '489_018_S_4349': 551,\n",
       " '490_137_S_4303': 552,\n",
       " '491_035_S_2061': 553,\n",
       " '492_041_S_4271': 554,\n",
       " '493_135_S_4309': 555,\n",
       " '495_009_S_4388': 556,\n",
       " '498_024_S_4392': 557,\n",
       " '499_035_S_4256': 558,\n",
       " '500_098_S_0896': 559,\n",
       " '501_016_S_4353': 560,\n",
       " '503_130_S_4250': 561,\n",
       " '504_130_S_4542': 562,\n",
       " '505_003_S_0908': 563,\n",
       " '508_127_S_2213': 564,\n",
       " '509_018_S_2138': 565,\n",
       " '510_027_S_0120': 566,\n",
       " '511_128_S_2151': 567,\n",
       " '512_032_S_2247': 568,\n",
       " '513_027_S_1387': 569,\n",
       " '514_098_S_0172': 570,\n",
       " '515_014_S_2308': 571,\n",
       " '516_023_S_1190': 572,\n",
       " '517_005_S_0448': 573,\n",
       " '518_068_S_2316': 574,\n",
       " '520_007_S_4272': 575,\n",
       " '521_098_S_4275': 576,\n",
       " '522_068_S_4332': 577,\n",
       " '523_018_S_4400': 578,\n",
       " '524_072_S_4383': 579,\n",
       " '525_153_S_4297': 580,\n",
       " '526_006_S_4515': 581,\n",
       " '528_023_S_4241': 582,\n",
       " '529_016_S_4591': 583,\n",
       " '530_128_S_4553': 584,\n",
       " '531_009_S_4543': 585,\n",
       " '532_127_S_4604': 586,\n",
       " '533_127_S_4500': 587,\n",
       " '534_941_S_4420': 588,\n",
       " '535_099_S_4480': 589,\n",
       " '536_094_S_4503': 590,\n",
       " '537_098_S_4506': 591,\n",
       " '538_072_S_4522': 592,\n",
       " '539_128_S_4599': 593,\n",
       " '541_116_S_4043': 594,\n",
       " '542_073_S_0311': 595,\n",
       " '543_019_S_4285': 596,\n",
       " '544_032_S_4348': 597,\n",
       " '545_021_S_4659': 598,\n",
       " '546_027_S_2183': 599,\n",
       " '547_014_S_2185': 600,\n",
       " '548_022_S_1351': 601,\n",
       " '549_068_S_0127': 602,\n",
       " '550_068_S_0872': 603,\n",
       " '551_014_S_4401': 604,\n",
       " '552_006_S_4346': 605,\n",
       " '553_114_S_0378': 606,\n",
       " '554_141_S_4426': 607,\n",
       " '555_099_S_4463': 608,\n",
       " '556_073_S_4443': 609,\n",
       " '557_127_S_4240': 610,\n",
       " '558_094_S_4560': 611,\n",
       " '559_007_S_4620': 612,\n",
       " '560_019_S_4549': 613,\n",
       " '561_067_S_4310': 614,\n",
       " '562_141_S_4232': 615,\n",
       " '564_006_S_1130': 616,\n",
       " '565_057_S_2398': 617,\n",
       " '568_136_S_4269': 618,\n",
       " '569_126_S_4458': 619,\n",
       " '570_072_S_4445': 620,\n",
       " '571_116_S_4453': 621,\n",
       " '572_114_S_4404': 622,\n",
       " '573_072_S_4394': 623,\n",
       " '574_031_S_4474': 624,\n",
       " '575_072_S_4613': 625,\n",
       " '576_116_S_4635': 626,\n",
       " '577_127_S_4645': 627,\n",
       " '579_136_S_4433': 628,\n",
       " '581_128_S_4609': 629,\n",
       " '582_137_S_4596': 630,\n",
       " '583_141_S_2333': 631,\n",
       " '585_007_S_4637': 632,\n",
       " '588_032_S_1169': 633,\n",
       " '589_099_S_1034': 634,\n",
       " '591_116_S_4092': 635,\n",
       " '592_136_S_0107': 636,\n",
       " '594_002_S_4270': 637,\n",
       " '595_035_S_2074': 638,\n",
       " '596_127_S_4301': 639,\n",
       " '597_099_S_4205': 640,\n",
       " '598_082_S_4339': 641,\n",
       " '599_032_S_2119': 642,\n",
       " '603_009_S_4337': 643,\n",
       " '604_012_S_1212': 644,\n",
       " '605_027_S_2336': 645,\n",
       " '606_068_S_2315': 646,\n",
       " '607_022_S_1394': 647,\n",
       " '609_057_S_1007': 648,\n",
       " '610_082_S_4224': 649,\n",
       " '612_135_S_4281': 650,\n",
       " '613_130_S_4415': 651,\n",
       " '615_137_S_4536': 652,\n",
       " '616_135_S_4657': 653,\n",
       " '617_023_S_0376': 654,\n",
       " '618_012_S_4643': 655,\n",
       " '619_099_S_4565': 656,\n",
       " '623_018_S_0055': 657,\n",
       " '624_023_S_0625': 658,\n",
       " '626_021_S_2125': 659,\n",
       " '627_007_S_2106': 660,\n",
       " '630_022_S_2087': 661,\n",
       " '631_036_S_0869': 662,\n",
       " '634_033_S_1016': 663,\n",
       " '635_005_S_0546': 664,\n",
       " '636_033_S_1098': 665,\n",
       " '637_033_S_0922': 666,\n",
       " '641_072_S_2093': 667,\n",
       " '642_072_S_2037': 668,\n",
       " '643_033_S_0906': 669,\n",
       " '644_094_S_1417': 670,\n",
       " '646_005_S_0610': 671,\n",
       " '647_127_S_0925': 672,\n",
       " '648_137_S_0972': 673,\n",
       " '649_036_S_0672': 674,\n",
       " '650_128_S_2011': 675,\n",
       " '651_033_S_1116': 676,\n",
       " '652_005_S_0602': 677,\n",
       " '653_052_S_2249': 678,\n",
       " '654_035_S_2199': 679,\n",
       " '655_021_S_2150': 680,\n",
       " '656_072_S_0315': 681,\n",
       " '657_082_S_2307': 682,\n",
       " '658_002_S_1261': 683,\n",
       " '659_123_S_1300': 684,\n",
       " '660_037_S_4028': 685,\n",
       " '661_072_S_4063': 686,\n",
       " '662_036_S_1023': 687,\n",
       " '663_011_S_4075': 688,\n",
       " '665_027_S_0074': 689,\n",
       " '666_128_S_2002': 690,\n",
       " '667_072_S_2026': 691,\n",
       " '670_016_S_0359': 692,\n",
       " '671_031_S_0351': 693,\n",
       " '672_127_S_0622': 694,\n",
       " '673_014_S_0520': 695,\n",
       " '674_135_S_4676': 696,\n",
       " '675_014_S_0658': 697,\n",
       " '676_012_S_1133': 698,\n",
       " '678_128_S_2003': 699,\n",
       " '679_003_S_1074': 700,\n",
       " '680_016_S_1117': 701,\n",
       " '682_099_S_2146': 702,\n",
       " '683_037_S_0501': 703,\n",
       " '684_031_S_2022': 704,\n",
       " '687_073_S_2190': 705,\n",
       " '689_068_S_2168': 706,\n",
       " '691_037_S_0377': 707,\n",
       " '692_067_S_2195': 708,\n",
       " '695_057_S_1269': 709,\n",
       " '696_129_S_1246': 710,\n",
       " '697_031_S_4005': 711,\n",
       " '698_067_S_2304': 712,\n",
       " '699_129_S_2332': 713,\n",
       " '700_022_S_4173': 714,\n",
       " '701_006_S_4150': 715,\n",
       " '702_002_S_0685': 716,\n",
       " '703_031_S_0618': 717,\n",
       " '704_031_S_4203': 718,\n",
       " '709_003_S_4152': 719,\n",
       " '710_002_S_4171': 720,\n",
       " '711_009_S_4324': 721,\n",
       " '713_128_S_2045': 722,\n",
       " '714_009_S_0842': 723,\n",
       " '715_128_S_2220': 724,\n",
       " '716_068_S_2248': 725,\n",
       " '717_022_S_2167': 726,\n",
       " '718_094_S_4162': 727,\n",
       " '719_023_S_4035': 728,\n",
       " '721_031_S_0830': 729,\n",
       " '723_031_S_4218': 730,\n",
       " '725_127_S_4148': 731,\n",
       " '726_072_S_4103': 732,\n",
       " '727_033_S_0741': 733,\n",
       " '728_037_S_0566': 734,\n",
       " '729_024_S_4280': 735,\n",
       " '730_082_S_2121': 736,\n",
       " '731_013_S_1186': 737,\n",
       " '732_011_S_1282': 738,\n",
       " '734_036_S_0945': 739,\n",
       " '735_100_S_0069': 740,\n",
       " '736_123_S_0298': 741,\n",
       " '737_032_S_0214': 742,\n",
       " '738_041_S_4060': 743,\n",
       " '739_126_S_4686': 744,\n",
       " '740_137_S_0459': 745,\n",
       " '741_041_S_0679': 746,\n",
       " '742_072_S_4057': 747,\n",
       " '743_116_S_4195': 748,\n",
       " '744_019_S_4252': 749,\n",
       " '745_073_S_4155': 750,\n",
       " '746_002_S_4262': 751,\n",
       " '747_041_S_1260': 752,\n",
       " '748_053_S_2357': 753,\n",
       " '749_123_S_2363': 754,\n",
       " '750_099_S_0352': 755,\n",
       " '751_009_S_2381': 756,\n",
       " '752_098_S_4003': 757,\n",
       " '753_016_S_1326': 758,\n",
       " '755_126_S_2360': 759,\n",
       " '756_018_S_4696': 760,\n",
       " '757_130_S_0289': 761,\n",
       " '758_051_S_1040': 762,\n",
       " '759_031_S_4042': 763,\n",
       " '760_007_S_2394': 764,\n",
       " '761_137_S_0722': 765,\n",
       " '762_116_S_0657': 766,\n",
       " '763_100_S_0047': 767,\n",
       " '764_018_S_2180': 768,\n",
       " '765_137_S_0973': 769,\n",
       " '766_098_S_0171': 770,\n",
       " '767_131_S_0441': 771,\n",
       " '768_129_S_4073': 772,\n",
       " '769_068_S_0802': 773,\n",
       " '770_023_S_4115': 774,\n",
       " '771_037_S_0588': 775,\n",
       " '772_037_S_0150': 776,\n",
       " '773_051_S_1131': 777,\n",
       " '774_011_S_4366': 778,\n",
       " '775_072_S_2116': 779,\n",
       " '776_035_S_0555': 780,\n",
       " '777_126_S_0605': 781,\n",
       " '778_068_S_4061': 782,\n",
       " '780_033_S_4179': 783,\n",
       " '781_024_S_4158': 784,\n",
       " '782_029_S_2395': 785,\n",
       " '783_153_S_4172': 786,\n",
       " '784_005_S_4185': 787,\n",
       " '785_067_S_4212': 788,\n",
       " '787_006_S_4192': 789,\n",
       " '788_041_S_4200': 790,\n",
       " '789_128_S_0272': 791,\n",
       " '790_100_S_1286': 792,\n",
       " '791_041_S_4014': 793,\n",
       " '792_031_S_4024': 794,\n",
       " '793_052_S_0671': 795,\n",
       " '794_073_S_4216': 796,\n",
       " '795_082_S_4090': 797,\n",
       " '797_114_S_2392': 798,\n",
       " '798_041_S_4041': 799,\n",
       " '799_037_S_4214': 800,\n",
       " '800_035_S_4114': 801,\n",
       " '801_123_S_4096': 802,\n",
       " '802_005_S_4168': 803,\n",
       " '803_072_S_4131': 804,\n",
       " '804_032_S_2240': 805,\n",
       " '806_002_S_1268': 806,\n",
       " '807_021_S_0276': 807,\n",
       " '808_098_S_4050': 808,\n",
       " '809_123_S_4127': 809,\n",
       " '810_123_S_4170': 810,\n",
       " '811_099_S_0051': 811,\n",
       " '813_099_S_2042': 812,\n",
       " '814_073_S_4259': 813,\n",
       " '815_037_S_4302': 814,\n",
       " '817_082_S_4244': 815,\n",
       " '818_135_S_4356': 816,\n",
       " '101_018_S_0682': 817,\n",
       " '103_021_S_0424': 818,\n",
       " '106_006_S_0484': 819,\n",
       " '107_133_S_0629': 820,\n",
       " '108_128_S_1409': 821,\n",
       " '109_099_S_0040': 822,\n",
       " '10_005_S_1224': 823,\n",
       " '110_068_S_0442': 824,\n",
       " '114_073_S_0565': 825,\n",
       " '115_131_S_0497': 826,\n",
       " '116_033_S_0723': 827,\n",
       " '117_130_S_0449': 828,\n",
       " '118_033_S_0516': 829,\n",
       " '121_137_S_0438': 830,\n",
       " '123_018_S_0450': 831,\n",
       " '124_041_S_0721': 832,\n",
       " '126_012_S_1009': 833,\n",
       " '128_022_S_0044': 834,\n",
       " '12_010_S_0472': 835,\n",
       " '130_941_S_1311': 836,\n",
       " '131_136_S_0874': 837,\n",
       " '132_053_S_0621': 838,\n",
       " '134_029_S_1215': 839,\n",
       " '135_067_S_1185': 840,\n",
       " '136_067_S_0038': 841,\n",
       " '138_067_S_0019': 842,\n",
       " '141_007_S_0316': 843,\n",
       " '142_141_S_1152': 844,\n",
       " '143_141_S_0853': 845,\n",
       " '145_126_S_0784': 846,\n",
       " '146_100_S_0006': 847,\n",
       " '147_013_S_0575': 848,\n",
       " '153_137_S_0481': 849,\n",
       " '154_128_S_0216': 850,\n",
       " '156_128_S_0310': 851,\n",
       " '157_100_S_0190': 852,\n",
       " '158_130_S_0783': 853,\n",
       " '159_098_S_0884': 854,\n",
       " '15_099_S_0470': 855,\n",
       " '162_041_S_1002': 856,\n",
       " '163_027_S_0417': 857,\n",
       " '165_023_S_1289': 858,\n",
       " '167_033_S_1279': 859,\n",
       " '169_067_S_0110': 860,\n",
       " '16_128_S_0245': 861,\n",
       " '170_073_S_0445': 862,\n",
       " '172_072_S_1211': 863,\n",
       " '173_022_S_0544': 864,\n",
       " '176_035_S_0204': 865,\n",
       " '178_941_S_1194': 866,\n",
       " '179_007_S_0041': 867,\n",
       " '17_137_S_0825': 868,\n",
       " '180_011_S_0168': 869,\n",
       " '181_006_S_0675': 870,\n",
       " '182_016_S_1263': 871,\n",
       " '183_041_S_1423': 872,\n",
       " '184_027_S_0461': 873,\n",
       " '185_052_S_1168': 874,\n",
       " '187_126_S_0506': 875,\n",
       " '189_023_S_0963': 876,\n",
       " '18_128_S_0266': 877,\n",
       " '190_067_S_0177': 878,\n",
       " '191_029_S_1073': 879,\n",
       " '192_067_S_0290': 880,\n",
       " '194_033_S_1308': 881,\n",
       " '196_037_S_0454': 882,\n",
       " '198_126_S_1221': 883,\n",
       " '199_012_S_0634': 884,\n",
       " '19_067_S_0243': 885,\n",
       " '203_068_S_1191': 886,\n",
       " '204_136_S_0184': 887,\n",
       " '205_141_S_1024': 888,\n",
       " '206_136_S_0194': 889,\n",
       " '208_013_S_1120': 890,\n",
       " '209_137_S_0443': 891,\n",
       " '210_036_S_0673': 892,\n",
       " '211_053_S_0507': 893,\n",
       " '212_023_S_1126': 894,\n",
       " '215_116_S_0392': 895,\n",
       " '216_082_S_1119': 896,\n",
       " '217_029_S_1184': 897,\n",
       " '218_130_S_0969': 898,\n",
       " '21_016_S_1138': 899,\n",
       " '221_062_S_0730': 900,\n",
       " '222_098_S_0288': 901,\n",
       " '223_133_S_0638': 902,\n",
       " '224_027_S_0179': 903,\n",
       " '227_123_S_0050': 904,\n",
       " '228_130_S_0285': 905,\n",
       " '229_099_S_0534': 906,\n",
       " '230_002_S_0782': 907,\n",
       " '231_067_S_0029': 908,\n",
       " '232_007_S_1339': 909,\n",
       " '234_006_S_0653': 910,\n",
       " '238_023_S_0139': 911,\n",
       " '239_029_S_1384': 912,\n",
       " '240_027_S_1082': 913,\n",
       " '241_128_S_0500': 914,\n",
       " '244_057_S_0957': 915,\n",
       " '246_082_S_0304': 916,\n",
       " '248_141_S_0810': 917,\n",
       " '249_041_S_1391': 918,\n",
       " '24_007_S_0070': 919,\n",
       " '250_016_S_0590': 920,\n",
       " '251_007_S_0414': 921,\n",
       " '253_007_S_0249': 922,\n",
       " '255_031_S_0321': 923,\n",
       " '256_082_S_0363': 924,\n",
       " '257_062_S_0690': 925,\n",
       " '258_007_S_1248': 926,\n",
       " '259_011_S_0861': 927,\n",
       " '25_133_S_1170': 928,\n",
       " '262_052_S_1251': 929,\n",
       " '263_072_S_1380': 930,\n",
       " '265_141_S_0726': 931,\n",
       " '267_130_S_0102': 932,\n",
       " '268_141_S_1004': 933,\n",
       " '269_130_S_0423': 934,\n",
       " '26_128_S_0522': 935,\n",
       " '271_082_S_1079': 936,\n",
       " '272_141_S_1231': 937,\n",
       " '273_062_S_0768': 938,\n",
       " '274_136_S_0195': 939,\n",
       " '275_018_S_0087': 940,\n",
       " '277_018_S_0335': 941,\n",
       " '279_016_S_0769': 942,\n",
       " '27_141_S_1245': 943,\n",
       " '281_141_S_1051': 944,\n",
       " '282_022_S_0066': 945,\n",
       " '285_062_S_1294': 946,\n",
       " '286_100_S_0747': 947,\n",
       " '287_136_S_0429': 948,\n",
       " '289_023_S_0126': 949,\n",
       " '290_941_S_1363': 950,\n",
       " '291_031_S_1209': 951,\n",
       " '292_037_S_0627': 952,\n",
       " '294_036_S_0976': 953,\n",
       " '296_029_S_0914': 954,\n",
       " '297_073_S_0312': 955,\n",
       " '298_021_S_0178': 956,\n",
       " '299_021_S_0647': 957,\n",
       " '29_007_S_1304': 958,\n",
       " '2_005_S_1341': 959,\n",
       " '300_021_S_0332': 960,\n",
       " '301_027_S_0850': 961,\n",
       " '302_131_S_0319': 962,\n",
       " '303_032_S_1101': 963,\n",
       " '304_002_S_0559': 964,\n",
       " '305_032_S_0677': 965,\n",
       " '309_041_S_1411': 966,\n",
       " '310_137_S_0841': 967,\n",
       " '311_013_S_1276': 968,\n",
       " '312_029_S_0999': 969,\n",
       " '313_022_S_0543': 970,\n",
       " '314_082_S_0469': 971,\n",
       " '315_012_S_1292': 972,\n",
       " '316_082_S_1377': 973,\n",
       " '319_002_S_0413': 974,\n",
       " '31_006_S_0547': 975,\n",
       " '320_032_S_0718': 976,\n",
       " '321_141_S_1244': 977,\n",
       " '323_126_S_0891': 978,\n",
       " '324_013_S_1205': 979,\n",
       " '325_005_S_0222': 980,\n",
       " '32_009_S_0751': 981,\n",
       " '330_099_S_0551': 982,\n",
       " '333_002_S_0295': 983,\n",
       " '334_100_S_1154': 984,\n",
       " '336_128_S_0188': 985,\n",
       " '337_027_S_1213': 986,\n",
       " '339_029_S_1318': 987,\n",
       " '33_123_S_0390': 988,\n",
       " '341_037_S_0327': 989,\n",
       " '342_033_S_1284': 990,\n",
       " '343_068_S_0476': 991,\n",
       " '344_029_S_1056': 992,\n",
       " '345_067_S_0045': 993,\n",
       " '348_023_S_0078': 994,\n",
       " '349_100_S_0015': 995,\n",
       " '350_012_S_0689': 996,\n",
       " '352_114_S_0979': 997,\n",
       " '353_128_S_1406': 998,\n",
       " '354_137_S_1041': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "GWAS_ID = 'ukb-b-14057'\n",
    "usable_samples_ADNI = pd.read_csv(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n",
    "usable_samples_ADNI = {sample:idx for idx,sample in enumerate(usable_samples_ADNI)}\n",
    "json.dump(usable_samples_ADNI, open('./usable_samples_ADNI.json', 'w'))\n",
    "# json.dump(usable_samples_ADNI, open(inputFolder+'usable_samples_ADNI.json', 'w'))\n",
    "\n",
    "usable_samples_ADNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17813/4095858758.py:2: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  usable_samples_ADNI_unsplitted = pd.read_csv(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n"
     ]
    }
   ],
   "source": [
    "GWAS_ID = 'ukb-b-13806'\n",
    "usable_samples_ADNI_unsplitted = pd.read_csv(f'./PRSice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n",
    "usable_samples_ADNI = {'_'.join(sample.split('_')[1:]):sample for sample in (usable_samples_ADNI_unsplitted)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progression from non-dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17813/2699186156.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 5)\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "Final Samples Non-Dementia to Dementia length :  348\n",
      "Final Samples Non-Dementia to Non-Dementia length :  866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(348, 866, 1214)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "THRESHOLD_MONTH = 12*2\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month','DX_bl']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Non_Dementia_to_Dementia= set()\n",
    "Final_Samples_Non_Dementia_to_NonDementia= set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    dx_bl= ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX_bl'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    # if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "    if last_dx == 'Dementia' and dx_bl != 'AD' and sample in usable_samples_ADNI:\n",
    "        \n",
    "        \n",
    "        Final_Samples_Non_Dementia_to_Dementia = Final_Samples_Non_Dementia_to_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "                \n",
    "\n",
    "\n",
    "for sample in Samples_NonDementia:\n",
    "    dx_bl= ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX_bl'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and dx_bl !='AD' and sample in usable_samples_ADNI:\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_Non_Dementia_to_NonDementia = Final_Samples_Non_Dementia_to_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Non_Dementia_to_Dementia = [[x, 1] for x in Final_Samples_Non_Dementia_to_Dementia]\n",
    "Final_Samples_Non_Dementia_to_NonDementia = [[x, 0] for x in Final_Samples_Non_Dementia_to_NonDementia]\n",
    "print(\"Final Samples Non-Dementia to Dementia length : \", len(Final_Samples_Non_Dementia_to_Dementia) )\n",
    "print(\"Final Samples Non-Dementia to Non-Dementia length : \", len(Final_Samples_Non_Dementia_to_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Non_Dementia_to_Dementia + Final_Samples_Non_Dementia_to_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_2yrs_transition_from_Non_dementia.json', 'w'))\n",
    "json.dump(Final_Samples_Non_Dementia_to_Dementia, open('Final_Samples_Non_Dementia_to_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_Non_Dementia_to_NonDementia, open('Final_Samples_Non_Dementia_to_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Non_Dementia_to_Dementia.__len__(), Final_Samples_Non_Dementia_to_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29469/195097219.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  RID        PTID VISCODE  SITE COLPROT ORIGPROT  EXAMDATE DX_bl  \\\n",
      "0           0    2  011_S_0002      bl    11   ADNI1    ADNI1  08-09-05    CN   \n",
      "1           1    3  011_S_0003      bl    11   ADNI1    ADNI1  12-09-05    AD   \n",
      "2           2    3  011_S_0003     m06    11   ADNI1    ADNI1  13-03-06    AD   \n",
      "3           3    3  011_S_0003     m12    11   ADNI1    ADNI1  12-09-06    AD   \n",
      "4           4    3  011_S_0003     m24    11   ADNI1    ADNI1  12-09-07    AD   \n",
      "\n",
      "    AGE  ...   FDG_bl  PIB_bl AV45_bl  Years_bl  Month_bl  Month     M  \\\n",
      "0  74.3  ...  1.36665     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "1  81.3  ...  1.08355     NaN     NaN  0.000000   0.00000      0   0.0   \n",
      "2  81.3  ...  1.08355     NaN     NaN  0.498289   5.96721      6   6.0   \n",
      "3  81.3  ...  1.08355     NaN     NaN  0.999316  11.96720     12  12.0   \n",
      "4  81.3  ...  1.08355     NaN     NaN  1.998630  23.93440     24  24.0   \n",
      "\n",
      "   update_stamp     IID_fix             IID  \n",
      "0       55:41.0  011_S_0002   17_011_S_0002  \n",
      "1       55:41.0  011_S_0003  711_011_S_0003  \n",
      "2       55:41.0  011_S_0003  711_011_S_0003  \n",
      "3       55:41.0  011_S_0003  711_011_S_0003  \n",
      "4       55:41.0  011_S_0003  711_011_S_0003  \n",
      "\n",
      "[5 rows x 116 columns]\n",
      "(15122, 116)\n",
      "Dementia shape :  (2338, 5)\n",
      "Samples Dementia length :  787\n",
      "Samples NonDementia length :  1481\n",
      "Final Samples Non-Dementia to Dementia length :  348\n",
      "Final Samples Non-Dementia to Non-Dementia length :  504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(348, 504, 852)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "THRESHOLD_MONTH = 12*4\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "print(ADNIMERGE.head())\n",
    "print(ADNIMERGE.shape)\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month','DX_bl']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "Dementia = ADNI_TEMP[ADNI_TEMP['DX']=='Dementia']\n",
    "print(\"Dementia shape : \", Dementia.shape)\n",
    "Samples_Dementia = set(Dementia['PTID'].unique())\n",
    "print(\"Samples Dementia length : \", len(Samples_Dementia) )\n",
    "Samples_NonDementia = set(ADNI_TEMP['PTID'].unique()) - Samples_Dementia\n",
    "print(\"Samples NonDementia length : \", len( Samples_NonDementia ) )\n",
    "\n",
    "# Samples_NonDementia\n",
    "Final_Samples_Non_Dementia_to_Dementia= set()\n",
    "Final_Samples_Non_Dementia_to_NonDementia= set()\n",
    "for sample in Samples_Dementia:\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    dx_bl= ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX_bl'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    \n",
    "    # if True:\n",
    "#     if last_month_of_dx >= THRESHOLD_MONTH :\n",
    "#         if last_dx != 'Dementia':\n",
    "#             print(last_dx)\n",
    "        \n",
    "    if last_dx == 'Dementia' and dx_bl != 'AD' and sample in usable_samples_ADNI:\n",
    "        \n",
    "        \n",
    "        Final_Samples_Non_Dementia_to_Dementia = Final_Samples_Non_Dementia_to_Dementia.union({usable_samples_ADNI[sample]})\n",
    "            \n",
    "        \n",
    "#             why check for dementia again?\n",
    "                \n",
    "\n",
    "\n",
    "for sample in Samples_NonDementia:\n",
    "    dx_bl= ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX_bl'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if last_month_of_dx >= THRESHOLD_MONTH and dx_bl !='AD' and sample in usable_samples_ADNI:\n",
    "#     if sample in usable_samples_ADNI:\n",
    "        Final_Samples_Non_Dementia_to_NonDementia = Final_Samples_Non_Dementia_to_NonDementia | {usable_samples_ADNI[sample]} # union symbol in python sets\n",
    "    \n",
    "# sum(ADNI_TEMP['Month'] == 6), ADNI_TEMP.shape\n",
    "# Dementia\n",
    "Final_Samples_Non_Dementia_to_Dementia = [[x, 1] for x in Final_Samples_Non_Dementia_to_Dementia]\n",
    "Final_Samples_Non_Dementia_to_NonDementia = [[x, 0] for x in Final_Samples_Non_Dementia_to_NonDementia]\n",
    "print(\"Final Samples Non-Dementia to Dementia length : \", len(Final_Samples_Non_Dementia_to_Dementia) )\n",
    "print(\"Final Samples Non-Dementia to Non-Dementia length : \", len(Final_Samples_Non_Dementia_to_NonDementia) )\n",
    "\n",
    "Final_Samples = Final_Samples_Non_Dementia_to_Dementia + Final_Samples_Non_Dementia_to_NonDementia\n",
    "# json.dump(Final_Samples, open(inputFolder+'Final_Samples.json', 'w'))\n",
    "json.dump(Final_Samples, open('Final_Samples_4yrs_transition_from_Non_dementia.json', 'w'))\n",
    "json.dump(Final_Samples_Non_Dementia_to_Dementia, open('Final_Samples_Non_Dementia_to_Dementia.json', 'w'))\n",
    "json.dump(Final_Samples_Non_Dementia_to_NonDementia, open('Final_Samples_Non_Dementia_to_NonDementia.json', 'w'))\n",
    "\n",
    "\n",
    "Final_Samples_Non_Dementia_to_Dementia.__len__(), Final_Samples_Non_Dementia_to_NonDementia.__len__(), Final_Samples.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progression from CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17813/2379879776.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CN data size  516\n",
      "samples progressed:  88\n",
      "samples not progressed:  282\n"
     ]
    }
   ],
   "source": [
    "# progression from CN\n",
    "THRESHOLD_MONTH = 12*2\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month','DX_bl']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "ADNI_TEMP = ADNI_TEMP[ADNI_TEMP['DX_bl']=='CN']\n",
    "\n",
    "all_cn_samples = set(ADNI_TEMP['PTID'].unique())\n",
    "print(\"Baseline CN data size \",all_cn_samples.__len__())\n",
    "samples_progressed= set()\n",
    "samples_not_progressed= set()\n",
    "for sample in all_cn_samples :\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if sample in usable_samples_ADNI:\n",
    "        if (last_dx == 'Dementia' or last_dx == 'MCI'):\n",
    "                samples_progressed = samples_progressed.union({usable_samples_ADNI[sample]})\n",
    "        elif last_month_of_dx >= THRESHOLD_MONTH :\n",
    "                samples_not_progressed = samples_not_progressed.union({usable_samples_ADNI[sample]})\n",
    "print(\"samples progressed: \", len(samples_progressed))\n",
    "print(\"samples not progressed: \", len(samples_not_progressed))\n",
    "samples_progressed = [[x, 1] for x in samples_progressed]\n",
    "samples_not_progressed = [[x, 0] for x in samples_not_progressed]\n",
    "final_samples=samples_progressed+samples_not_progressed\n",
    "json.dump(final_samples, open('final_samples_progression_from_cn_2yrs.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29469/173764755.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CN data size  516\n",
      "samples progressed:  88\n",
      "samples not progressed:  175\n"
     ]
    }
   ],
   "source": [
    "# progression from CN\n",
    "THRESHOLD_MONTH = 12*4\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month','DX_bl']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "ADNI_TEMP = ADNI_TEMP[ADNI_TEMP['DX_bl']=='CN']\n",
    "\n",
    "all_cn_samples = set(ADNI_TEMP['PTID'].unique())\n",
    "print(\"Baseline CN data size \",all_cn_samples.__len__())\n",
    "samples_progressed= set()\n",
    "samples_not_progressed= set()\n",
    "for sample in all_cn_samples :\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if sample in usable_samples_ADNI:\n",
    "        if (last_dx == 'Dementia' or last_dx == 'MCI'):\n",
    "                samples_progressed = samples_progressed.union({usable_samples_ADNI[sample]})\n",
    "        elif last_month_of_dx >= THRESHOLD_MONTH :\n",
    "                samples_not_progressed = samples_not_progressed.union({usable_samples_ADNI[sample]})\n",
    "print(\"samples progressed: \", len(samples_progressed))\n",
    "print(\"samples not progressed: \", len(samples_not_progressed))\n",
    "samples_progressed = [[x, 1] for x in samples_progressed]\n",
    "samples_not_progressed = [[x, 0] for x in samples_not_progressed]\n",
    "final_samples=samples_progressed+samples_not_progressed\n",
    "json.dump(final_samples, open('final_samples_progression_from_cn_4yrs.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progression from MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17813/1024663125.py:3: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
      "/tmp/ipykernel_17813/1024663125.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_mci_samples=all_mci_samples.append(ADNI_TEMP[ADNI_TEMP['DX_bl']=='LMCI'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MCI data size  1054\n",
      "samples progressed:  319\n",
      "samples not progressed:  386\n"
     ]
    }
   ],
   "source": [
    "# progression from MCI\n",
    "THRESHOLD_MONTH = 12*2\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month','DX_bl']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "all_mci_samples = ADNI_TEMP[ADNI_TEMP['DX_bl']=='EMCI']\n",
    "all_mci_samples=all_mci_samples.append(ADNI_TEMP[ADNI_TEMP['DX_bl']=='LMCI'])\n",
    "\n",
    "all_mci_samples = set(all_mci_samples['PTID'].unique())\n",
    "print(\"Baseline MCI data size \",all_mci_samples.__len__())\n",
    "samples_progressed= set()\n",
    "samples_not_progressed= set()\n",
    "for sample in all_mci_samples :\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if sample in usable_samples_ADNI:\n",
    "        if (last_dx == 'Dementia' ):\n",
    "                samples_progressed = samples_progressed.union({usable_samples_ADNI[sample]})\n",
    "        elif last_month_of_dx >= THRESHOLD_MONTH :\n",
    "                samples_not_progressed = samples_not_progressed.union({usable_samples_ADNI[sample]})\n",
    "print(\"samples progressed: \", len(samples_progressed))\n",
    "print(\"samples not progressed: \", len(samples_not_progressed))\n",
    "samples_progressed = [[x, 1] for x in samples_progressed]\n",
    "samples_not_progressed = [[x, 0] for x in samples_not_progressed]\n",
    "final_samples=samples_progressed+samples_not_progressed\n",
    "json.dump(final_samples, open('final_samples_progression_from_mci_2yrs.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29469/2147715290.py:4: DtypeWarning: Columns (19,20,21,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
      "/tmp/ipykernel_29469/2147715290.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_mci_samples=all_mci_samples.append(ADNI_TEMP[ADNI_TEMP['DX_bl']=='LMCI'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MCI data size  1054\n",
      "samples progressed:  319\n",
      "samples not progressed:  240\n"
     ]
    }
   ],
   "source": [
    "# progression from MCI\n",
    "import pandas as pd\n",
    "THRESHOLD_MONTH = 12*4\n",
    "ADNIMERGE = pd.read_csv('ADNIMERGE_NEW.csv')\n",
    "ADNI_TEMP = ADNIMERGE[['RID', 'PTID', 'DX', 'Month','DX_bl']].dropna().sort_values(by=['PTID', 'Month'])\n",
    "all_mci_samples = ADNI_TEMP[ADNI_TEMP['DX_bl']=='EMCI']\n",
    "all_mci_samples=all_mci_samples.append(ADNI_TEMP[ADNI_TEMP['DX_bl']=='LMCI'])\n",
    "\n",
    "all_mci_samples = set(all_mci_samples['PTID'].unique())\n",
    "print(\"Baseline MCI data size \",all_mci_samples.__len__())\n",
    "samples_progressed= set()\n",
    "samples_not_progressed= set()\n",
    "for sample in all_mci_samples :\n",
    "    last_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['DX'].iloc[-1]\n",
    "    last_month_of_dx = ADNI_TEMP[ADNI_TEMP['PTID']==sample]['Month'].iloc[-1]\n",
    "    if sample in usable_samples_ADNI:\n",
    "        if (last_dx == 'Dementia' ):\n",
    "                samples_progressed = samples_progressed.union({usable_samples_ADNI[sample]})\n",
    "        elif last_month_of_dx >= THRESHOLD_MONTH :\n",
    "                samples_not_progressed = samples_not_progressed.union({usable_samples_ADNI[sample]})\n",
    "print(\"samples progressed: \", len(samples_progressed))\n",
    "print(\"samples not progressed: \", len(samples_not_progressed))\n",
    "samples_progressed = [[x, 1] for x in samples_progressed]\n",
    "samples_not_progressed = [[x, 0] for x in samples_not_progressed]\n",
    "final_samples=samples_progressed+samples_not_progressed\n",
    "json.dump(final_samples, open('final_samples_progression_from_mci_4yrs.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad_venv_2",
   "language": "python",
   "name": "ad_venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
