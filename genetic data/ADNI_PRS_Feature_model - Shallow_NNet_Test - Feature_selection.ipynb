{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "feature_indices_to_consider = list(range(23)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "\n",
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=32, drop_probab=.8):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "#         self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "#         self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         self.num_hidden = num_hidden\n",
    "        self.drop_probab = drop_probab\n",
    "        self.dropout = nn.functional.dropout\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "#         for i in range(self.num_hidden):\n",
    "#             features = self.fc_hidden[i](features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['018_S_0633', '1'],\n",
       "        ['037_S_0467', '1'],\n",
       "        ['128_S_0517', '1'],\n",
       "        ...,\n",
       "        ['099_S_4202', '0'],\n",
       "        ['072_S_4445', '0'],\n",
       "        ['029_S_2395', '0']], dtype='<U10'),\n",
       " (10, 74, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "negative_samples = Final_Samples[370:]\n",
    "random.seed(7)\n",
    "random.shuffle(negative_samples)\n",
    "Final_Samples = Final_Samples[:370] + negative_samples[:370]\n",
    "len([x[1] for x in Final_Samples if x[1] == 1]), len([x[1] for x in Final_Samples if x[1] == 0])\n",
    "Final_Samples = np.array(Final_Samples)\n",
    "Final_Samples, Final_Samples.reshape(10, -1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_model(\n",
       "  (fc1): Linear(in_features=18, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (outLayer): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model(num_features=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class dataSet(data.Dataset):\n",
    "    def __init__(self, Final_Samples, feature_matrix, usable_samples_ADNI, feature_indices_to_consider=feature_indices_to_consider):\n",
    "        super(dataSet, self).__init__()  \n",
    "        self.data_len = len(Final_Samples)\n",
    "        self.usable_samples_ADNI = usable_samples_ADNI\n",
    "        self.Final_Samples = Final_Samples\n",
    "        self.feature_indices_to_consider = feature_indices_to_consider\n",
    "        self.feature_matrix = feature_matrix[:, self.feature_indices_to_consider]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        features = torch.from_numpy(self.feature_matrix[self.usable_samples_ADNI[self.Final_Samples[index][0]]]).float()\n",
    "        label = torch.tensor([float(self.Final_Samples[index][1])]).float()\n",
    "        return features, label\n",
    "    \n",
    "    def update_prs_features(self, mean, std):\n",
    "        self.feature_matrix = (self.feature_matrix - mean) / std\n",
    "        \n",
    "    def get_mean_std(self):\n",
    "        mean = self.feature_matrix.mean(0)\n",
    "        std = self.feature_matrix.std(0)\n",
    "        return mean, std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 23)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final_Samples\n",
    "usable_samples_ADNI = json.load(open('./usable_samples_ADNI.json'))\n",
    "PRS_feature_matrix = np.load('./PRS_feature_matrix.npy')\n",
    "\n",
    "def random_samples(total_folds, random_seed=None):\n",
    "    Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "    negative_samples = Final_Samples[370:]\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 2)\n",
    "    random.shuffle(negative_samples)\n",
    "    Final_Samples = Final_Samples[:370] + negative_samples[:370]   \n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed)\n",
    "    random.shuffle(Final_Samples)\n",
    "    Final_Samples = np.array(Final_Samples)\n",
    "    N_splits = Final_Samples.reshape(total_folds, -1, 2)\n",
    "    return N_splits\n",
    "\n",
    "def generate_datasets(N_splits, fold_num):\n",
    "    test_samples = N_splits[fold_num:fold_num+1].reshape([-1, 2])\n",
    "    train_samples = np.concatenate([N_splits[0:fold_num],N_splits[fold_num+1:]], 0).reshape([-1, 2])\n",
    "    train_set = dataSet(Final_Samples=train_samples, \n",
    "                        feature_matrix=PRS_feature_matrix, \n",
    "                        usable_samples_ADNI=usable_samples_ADNI)\n",
    "    val_set = dataSet(Final_Samples=test_samples, \n",
    "                      feature_matrix=PRS_feature_matrix, \n",
    "                      usable_samples_ADNI=usable_samples_ADNI)\n",
    "    mean, std = train_set.get_mean_std()\n",
    "    train_set.update_prs_features(mean, std)\n",
    "    val_set.update_prs_features(mean, std)\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def generate_loader(train_set, val_set, num_workers):\n",
    "    train_batch_size = train_set.__len__()\n",
    "    val_batch_size = val_set.__len__()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                              batch_size=train_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_set, val_set = generate_datasets(N_splits=random_samples(total_folds=37), fold_num=0)\n",
    "val_set.feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "#         print(features.shape, label.shape)\n",
    "        probab = model(features)\n",
    "    \n",
    "        if is_training:  \n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "    return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NH:18\n",
      "\n",
      "#F37\n",
      "random_seed:2: 0.7351351351351351 0.07610879913691203\n",
      "random_seed:6: 0.7554054054054056 0.06856027510251797\n",
      "random_seed:108: 0.7662162162162163 0.0726469192200844\n",
      "random_seed:90: 0.7513513513513514 0.07840167407762318\n",
      "random_seed:5: 0.7554054054054052 0.06856027510251797\n",
      "global_best_acc_val:0.95\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "usable_indices = [usable_samples_ADNI[Final_Samples[i][0]] for i in range(len(Final_Samples))]\n",
    "usable_features = PRS_feature_matrix[usable_indices][:, feature_indices_to_consider]\n",
    "usable_labels = np.array([float(Final_Samples[i][1]) for i in range(len(Final_Samples))])\n",
    "usable_features = torch.autograd.Variable(torch.from_numpy(usable_features)).to(DEVICE).float()\n",
    "\n",
    "GENERATE_SHAP = True\n",
    "total_epochs = 100 #150 \n",
    "num_features_list = [usable_features.shape[1]]\n",
    "random_integers = [2, 6, 108, 90, 5]\n",
    "folds_list = [37]#[37*2]\n",
    "\n",
    "shap_values_list = []\n",
    "for num_features in num_features_list:\n",
    "    print(f'NH:{num_features}')\n",
    "    global_best_acc_val = 0.\n",
    "    for total_folds in folds_list:\n",
    "        print(f'\\n#F{total_folds}')\n",
    "        for random_seed in random_integers:\n",
    "            N_splits = random_samples(total_folds=total_folds, random_seed=random_seed)\n",
    "            accuracies = []\n",
    "            temp_shap_values = np.zeros(usable_features.shape)\n",
    "            for fold_num in range(total_folds):\n",
    "    #             print(f'fold-{fold_num}:')\n",
    "                train_set, val_set = generate_datasets(N_splits=N_splits, fold_num=fold_num)\n",
    "                train_loader, val_loader = generate_loader(train_set=train_set, val_set=val_set, num_workers=0)\n",
    "                model = simple_model(num_features=usable_features.shape[1], hidden_dim=32, drop_probab=.8)\n",
    "                model = model.to(DEVICE)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                criterion = torch.nn.BCEWithLogitsLoss() \n",
    "                best_acc_val = 0.\n",
    "                model_best = None\n",
    "                for epoch_num in range(total_epochs):\n",
    "                    model.train()\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=True, \n",
    "                                                                                             loader=train_loader)\n",
    "                    model.eval()\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=False, \n",
    "                                                                                             loader=val_loader)\n",
    "                    if acc_val > best_acc_val:\n",
    "                        best_acc_val = acc_val\n",
    "                        if acc_val > global_best_acc_val:\n",
    "                            global_best_acc_val = acc_val\n",
    "    #                         print('global updated!')\n",
    "                        model_best = deepcopy(model)\n",
    "    #                     print(f'#F:{total_folds}| seed:{random_seed}, fold:{fold_num}, epoch:{epoch_num} -> local:{best_acc_val}, global:{global_best_acc_val}')  \n",
    "                    if epoch_num + 1 == total_epochs:\n",
    "    #                     print(f'LAST_Epoch:{epoch_num}, train_acc:{acc_train}, val_acc:{acc_val}, local_best:{best_acc_val}, global_best:{global_best_acc_val}')\n",
    "                        pass\n",
    "                accuracies += [best_acc_val]\n",
    "#                 print(fold_num, ':', accuracies)\n",
    "                if GENERATE_SHAP:\n",
    "                    explainer = shap.GradientExplainer(model_best.to(DEVICE), usable_features,\n",
    "                                                       batch_size=usable_features.shape[0]) #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "                    shap_values = explainer.shap_values(usable_features, nsamples=100)\n",
    "                    temp_shap_values += shap_values\n",
    "            if GENERATE_SHAP:\n",
    "                temp_shap_values /= total_folds\n",
    "                shap_values_list += [temp_shap_values] \n",
    "            print(f'random_seed:{random_seed}:', np.mean(accuracies), np.std(accuracies))\n",
    "            \n",
    "    print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "usable_features = usable_features.cpu().detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aa4ee3570470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0musable_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musable_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_list[0].shape\n",
    "\n",
    "import pickle\n",
    "pickle.dump(shap_values_list, open('shap_values_list.pkl', 'wb'))\n",
    "shap_values = np.sum(shap_values_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap_values_list\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "shap.force_plot(.5, shap_values[0,:], usable_features[0, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(.5, shap_values[:,:], usable_features[:, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traits[20]\n",
    "shap_values = np.sum(shap_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GWAS_IDS = ['ieu-b-109', 'ukb-b-12064', 'ukb-b-13806', 'ukb-d-20405_0', 'ieu-b-38', 'ukb-b-6134', 'ieu-b-110', 'ukb-b-17627', 'ukb-b-19953', 'ukb-b-8476', 'ukb-d-20405_1', 'ukb-d-20405_2', 'ukb-b-2209', 'ukb-b-4424', 'ukb-b-7663', 'ukb-b-18275', 'ukb-b-770', 'met-d-Total_C', 'ieu-b-25', 'ieu-b-111', 'ukb-b-3957', 'ieu-b-39', 'ukb-b-6324']\n",
    "traits = ['HDL cholesterol', 'Non-cancer illness code, self-reported: depression', 'Non-cancer illness code, self-reported: type 2 diabetes', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: No', 'systolic blood pressure', 'Age completed full time education', 'LDL cholesterol', 'Non-oily fish intake', 'Body mass index (BMI)', 'Loneliness, isolation', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, but not in the last year', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, during the last year', 'Oily fish intake', 'Sleep duration', 'Types of physical activity in last 4 weeks: Strenuous sports', 'Hearing difficulty/problems with background noise', 'Other meat intake', 'Total cholesterol', 'Cigarettes per Day', 'triglycerides', 'Sleeplessness / insomnia', 'diastolic blood pressure', 'Processed meat intake']\n",
    "\n",
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 100\n",
    "# #F37\n",
    "# random_seed:2: 0.7743243243243244 0.0776527453976143\n",
    "# random_seed:6: 0.7418918918918919 0.05636933409355193\n",
    "# random_seed:108: 0.7648648648648649 0.06136385156617693\n",
    "# random_seed:90: 0.7527027027027028 0.0646674925081373\n",
    "# random_seed:5: 0.7648648648648649 0.06352788796509815\n",
    "# global_best_acc_val:0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 100\n",
    "# #F74\n",
    "# random_seed:2: 0.845945945945946 0.09752226244685794\n",
    "# random_seed:6: 0.8405405405405405 0.1064398306352174\n",
    "# random_seed:108: 0.8472972972972973 0.07749973491318739\n",
    "# random_seed:90: 0.8391891891891893 0.08513513513513514\n",
    "# random_seed:5: 0.8527027027027028 0.07017925498702046\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 100\n",
    "# #F74\n",
    "# random_seed:2: 0.677027027027027 0.144771298948616\n",
    "# random_seed:6: 0.677027027027027 0.13208033724122614\n",
    "# random_seed:108: 0.6716216216216216 0.14843350524208282\n",
    "# random_seed:90: 0.6783783783783782 0.13682724387942852\n",
    "# random_seed:5: 0.6500000000000001 0.13177581289775778\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 200\n",
    "# #F74\n",
    "# random_seed:2: 0.6716216216216216 0.12683280148488987\n",
    "# random_seed:6: 0.6972972972972974 0.13653330746286582\n",
    "# random_seed:108: 0.6756756756756758 0.12925024001317129\n",
    "# random_seed:90: 0.681081081081081 0.12910887472968852\n",
    "# random_seed:5: 0.681081081081081 0.12156154672458931\n",
    "# global_best_acc_val:1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# # Learning rate: 0.001, epochs 150\n",
    "# #F74\n",
    "# random_seed:2: 0.6864864864864865 0.14825500704759437\n",
    "# random_seed:6: 0.6945945945945947 0.13545907120153156\n",
    "# random_seed:108: 0.7027027027027029 0.1173732473005863\n",
    "# random_seed:90: 0.6783783783783784 0.1265661564753308\n",
    "# random_seed:5: 0.6918918918918918 0.13126902102249613\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# # Learning rate: 0.001, epochs 100\n",
    "# #F74\n",
    "# random_seed:2: 0.6824324324324323 0.13592337677361732\n",
    "# random_seed:6: 0.7040540540540541 0.12886111231612285\n",
    "# random_seed:108: 0.672972972972973 0.12975787982873305\n",
    "# random_seed:90: 0.6716216216216215 0.10593250052576224\n",
    "# random_seed:5: 0.677027027027027 0.12030063388354634\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 2\n",
    "# #F74\n",
    "# random_seed:2: 0.677027027027027 0.1466262921164212\n",
    "# random_seed:6: 0.6756756756756755 0.14594594594594595\n",
    "# random_seed:108: 0.6837837837837837 0.13658679749722463\n",
    "# random_seed:90: 0.6783783783783784 0.1378113382052104\n",
    "# random_seed:5: 0.6756756756756758 0.13336680855649005\n",
    "# global_best_acc_val:1.0\n",
    "\n",
    "# Hidden Layers: 1\n",
    "# #F74\n",
    "# random_seed:2: 0.6445945945945946 0.14059900031543793\n",
    "# random_seed:6: 0.6702702702702703 0.138261141215445\n",
    "# random_seed:108: 0.6270270270270272 0.14265560238266645\n",
    "# random_seed:90: 0.6486486486486486 0.11768400571393105\n",
    "# random_seed:5: 0.6378378378378377 0.13426748484728887\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
