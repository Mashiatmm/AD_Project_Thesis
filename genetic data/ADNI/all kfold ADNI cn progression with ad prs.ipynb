{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the input dataset :\n",
    "\n",
    "1. usable_samples_ADNI.json : stores the IID (index) for each row of PRS_feature_matrix.npy\n",
    "2. PRS_feature_matrix.npy : PR Score for different features\n",
    "3. Covar_FILE_bigger_dataset : for reading covar such as age, gender\n",
    "4. Final_Samples.json : contains ID and output for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tpot\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs = '4yrs'\n",
    "model_suffix=\"_cn_progression\"\n",
    "dataset_suffix=\"from_cn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covar for ADNI Plink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter parameters :\n",
    "    1. Number of features\n",
    "    2. Number of Hidden Layers \n",
    "    3. Dimension of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = 52\n",
    "hidden = 4\n",
    "hidden_dimension = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indices of features to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_in_file: saves model accuracy in a text file\n",
    "#     args : model_name : name of model with layers and dimensions\n",
    "#            accuracy : accuracy  score\n",
    "def save_in_file(model_name, accuracy):\n",
    "    model_file = open(\"model_details.txt\",\"a\")\n",
    "    model_file.write(model_name + \" -> accuracy : \" + str(accuracy) + \"\\n\" )\n",
    "    model_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications**\n",
    "1. Added relu in the hidden layers and sigmoid in the output layer as activation functions\n",
    "2. Added dropout in the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim= hidden_dimension, drop_probab=.5):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        num_hidden = hidden\n",
    "        hidden_dim = hidden_dimension\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "#         self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.drop_probab = drop_probab\n",
    "        self.dropout = nn.functional.dropout\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        for i in range(self.num_hidden):\n",
    "            features = self.fc_hidden[i](features)\n",
    "            # added by Mashiat\n",
    "            features = self.dropout(features, p=self.drop_probab)\n",
    "            features = self.relu( features )\n",
    "            ####################\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Pandas Dataframe to Dataset class\n",
    "\n",
    "overriding the constructor, getitem, len function of the original class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class df_dataSet(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.features = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y.values, dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch function : runs an epoch of a model\n",
    "#                 args :\n",
    "#                         model : neural network model\n",
    "#                         optimizer :\n",
    "#                         criterion :\n",
    "#                         is_training : train - true or test - false\n",
    "#                         loader : torch dataset\n",
    "#                 returns :\n",
    "#                         different accuracy score for the dataset of per epoch\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "#     print(loader)\n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "        label = torch.reshape(label, (label.shape[0], 1))\n",
    "        probab = model(features)\n",
    "        if is_training:  \n",
    "#             print(probab.shape, label.shape)\n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "    return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "#     return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**epoch function for LOOCV**\n",
    "\n",
    "Without precision, recall, ROC, AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and column priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = pd.read_csv('adni_shuffled_balanced_' + yrs + '_'+dataset_suffix+'.csv')\n",
    "\n",
    "# Read from the output file and store keys in a list\n",
    "input_file = \"column_importance_ADNI\"+dataset_suffix+\".txt\"\n",
    "col_imp = []\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        key = line.strip()  # Remove newline character\n",
    "        col_imp.append(key)\n",
    "\n",
    "# print(\"Keys read from the file:\", col_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Column Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['Non-cancer illness code, self-reported: type 2 diabetes', 'Cigarettes per Day', 'Age completed full time education', 'Non-cancer illness code, self-reported: depression', \"Illnesses of mother: Alzheimer's disease/dementia\"]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"ADNI_feature_count.txt\", \"r\")\n",
    "count = f.read()\n",
    "print( int(count) )\n",
    "cols_to_take = int(count)\n",
    "selected_col = col_imp[:cols_to_take]\n",
    "print(selected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Non-cancer illness code, self-reported: type 2 diabetes', 'Cigarettes per Day', 'Age completed full time education', 'Non-cancer illness code, self-reported: depression', \"Illnesses of mother: Alzheimer's disease/dementia\", \"Alzheimer's disease\", 'output']\n"
     ]
    }
   ],
   "source": [
    "if 'Alzheimer\\'s Disease' not in selected_col:\n",
    "    selected_col.append('Alzheimer\\'s disease' )\n",
    "\n",
    "selected_col.append( 'output' )\n",
    "print(selected_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 7)\n"
     ]
    }
   ],
   "source": [
    "shuffled = shuffled[selected_col]\n",
    "\n",
    "print( shuffled.shape )\n",
    "# print( shuffled.head() )\n",
    "\n",
    "# dropping last / output column in df\n",
    "shuffled_X = shuffled.iloc[: , :-1]\n",
    "shuffled_Y =  shuffled.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc):\n",
    "    print(\"accuracy:\",avg_acc)\n",
    "    print(\"precision:\",avg_prec)\n",
    "    print(\"recall:\",avg_rec)\n",
    "    print(\"fscore:\",avg_fsc)\n",
    "    print(\"auroc:\",avg_roc)\n",
    "    print(\"auprc:\",avg_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# CSV file path\n",
    "def write_out_to_csv(datatype, model, years, acc, prec, rec, auprc, auroc, fscore):\n",
    "    csv_file_path = \"scores.csv\"\n",
    "\n",
    "    # Check if the CSV file already exists\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        # Create a new CSV file and write header\n",
    "        with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([\"Datatype\", \"Model\", \"Year\", \"Size\", \"Features\",\"Average Accuracy\", \"Average Precision\", \"Average Recall\", \"Average F-Score\", \"Average ROC AUC\", \"Average PR AUC\"])\n",
    "\n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([datatype, model, years, shuffled.shape[0], shuffled_X.shape[1], acc, prec, rec, fscore, auroc, auprc])\n",
    "\n",
    "    print(\"Average scores have been appended to the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/09/a-comprehensive-guide-on-neural-networks-performance-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF:6\n",
      "\n",
      "#F10\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "random_seed:42: 0.6243589743589744 0.08620305371289354 0.7967948717948719 0.0787209763052622 train acc: 0.7413793103448276\n",
      "accuraacies of validation:  [0.6923076923076923, 0.9230769230769231, 0.7692307692307693, 0.7692307692307693, 0.8461538461538461, 0.7692307692307693, 0.6923076923076923, 0.9230769230769231, 0.8333333333333334, 0.75]\n",
      "global_best_acc_val:0.9230769230769231\n",
      "precision avg :  0.4912820512820513\n",
      "recall avg :  0.6375\n",
      "AUPRC avg :  0.6090424697106245\n",
      "AUROC avg :  0.5355158730158729\n",
      "FScore avg :  0.5313419913419913\n",
      "accuracy: [0.79679487]\n",
      "precision: 0.4912820512820513\n",
      "recall: 0.6375\n",
      "fscore: 0.5313419913419913\n",
      "auroc: 0.5355158730158729\n",
      "auprc: 0.6090424697106245\n",
      "Average scores have been appended to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "GENERATE_SHAP = True\n",
    "total_epochs = 500 #250(ideal)\n",
    "num_features = shuffled_X.shape[1]\n",
    "# random_integers = [2, 6, 108, 90, 5]\n",
    "random_seed = random_seed#, 92, 0, 87, 73, 82, 54]\n",
    "\n",
    "total_folds = 10#[37*2]\n",
    "\n",
    "avg_val_acc = []\n",
    "\n",
    "shap_values_list = []\n",
    "# for num_features in num_features_list:\n",
    "print(f'NF:{num_features}')\n",
    "global_best_acc_val = 0.\n",
    "precision_avg = 0\n",
    "recall_avg = 0\n",
    "auprc_avg = 0\n",
    "auroc_avg = 0\n",
    "fscore_avg = 0\n",
    "print(f'\\n#F{total_folds}')\n",
    "accuracies = []\n",
    "accuracies_val = []\n",
    "temp_shap_values = np.zeros(shuffled_X.shape)\n",
    "\n",
    "kf = KFold(n_splits = total_folds, random_state=None)\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    print(\"Split :\")\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "\n",
    "    train_dataset = df_dataSet( X_train, y_train )\n",
    "    valid_dataset = df_dataSet( X_test, y_test )\n",
    "\n",
    "    train_batch_size = train_dataset.__len__()\n",
    "    val_batch_size = valid_dataset.__len__()\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = False, num_workers = 0)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = val_batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "    model = simple_model(num_features = shuffled_X.shape[1], hidden_dim = hidden_dimension)\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss() \n",
    "    best_acc_val = 0.\n",
    "    model_best = None\n",
    "\n",
    "    for epoch_num in range(total_epochs):\n",
    "        model.train()\n",
    "        precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                 criterion=criterion, is_training=True, \n",
    "                                                                               loader=train_loader)\n",
    "        model.eval()\n",
    "        precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, \n",
    "                                                                                 optimizer=optimizer, \n",
    "                                                                                 criterion=criterion, is_training=False, \n",
    "                                                                                loader=valid_loader)\n",
    "        if acc_val > best_acc_val:\n",
    "            best_acc_val = acc_val\n",
    "            if acc_val > global_best_acc_val:\n",
    "                global_best_acc_val = acc_val\n",
    "\n",
    "            torch.save(model.state_dict(), 'PRS_model'+model_suffix+'.pt')\n",
    "\n",
    "    model_best = simple_model(num_features= shuffled_X.shape[1], hidden_dim = hidden_dimension, drop_probab=.0)\n",
    "    model_best.load_state_dict(torch.load('PRS_model'+model_suffix+'.pt'))\n",
    "    model_best = model_best.to(DEVICE)\n",
    "    model_best.eval()\n",
    "    precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true = epoch(model=model_best, \n",
    "                                                                             optimizer=optimizer, \n",
    "                                                                             criterion=criterion, is_training=False, \n",
    "                                                                             loader=valid_loader)\n",
    "    accuracies += [acc_test]\n",
    "    accuracies_val += [best_acc_val]\n",
    "#                 print(\"precision : \", precision, \" ; recall : \", recall)\n",
    "    precision_avg += precision\n",
    "    recall_avg += recall\n",
    "    auprc_avg += auprc\n",
    "    auroc_avg += auroc\n",
    "    fscore_avg += fscore\n",
    "\n",
    "\n",
    "    print(total_folds, ':')\n",
    "\n",
    "print(f'random_seed:{random_seed}:', np.mean(accuracies), np.std(accuracies), \n",
    "      np.mean(accuracies_val), np.std(accuracies_val), 'train acc:', acc_train)\n",
    "avg_val_acc += [np.mean(accuracies_val)]\n",
    "print(\"accuraacies of validation: \", accuracies_val)\n",
    "print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "precision_avg = precision_avg * 1.0 / total_folds\n",
    "recall_avg = recall_avg * 1.0 / total_folds\n",
    "auprc_avg = auprc_avg * 1.0 / total_folds\n",
    "auroc_avg = auroc_avg * 1.0 / total_folds\n",
    "fscore_avg = fscore_avg * 1.0 / total_folds\n",
    "print( \"precision avg : \", precision_avg )\n",
    "print( \"recall avg : \", recall_avg )\n",
    "print( \"AUPRC avg : \", auprc_avg )\n",
    "print( \"AUROC avg : \", auroc_avg )\n",
    "print( \"FScore avg : \", fscore_avg )\n",
    "avg_val_acc = np.array(avg_val_acc)\n",
    "printScores(avg_val_acc,precision_avg,recall_avg,fscore_avg,auroc_avg,auprc_avg)\n",
    "write_out_to_csv(\"ADNI\", \"NN\", yrs, avg_val_acc, precision_avg,recall_avg,fscore_avg,auroc_avg,auprc_avg)\n",
    "\n",
    " \n",
    "\n",
    "# # usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n",
    "\n",
    "# # print(avg_val_acc.max(), avg_val_acc.min(), avg_val_acc.mean(), avg_val_acc.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision, recall, fscore, auroc, auprc,accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScores(true,pred_binary,pred):\n",
    "    print(true.shape,pred_binary.shape,pred.shape)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "\n",
    "\n",
    "#     print(\"precision\",precision[1],\"recall\", recall[1], \"fscore\",fscore[1], \"auroc\", auroc,\"auprc\", auprc,\"accuracy\" ,acc)\n",
    "    return acc, precision[1], recall[1], fscore, auroc, auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(12,) (12,) (12,)\n",
      "(12,) (12,) (12,)\n",
      "accuracy: 0.5397435897435898\n",
      "precision: 0.5085714285714286\n",
      "recall: 0.47095238095238096\n",
      "fscore: 0.5172213570742983\n",
      "auroc: 0.5201587301587302\n",
      "auprc: 0.5930983714257524\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1)\n",
    "# training_features, testing_features, training_target, testing_target = \\\n",
    "#             train_test_split(shuffled_X, shuffled_Y, random_state=1)\n",
    "# print(\"training_features\",X_train.shape)\n",
    "# print(\"testing_features\",X_test.shape)\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.6719885773624091\n",
    "exported_pipeline = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, max_features=1,\n",
    "                    min_samples_leaf=7, min_samples_split=6, n_estimators=100, random_state = random_seed )\n",
    "# Fix random state in exported estimator\n",
    "accuracies=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "fscores=[]\n",
    "aurocs=[]\n",
    "auprcs=[]\n",
    "if hasattr(exported_pipeline, 'random_state'):\n",
    "    setattr(exported_pipeline, 'random_state', random_seed)\n",
    "    \n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    y_pred = exported_pipeline.predict(X_test)\n",
    "\n",
    "    true=y_test\n",
    "    pred_binary=y_pred\n",
    "    pred=exported_pipeline.predict_proba(X_test).T\n",
    "    pred=pred[1]\n",
    "    acc,precision, recall,fscore, auroc, auprc=GetScores(true,pred_binary,pred) \n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "avg_acc=np.mean(accuracies)\n",
    "avg_prec=np.mean(precisions)\n",
    "avg_rec=np.mean(recalls)\n",
    "avg_fsc=np.mean(fscores)\n",
    "avg_roc=np.mean(aurocs)\n",
    "avg_prc=np.mean(auprcs)\n",
    "printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores have been appended to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "write_out_to_csv(\"ADNI\", \"XGBoost\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(12,) (12,) (12,)\n",
      "(12,) (12,) (12,)\n",
      "accuracy: 0.5942307692307692\n",
      "precision: 0.6112698412698412\n",
      "recall: 0.5792460317460317\n",
      "fscore: 0.5809796494355318\n",
      "auroc: 0.6222321428571428\n",
      "auprc: 0.6087364977394738\n",
      "Average scores have been appended to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(shuffled_X, shuffled_Y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Preprocess the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# Create an SVM object\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear', random_state=random_seed, probability=True)\n",
    "\n",
    "\n",
    "accuracies=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "fscores=[]\n",
    "aurocs=[]\n",
    "auprcs=[]\n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "   \n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "#     print(cm)\n",
    "\n",
    "    true=y_test\n",
    "    pred_binary=y_pred\n",
    "    pred=classifier.predict_proba(X_test).T\n",
    "    pred=pred[1]\n",
    "    acc,precision, recall,fscore, auroc, auprc=GetScores(true,pred_binary,pred)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "avg_acc=np.mean(accuracies)\n",
    "avg_prec=np.mean(precisions)\n",
    "avg_rec=np.mean(recalls)\n",
    "avg_fsc=np.mean(fscores)\n",
    "avg_roc=np.mean(aurocs)\n",
    "avg_prc=np.mean(auprcs)\n",
    "printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc)\n",
    "write_out_to_csv(\"ADNI\", \"SVM\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(13,) (13,) (13,)\n",
      "(12,) (12,) (12,)\n",
      "(12,) (12,) (12,)\n",
      "accuracy: 0.5391025641025642\n",
      "precision: 0.4704761904761905\n",
      "recall: 0.4575\n",
      "fscore: 0.50953969233381\n",
      "auroc: 0.5383333333333333\n",
      "auprc: 0.6037539528923457\n",
      "Average scores have been appended to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=random_seed)\n",
    "accuracies=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "fscores=[]\n",
    "aurocs=[]\n",
    "auprcs=[]\n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred_bin = rfc.predict(X_test)\n",
    "    y_pred_frac= rfc.predict_proba(X_test).T[1]\n",
    "    acc,precision, recall,fscore, auroc, auprc=GetScores(y_test,y_pred_bin,y_pred_frac)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "avg_acc=np.mean(accuracies)\n",
    "avg_prec=np.mean(precisions)\n",
    "avg_rec=np.mean(recalls)\n",
    "avg_fsc=np.mean(fscores)\n",
    "avg_roc=np.mean(aurocs)\n",
    "avg_prc=np.mean(auprcs)\n",
    "printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc)\n",
    "write_out_to_csv(\"ADNI\", \"RF\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad_venv_2",
   "language": "python",
   "name": "ad_venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
