{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the input dataset :\n",
    "\n",
    "1. usable_samples_ADNI.json : stores the IID (index) for each row of PRS_feature_matrix.npy\n",
    "2. PRS_feature_matrix.npy : PR Score for different features\n",
    "3. Covar_FILE_bigger_dataset : for reading covar such as age, gender\n",
    "4. Final_Samples.json : contains ID and output for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tpot\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skfeature-chappers\n",
    "# !pip install mlxtend\n",
    "# !pip --version\n",
    "# !pip install imblearn\n",
    "# !conda install -c conda-forge imbalanced-learn\n",
    "# conda install -c conda-forge tqdm\n",
    "# conda install mlxtend\n",
    "\n",
    "# conda config --add channels conda-forge\n",
    "# conda install hyperopt\n",
    "\n",
    "# conda config --add channels conda-forge\n",
    "# conda config --set channel_priority strict\n",
    "# conda install auto-sklearn\n",
    "# conda install -c conda-forge tpot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the installed packages with their versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/bayzid/anaconda3:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_anaconda_depends         2020.07                  py37_0  \r\n",
      "_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \r\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n",
      "_openmp_mutex             4.5                  2_kmp_llvm    conda-forge\r\n",
      "absl-py                   1.0.0                    pypi_0    pypi\r\n",
      "adjusttext                0.7.3                    pypi_0    pypi\r\n",
      "alabaster                 0.7.13             pyhd8ed1ab_0    conda-forge\r\n",
      "alignment-c               1.0.0                    pypi_0    pypi\r\n",
      "anaconda                  custom                   py37_1  \r\n",
      "anaconda-client           1.12.0             pyhd8ed1ab_0    conda-forge\r\n",
      "anaconda-navigator        2.1.0            py37h06a4308_0  \r\n",
      "anaconda-project          0.11.1             pyhd8ed1ab_0    conda-forge\r\n",
      "anyio                     3.7.1              pyhd8ed1ab_0    conda-forge\r\n",
      "argh                      0.27.2             pyhd8ed1ab_0    conda-forge\r\n",
      "argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge\r\n",
      "argon2-cffi-bindings      21.2.0           py37h540881e_2    conda-forge\r\n",
      "asn1crypto                1.5.1              pyhd8ed1ab_0    conda-forge\r\n",
      "astor                     0.8.1                    pypi_0    pypi\r\n",
      "astroid                   2.5.8            py37h89c1867_0    conda-forge\r\n",
      "astropy                   3.2.3            py37h516909a_0    conda-forge\r\n",
      "atomicwrites              1.4.1              pyhd8ed1ab_0    conda-forge\r\n",
      "attrs                     23.1.0             pyh71513ae_1    conda-forge\r\n",
      "autopep8                  1.5.5              pyh44b312d_0    conda-forge\r\n",
      "babel                     2.12.1             pyhd8ed1ab_1    conda-forge\r\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\n",
      "backports                 1.0                pyhd8ed1ab_3    conda-forge\r\n",
      "backports.functools_lru_cache 1.6.5              pyhd8ed1ab_0    conda-forge\r\n",
      "backports.shutil_get_terminal_size 1.0.0                      py_3    conda-forge\r\n",
      "backports.tempfile        1.0                        py_0    conda-forge\r\n",
      "backports.weakref         1.0.post1       pyhd8ed1ab_1003    conda-forge\r\n",
      "backports.zoneinfo        0.2.1            py37h540881e_5    conda-forge\r\n",
      "beautifulsoup4            4.12.2             pyha770c72_0    conda-forge\r\n",
      "bioinfokit                2.1.0                    pypi_0    pypi\r\n",
      "bitarray                  2.6.0            py37h540881e_0    conda-forge\r\n",
      "bkcharts                  0.2              py37h06a4308_1  \r\n",
      "black                     22.10.0          py37h89c1867_1    conda-forge\r\n",
      "blas                      2.3                    openblas    conda-forge\r\n",
      "bleach                    6.0.0              pyhd8ed1ab_0    conda-forge\r\n",
      "blosc                     1.21.4               h0f2a231_0    conda-forge\r\n",
      "bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge\r\n",
      "boto                      2.49.0                     py_0    conda-forge\r\n",
      "bottleneck                1.3.5            py37hda87dfa_0    conda-forge\r\n",
      "brotli                    1.0.9                h166bdaf_9    conda-forge\r\n",
      "brotli-bin                1.0.9                h166bdaf_9    conda-forge\r\n",
      "brotlipy                  0.7.0           py37h540881e_1004    conda-forge\r\n",
      "bzip2                     1.0.8                h7f98852_4    conda-forge\r\n",
      "c-ares                    1.19.1               hd590300_0    conda-forge\r\n",
      "ca-certificates           2023.7.22            hbcca054_0    conda-forge\r\n",
      "cached-property           1.5.2                hd8ed1ab_1    conda-forge\r\n",
      "cached_property           1.5.2              pyha770c72_1    conda-forge\r\n",
      "cairo                     1.16.0            h18b612c_1001    conda-forge\r\n",
      "capnproto                 0.10.2               h6239696_0    conda-forge\r\n",
      "certifi                   2023.7.22          pyhd8ed1ab_0    conda-forge\r\n",
      "cffi                      1.15.1           py37h74dc2b5_0  \r\n",
      "chardet                   5.0.0            py37h89c1867_0    conda-forge\r\n",
      "charset-normalizer        3.2.0              pyhd8ed1ab_0    conda-forge\r\n",
      "click                     8.1.3            py37h89c1867_0    conda-forge\r\n",
      "click-plugins             1.1.1                    pypi_0    pypi\r\n",
      "cligj                     0.7.2                    pypi_0    pypi\r\n",
      "cloudpickle               2.2.1              pyhd8ed1ab_0    conda-forge\r\n",
      "clyent                    1.2.2                      py_1    conda-forge\r\n",
      "colorama                  0.4.6              pyhd8ed1ab_0    conda-forge\r\n",
      "conda                     22.9.0           py37h89c1867_1    conda-forge\r\n",
      "conda-build               3.21.4           py37h89c1867_0    conda-forge\r\n",
      "conda-content-trust       0.1.3              pyhd8ed1ab_0    conda-forge\r\n",
      "conda-env                 2.6.0                         1  \r\n",
      "conda-pack                0.7.0              pyh6c4a22f_0    conda-forge\r\n",
      "conda-package-handling    1.9.0            py37h540881e_0    conda-forge\r\n",
      "conda-repo-cli            1.0.24           py37h06a4308_0  \r\n",
      "conda-token               0.4.0              pyhd3eb1b0_0  \r\n",
      "conda-verify              3.4.2                      py_1  \r\n",
      "contextlib2               21.6.0             pyhd8ed1ab_0    conda-forge\r\n",
      "coverage                  6.5.0            py37h540881e_0    conda-forge\r\n",
      "cryptography              37.0.4           py37h38fbfac_0    conda-forge\r\n",
      "cudatoolkit               10.2.89             hdec6ad0_12    conda-forge\r\n",
      "cudnn                     8.2.1.32             h2c0ae14_0    conda-forge\r\n",
      "curl                      7.86.0               h7bff187_1    conda-forge\r\n",
      "cycler                    0.11.0             pyhd8ed1ab_0    conda-forge\r\n",
      "cython                    0.29.32          py37hd23a5d3_0    conda-forge\r\n",
      "cytoolz                   0.12.0           py37h540881e_0    conda-forge\r\n",
      "dask                      2021.4.0           pyhd3eb1b0_0  \r\n",
      "dask-core                 2021.4.0           pyhd3eb1b0_0  \r\n",
      "dbus                      1.13.6               he372182_0    conda-forge\r\n",
      "debugpy                   1.6.3            py37hd23a5d3_0    conda-forge\r\n",
      "decorator                 5.1.1              pyhd8ed1ab_0    conda-forge\r\n",
      "defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge\r\n",
      "dendropy                  4.6.1                    pypi_0    pypi\r\n",
      "diff-match-patch          20230430           pyhd8ed1ab_0    conda-forge\r\n",
      "distributed               2021.4.1         py37h06a4308_0  \r\n",
      "docutils                  0.19             py37h89c1867_0    conda-forge\r\n",
      "entrypoints               0.4                pyhd8ed1ab_0    conda-forge\r\n",
      "enum34                    1.1.10           py37hc8dfbb8_2    conda-forge\r\n",
      "et_xmlfile                1.1.0              pyhd8ed1ab_0    conda-forge\r\n",
      "exceptiongroup            1.1.2              pyhd8ed1ab_0    conda-forge\r\n",
      "expat                     2.5.0                hcb278e6_1    conda-forge\r\n",
      "fastcache                 1.1.0            py37h540881e_4    conda-forge\r\n",
      "filelock                  3.12.2             pyhd8ed1ab_0    conda-forge\r\n",
      "fiona                     1.8.20                   pypi_0    pypi\r\n",
      "flake8                    3.8.4                      py_0    conda-forge\r\n",
      "flask                     2.2.3              pyhd8ed1ab_0    conda-forge\r\n",
      "flit-core                 3.9.0              pyhd8ed1ab_0    conda-forge\r\n",
      "fontconfig                2.14.2               h14ed4e7_0    conda-forge\r\n",
      "fonttools                 4.25.0             pyhd3eb1b0_0  \r\n",
      "freetype                  2.12.1               hca18f0e_1    conda-forge\r\n",
      "fribidi                   1.0.10               h36c2ea0_0    conda-forge\r\n",
      "fsspec                    2022.11.0          pyhd8ed1ab_0    conda-forge\r\n",
      "future                    0.18.2           py37h89c1867_5    conda-forge\r\n",
      "gast                      0.5.3                    pypi_0    pypi\r\n",
      "geopandas                 0.10.2                   pypi_0    pypi\r\n",
      "get_terminal_size         1.0.0                haa9412d_0  \r\n",
      "gevent                    21.1.2           py37h27cfd23_1  \r\n",
      "glib                      2.69.1               h4ff587b_1  \r\n",
      "glob2                     0.7                        py_0    conda-forge\r\n",
      "gmp                       6.2.1                h58526e2_0    conda-forge\r\n",
      "gmpy2                     2.1.2            py37h025e8b9_0    conda-forge\r\n",
      "google-pasta              0.2.0                    pypi_0    pypi\r\n",
      "graphite2                 1.3.14               h295c915_1  \r\n",
      "greenlet                  0.4.17           py37h5e8e339_2    conda-forge\r\n",
      "grpcio                    1.43.0                   pypi_0    pypi\r\n",
      "gsl                       2.7                  he838d99_0    conda-forge\r\n",
      "gst-plugins-base          1.14.0               hbbd80ab_1  \r\n",
      "gstreamer                 1.14.1               h5eee18b_1  \r\n",
      "h5py                      3.7.0           nompi_py37hc97082c_100    conda-forge\r\n",
      "harfbuzz                  2.8.0                h6f93f22_0  \r\n",
      "hdf5                      1.12.1               h69dfa17_1  \r\n",
      "heapdict                  1.0.1                      py_0    conda-forge\r\n",
      "helpdev                   0.7.1              pyhd8ed1ab_0    conda-forge\r\n",
      "html5lib                  1.1                pyh9f0ad1d_0    conda-forge\r\n",
      "huggingface-hub           0.10.1                   pypi_0    pypi\r\n",
      "hypothesis                6.56.3           py37h89c1867_1    conda-forge\r\n",
      "icu                       58.2              hf484d3e_1000    conda-forge\r\n",
      "idna                      3.4                pyhd8ed1ab_0    conda-forge\r\n",
      "ijson                     3.1.3              pyhd3deb0d_0    conda-forge\r\n",
      "imagecodecs-lite          2019.12.3        py37hc105733_5    conda-forge\r\n",
      "imageio                   2.31.1             pyh24c5eb1_0    conda-forge\r\n",
      "imagesize                 1.4.1              pyhd8ed1ab_0    conda-forge\r\n",
      "importlib-metadata        4.10.0                   pypi_0    pypi\r\n",
      "importlib_metadata        4.11.4               hd8ed1ab_0    conda-forge\r\n",
      "importlib_resources       6.0.0              pyhd8ed1ab_0    conda-forge\r\n",
      "iniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge\r\n",
      "intel-openmp              2022.1.0          h9e868ea_3769  \r\n",
      "intervaltree              3.1.0              pyhd8ed1ab_1    conda-forge\r\n",
      "ipykernel                 6.15.0             pyh210e3f2_0    conda-forge\r\n",
      "ipython                   7.33.0           py37h89c1867_0    conda-forge\r\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\r\n",
      "ipywidgets                8.0.7              pyhd8ed1ab_0    conda-forge\r\n",
      "isort                     5.10.1             pyhd8ed1ab_0    conda-forge\r\n",
      "itsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge\r\n",
      "jbig                      2.1               h7f98852_2003    conda-forge\r\n",
      "jdcal                     1.4.1                      py_0    conda-forge\r\n",
      "jedi                      0.17.2           py37h89c1867_2    conda-forge\r\n",
      "jeepney                   0.8.0              pyhd8ed1ab_0    conda-forge\r\n",
      "jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge\r\n",
      "joblib                    1.3.0              pyhd8ed1ab_1    conda-forge\r\n",
      "jpeg                      9e                   h0b41bf4_3    conda-forge\r\n",
      "json5                     0.9.14             pyhd8ed1ab_0    conda-forge\r\n",
      "jsonschema                4.17.3             pyhd8ed1ab_0    conda-forge\r\n",
      "jupyter                   1.0.0            py37h89c1867_7    conda-forge\r\n",
      "jupyter_client            7.0.6              pyhd8ed1ab_0    conda-forge\r\n",
      "jupyter_console           6.4.4              pyhd8ed1ab_0    conda-forge\r\n",
      "jupyter_core              4.9.1            py37h89c1867_0    conda-forge\r\n",
      "jupyter_server            1.23.2             pyhd8ed1ab_0    conda-forge\r\n",
      "jupyterlab                3.5.3              pyhd8ed1ab_0    conda-forge\r\n",
      "jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge\r\n",
      "jupyterlab_server         2.24.0             pyhd8ed1ab_0    conda-forge\r\n",
      "jupyterlab_widgets        3.0.8              pyhd8ed1ab_0    conda-forge\r\n",
      "keras-applications        1.0.8                    pypi_0    pypi\r\n",
      "keras-preprocessing       1.1.2                    pypi_0    pypi\r\n",
      "keyring                   23.2.1           py37h89c1867_0    conda-forge\r\n",
      "keyutils                  1.6.1                h166bdaf_0    conda-forge\r\n",
      "kiwisolver                1.4.4            py37h7cecad7_0    conda-forge\r\n",
      "kmer-jellyfish            2.3.0                h9f5acd7_3    bioconda\r\n",
      "krb5                      1.19.3               h3790be6_0    conda-forge\r\n",
      "lazy-object-proxy         1.6.0            py37h27cfd23_0  \r\n",
      "lcms2                     2.14                 h6ed2654_0    conda-forge\r\n",
      "ld_impl_linux-64          2.40                 h41732ed_0    conda-forge\r\n",
      "lerc                      4.0.0                h27087fc_0    conda-forge\r\n",
      "libarchive                3.5.2                hb890918_3    conda-forge\r\n",
      "libblas                   3.9.0                3_openblas    conda-forge\r\n",
      "libbrotlicommon           1.0.9                h166bdaf_9    conda-forge\r\n",
      "libbrotlidec              1.0.9                h166bdaf_9    conda-forge\r\n",
      "libbrotlienc              1.0.9                h166bdaf_9    conda-forge\r\n",
      "libcblas                  3.9.0                3_openblas    conda-forge\r\n",
      "libcurl                   7.86.0               h7bff187_1    conda-forge\r\n",
      "libdeflate                1.14                 h166bdaf_0    conda-forge\r\n",
      "libedit                   3.1.20191231         he28a2e2_2    conda-forge\r\n",
      "libev                     4.33                 h516909a_1    conda-forge\r\n",
      "libexpat                  2.5.0                hcb278e6_1    conda-forge\r\n",
      "libffi                    3.3                  h58526e2_2    conda-forge\r\n",
      "libgcc-ng                 13.1.0               he5830b7_0    conda-forge\r\n",
      "libgfortran-ng            7.5.0               h14aa051_20    conda-forge\r\n",
      "libgfortran4              7.5.0               h14aa051_20    conda-forge\r\n",
      "libiconv                  1.17                 h166bdaf_0    conda-forge\r\n",
      "liblapack                 3.9.0                3_openblas    conda-forge\r\n",
      "liblapacke                3.9.0                3_openblas    conda-forge\r\n",
      "liblief                   0.12.3               h6a678d5_0  \r\n",
      "libllvm11                 11.1.0               he0ac6c6_5    conda-forge\r\n",
      "libllvm9                  9.0.1           default_hc23dcda_7    conda-forge\r\n",
      "libnghttp2                1.47.0               hdcd2b5c_1    conda-forge\r\n",
      "libopenblas               0.3.12          pthreads_hb3c22a3_1    conda-forge\r\n",
      "libpng                    1.6.39               h753d276_0    conda-forge\r\n",
      "libsodium                 1.0.18               h36c2ea0_1    conda-forge\r\n",
      "libspatialindex           1.9.3                h9c3ff4c_4    conda-forge\r\n",
      "libsqlite                 3.42.0               h2797004_0    conda-forge\r\n",
      "libssh2                   1.10.0               haa6b8db_3    conda-forge\r\n",
      "libstdcxx-ng              13.1.0               hfd8a6a1_0    conda-forge\r\n",
      "libtiff                   4.4.0                h82bc61c_5    conda-forge\r\n",
      "libtool                   2.4.7                h27087fc_0    conda-forge\r\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\r\n",
      "libuv                     1.44.2               h166bdaf_0    conda-forge\r\n",
      "libwebp-base              1.3.1                hd590300_0    conda-forge\r\n",
      "libxcb                    1.13              h7f98852_1004    conda-forge\r\n",
      "libxml2                   2.9.14               h74e7548_0  \r\n",
      "libxslt                   1.1.35               h4e12654_0  \r\n",
      "libzlib                   1.2.13               hd590300_5    conda-forge\r\n",
      "llvm-openmp               16.0.6               h4dfa4b3_0    conda-forge\r\n",
      "llvmlite                  0.39.1           py37h0761922_0    conda-forge\r\n",
      "locket                    1.0.0              pyhd8ed1ab_0    conda-forge\r\n",
      "lxml                      4.9.1            py37h540881e_0    conda-forge\r\n",
      "lz4-c                     1.9.4                hcb278e6_0    conda-forge\r\n",
      "lzo                       2.10              h516909a_1000    conda-forge\r\n",
      "markdown                  3.3.6                    pypi_0    pypi\r\n",
      "markupsafe                2.1.1            py37h540881e_1    conda-forge\r\n",
      "mash                      2.3                  hd3113c8_6    bioconda\r\n",
      "matplotlib                3.5.3            py37h89c1867_2    conda-forge\r\n",
      "matplotlib-base           3.5.3            py37hf395dca_2    conda-forge\r\n",
      "matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge\r\n",
      "matplotlib-venn           0.11.7                   pypi_0    pypi\r\n",
      "mccabe                    0.6.1                      py_1    conda-forge\r\n",
      "mistune                   2.0.5              pyhd8ed1ab_0    conda-forge\r\n",
      "mkl                       2022.2.1         h84fe81f_16997    conda-forge\r\n",
      "mkl-service               2.4.0            py37h94185c7_0    conda-forge\r\n",
      "mkl_fft                   1.3.1            py37h90e98c2_3    conda-forge\r\n",
      "mkl_random                1.2.2            py37h693438c_1    conda-forge\r\n",
      "mock                      5.1.0              pyhd8ed1ab_0    conda-forge\r\n",
      "more-itertools            10.0.0             pyhd8ed1ab_0    conda-forge\r\n",
      "mpc                       1.3.1                hfe3b2da_0    conda-forge\r\n",
      "mpfr                      4.2.0                hb012696_0    conda-forge\r\n",
      "mpmath                    1.3.0              pyhd8ed1ab_0    conda-forge\r\n",
      "msgpack-python            1.0.4            py37h7cecad7_0    conda-forge\r\n",
      "multipledispatch          0.6.0                      py_0    conda-forge\r\n",
      "munch                     2.5.0                    pypi_0    pypi\r\n",
      "munkres                   1.1.4              pyh9f0ad1d_0    conda-forge\r\n",
      "mypy_extensions           1.0.0              pyha770c72_0    conda-forge\r\n",
      "navigator-updater         0.2.1                    py37_0  \r\n",
      "nbclassic                 1.0.0              pyhb4ecaf3_1    conda-forge\r\n",
      "nbclient                  0.7.0              pyhd8ed1ab_0    conda-forge\r\n",
      "nbconvert                 7.2.5              pyhd8ed1ab_0    conda-forge\r\n",
      "nbconvert-core            7.2.5              pyhd8ed1ab_0    conda-forge\r\n",
      "nbconvert-pandoc          7.2.5              pyhd8ed1ab_0    conda-forge\r\n",
      "nbformat                  5.7.0              pyhd8ed1ab_0    conda-forge\r\n",
      "ncurses                   6.4                  hcb278e6_0    conda-forge\r\n",
      "nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge\r\n",
      "networkx                  2.6.3              pyhd8ed1ab_1    conda-forge\r\n",
      "nltk                      3.8.1              pyhd8ed1ab_0    conda-forge\r\n",
      "nomkl                     3.0                           0  \r\n",
      "nose                      1.3.7                   py_1006    conda-forge\r\n",
      "notebook                  6.5.4              pyha770c72_0    conda-forge\r\n",
      "notebook-shim             0.2.3              pyhd8ed1ab_0    conda-forge\r\n",
      "numba                     0.56.3           py37hf081915_0    conda-forge\r\n",
      "numexpr                   2.8.3           py37h85a3170_100    conda-forge\r\n",
      "numpy                     1.20.3           py37h038b26d_0    conda-forge\r\n",
      "numpy-base                1.18.5           py37h2f8d375_0  \r\n",
      "numpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge\r\n",
      "olefile                   0.46               pyh9f0ad1d_1    conda-forge\r\n",
      "openjpeg                  2.5.0                h7d73246_1    conda-forge\r\n",
      "openpyxl                  3.0.10           py37h540881e_1    conda-forge\r\n",
      "openssl                   1.1.1u               hd590300_0    conda-forge\r\n",
      "packaging                 23.1               pyhd8ed1ab_0    conda-forge\r\n",
      "pandas                    1.3.5            py37he8f5f7f_0    conda-forge\r\n",
      "pandoc                    3.1.3                h32600fe_0    conda-forge\r\n",
      "pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pango                     1.42.4               h7062337_4    conda-forge\r\n",
      "parso                     0.7.0                      py_0  \r\n",
      "partd                     1.4.0              pyhd8ed1ab_0    conda-forge\r\n",
      "patchelf                  0.17.2               h58526e2_0    conda-forge\r\n",
      "path                      16.5.0           py37h89c1867_0    conda-forge\r\n",
      "path.py                   12.5.0                        0    conda-forge\r\n",
      "pathlib                   1.0.1            py37h89c1867_6    conda-forge\r\n",
      "pathlib2                  2.3.7.post1      py37h89c1867_1    conda-forge\r\n",
      "pathspec                  0.11.1             pyhd8ed1ab_0    conda-forge\r\n",
      "pathtools                 0.1.2                      py_1    conda-forge\r\n",
      "patsy                     0.5.3              pyhd8ed1ab_0    conda-forge\r\n",
      "pcre                      8.45                 h9c3ff4c_0    conda-forge\r\n",
      "pep8                      1.7.1                      py_0    conda-forge\r\n",
      "pexpect                   4.8.0              pyh1a96a4e_2    conda-forge\r\n",
      "pickleshare               0.7.5                   py_1003    conda-forge\r\n",
      "pillow                    9.2.0            py37h850a105_2    conda-forge\r\n",
      "pip                       23.2.1             pyhd8ed1ab_0    conda-forge\r\n",
      "pixman                    0.38.0            h516909a_1003    conda-forge\r\n",
      "pkginfo                   1.9.6              pyhd8ed1ab_0    conda-forge\r\n",
      "pkgutil-resolve-name      1.3.10             pyhd8ed1ab_0    conda-forge\r\n",
      "platformdirs              3.9.1              pyhd8ed1ab_0    conda-forge\r\n",
      "pluggy                    1.0.0            py37h89c1867_3    conda-forge\r\n",
      "ply                       3.11                       py_1    conda-forge\r\n",
      "prometheus_client         0.17.1             pyhd8ed1ab_0    conda-forge\r\n",
      "prompt-toolkit            3.0.39             pyha770c72_0    conda-forge\r\n",
      "prompt_toolkit            3.0.39               hd8ed1ab_0    conda-forge\r\n",
      "protobuf                  3.19.3                   pypi_0    pypi\r\n",
      "psutil                    5.9.0            py37h5eee18b_0  \r\n",
      "pthread-stubs             0.4               h36c2ea0_1001    conda-forge\r\n",
      "ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\r\n",
      "py                        1.11.0             pyh6c4a22f_0    conda-forge\r\n",
      "py-lief                   0.12.3           py37h6a678d5_0  \r\n",
      "pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge\r\n",
      "pycosat                   0.6.4            py37h540881e_0    conda-forge\r\n",
      "pycparser                 2.21               pyhd8ed1ab_0    conda-forge\r\n",
      "pycrypto                  2.6.1           py37h5e8e339_1006    conda-forge\r\n",
      "pycurl                    7.45.1           py37haaec8a5_2    conda-forge\r\n",
      "pydocstyle                6.1.1              pyhd8ed1ab_0    conda-forge\r\n",
      "pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge\r\n",
      "pygments                  2.15.1             pyhd8ed1ab_0    conda-forge\r\n",
      "pyjwt                     2.8.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pylint                    2.7.2            py37h89c1867_0    conda-forge\r\n",
      "pyls-black                0.4.6              pyh9f0ad1d_0    conda-forge\r\n",
      "pyls-spyder               0.3.2              pyhd3eb1b0_0  \r\n",
      "pyodbc                    4.0.34           py37hd23a5d3_0    conda-forge\r\n",
      "pyopenssl                 22.0.0             pyhd8ed1ab_1    conda-forge\r\n",
      "pyparsing                 3.1.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pyproj                    3.2.1                    pypi_0    pypi\r\n",
      "pyqt                      5.9.2            py37hcca6a23_4    conda-forge\r\n",
      "pyrsistent                0.18.0           py37heee7806_0  \r\n",
      "pysocks                   1.7.1            py37h89c1867_5    conda-forge\r\n",
      "pytables                  3.7.0            py37h5dea08b_0    conda-forge\r\n",
      "pytest                    7.4.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-arraydiff          0.5.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-astropy            0.10.0             pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-astropy-header     0.2.2              pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-cov                4.1.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-doctestplus        0.13.0             pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-filter-subpackage  0.1.1                      py_0    conda-forge\r\n",
      "pytest-mock               3.11.1             pyhd8ed1ab_0    conda-forge\r\n",
      "pytest-openfiles          0.5.0                      py_0    conda-forge\r\n",
      "pytest-remotedata         0.4.0              pyhd8ed1ab_0    conda-forge\r\n",
      "python                    3.7.11               h12debd9_0  \r\n",
      "python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\n",
      "python-fastjsonschema     2.18.0             pyhd8ed1ab_0    conda-forge\r\n",
      "python-jsonrpc-server     0.4.0              pyh9f0ad1d_0    conda-forge\r\n",
      "python-language-server    0.36.2             pyhd8ed1ab_0    conda-forge\r\n",
      "python-libarchive-c       4.0              py37h89c1867_1    conda-forge\r\n",
      "python_abi                3.7                     2_cp37m    conda-forge\r\n",
      "pytoolconfig              1.2.5              pyhd8ed1ab_0    conda-forge\r\n",
      "pytz                      2023.3             pyhd8ed1ab_0    conda-forge\r\n",
      "pywavelets                1.3.0            py37hda87dfa_1    conda-forge\r\n",
      "pyxdg                     0.28               pyhd8ed1ab_0    conda-forge\r\n",
      "pyyaml                    6.0              py37h540881e_4    conda-forge\r\n",
      "pyzmq                     24.0.1           py37h0c0c2a8_0    conda-forge\r\n",
      "qdarkstyle                2.8.1              pyhd8ed1ab_2    conda-forge\r\n",
      "qt                        5.9.7                h5867ecd_1  \r\n",
      "qtawesome                 1.2.3              pyhd8ed1ab_0    conda-forge\r\n",
      "qtconsole                 5.4.3              pyhd8ed1ab_0    conda-forge\r\n",
      "qtconsole-base            5.4.3              pyha770c72_0    conda-forge\r\n",
      "qtpy                      2.3.1              pyhd8ed1ab_0    conda-forge\r\n",
      "readline                  8.2                  h8228510_1    conda-forge\r\n",
      "regex                     2022.7.9         py37h5eee18b_0  \r\n",
      "requests                  2.31.0             pyhd8ed1ab_0    conda-forge\r\n",
      "requests-toolbelt         1.0.0              pyhd8ed1ab_0    conda-forge\r\n",
      "ripgrep                   13.0.0               h2f28480_2    conda-forge\r\n",
      "rope                      1.9.0              pyhd8ed1ab_0    conda-forge\r\n",
      "rtree                     1.0.1            py37h0b55af0_0    conda-forge\r\n",
      "ruamel_yaml               0.15.80         py37h540881e_1007    conda-forge\r\n",
      "scikit-image              0.19.3           py37hfb7772e_1    conda-forge\r\n",
      "scikit-learn              1.0.2            py37hf9e9bfc_0    conda-forge\r\n",
      "scipy                     1.5.3            py37h8911b10_0    conda-forge\r\n",
      "seaborn                   0.12.2               hd8ed1ab_0    conda-forge\r\n",
      "seaborn-base              0.12.2             pyhd8ed1ab_0    conda-forge\r\n",
      "secretstorage             3.3.3            py37h89c1867_0    conda-forge\r\n",
      "send2trash                1.8.2              pyh41d4057_0    conda-forge\r\n",
      "sentencepiece             0.1.97                   pypi_0    pypi\r\n",
      "seqtk                     1.3                  h7132678_4    bioconda\r\n",
      "setuptools                59.8.0           py37h89c1867_1    conda-forge\r\n",
      "shapely                   1.8.0                    pypi_0    pypi\r\n",
      "simplegeneric             0.8.1                      py_1    conda-forge\r\n",
      "singledispatch            3.6.1              pyh44b312d_0    conda-forge\r\n",
      "sip                       4.19.8           py37hf484d3e_0  \r\n",
      "six                       1.16.0             pyh6c4a22f_0    conda-forge\r\n",
      "sklearn                   0.0                      pypi_0    pypi\r\n",
      "skmer                     3.1.0              pyhfa5458b_1    bioconda\r\n",
      "snappy                    1.1.10               h9fff704_0    conda-forge\r\n",
      "sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge\r\n",
      "snowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge\r\n",
      "sortedcollections         2.1.0              pyhd8ed1ab_0    conda-forge\r\n",
      "sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge\r\n",
      "soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge\r\n",
      "sphinx                    5.3.0              pyhd8ed1ab_0    conda-forge\r\n",
      "sphinxcontrib             1.0                      py37_1  \r\n",
      "sphinxcontrib-applehelp   1.0.4              pyhd8ed1ab_0    conda-forge\r\n",
      "sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge\r\n",
      "sphinxcontrib-htmlhelp    2.0.1              pyhd8ed1ab_0    conda-forge\r\n",
      "sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge\r\n",
      "sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge\r\n",
      "sphinxcontrib-serializinghtml 1.1.5              pyhd8ed1ab_2    conda-forge\r\n",
      "sphinxcontrib-websupport  1.2.4              pyhd8ed1ab_1    conda-forge\r\n",
      "spyder                    4.2.5            py37h89c1867_0    conda-forge\r\n",
      "spyder-kernels            1.10.2           py37h06a4308_0  \r\n",
      "sqlalchemy                1.3.24           py37h540881e_1    conda-forge\r\n",
      "sqlite                    3.42.0               h2c6b66d_0    conda-forge\r\n",
      "statsmodels               0.12.1           py37ha21ca33_1    conda-forge\r\n",
      "sympy                     1.10.1           py37h89c1867_1    conda-forge\r\n",
      "tabulate                  0.9.0                    pypi_0    pypi\r\n",
      "tbb                       2021.6.0             h924138e_1    conda-forge\r\n",
      "tblib                     1.7.0              pyhd8ed1ab_0    conda-forge\r\n",
      "tensorboard               1.14.0                   pypi_0    pypi\r\n",
      "tensorflow-estimator      1.14.0                   pypi_0    pypi\r\n",
      "tensorflow-gpu            1.14.0                   pypi_0    pypi\r\n",
      "termcolor                 1.1.0                    pypi_0    pypi\r\n",
      "terminado                 0.17.1             pyh41d4057_0    conda-forge\r\n",
      "testpath                  0.6.0              pyhd8ed1ab_0    conda-forge\r\n",
      "textdistance              4.5.0              pyhd8ed1ab_0    conda-forge\r\n",
      "textformats               1.2.2                    pypi_0    pypi\r\n",
      "textwrap3                 0.9.2                    pypi_0    pypi\r\n",
      "threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge\r\n",
      "three-merge               0.1.1              pyh9f0ad1d_0    conda-forge\r\n",
      "tifffile                  2019.7.26.2              py37_0    conda-forge\r\n",
      "tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge\r\n",
      "tk                        8.6.12               h27826a3_0    conda-forge\r\n",
      "tokenizers                0.13.1                   pypi_0    pypi\r\n",
      "toml                      0.10.2             pyhd8ed1ab_0    conda-forge\r\n",
      "tomli                     2.0.1              pyhd8ed1ab_0    conda-forge\r\n",
      "toolz                     0.12.0             pyhd8ed1ab_0    conda-forge\r\n",
      "torch                     1.11.0                   pypi_0    pypi\r\n",
      "tornado                   6.2              py37h540881e_0    conda-forge\r\n",
      "tqdm                      4.65.0             pyhd8ed1ab_1    conda-forge\r\n",
      "traitlets                 5.9.0              pyhd8ed1ab_0    conda-forge\r\n",
      "transformers              4.24.0                   pypi_0    pypi\r\n",
      "typed-ast                 1.4.3            py37h7f8727e_1  \r\n",
      "typing-extensions         4.7.1                hd8ed1ab_0    conda-forge\r\n",
      "typing_extensions         4.7.1              pyha770c72_0    conda-forge\r\n",
      "tzdata                    2023c                h71feb2d_0    conda-forge\r\n",
      "ujson                     5.5.0            py37hd23a5d3_0    conda-forge\r\n",
      "unicodecsv                0.14.1                     py_1    conda-forge\r\n",
      "unixodbc                  2.3.10               h583eb01_0    conda-forge\r\n",
      "urllib3                   1.26.15            pyhd8ed1ab_0    conda-forge\r\n",
      "watchdog                  1.0.2            py37h06a4308_1  \r\n",
      "wcwidth                   0.2.6              pyhd8ed1ab_0    conda-forge\r\n",
      "webencodings              0.5.1                      py_1    conda-forge\r\n",
      "websocket-client          1.6.1              pyhd8ed1ab_0    conda-forge\r\n",
      "werkzeug                  2.2.3              pyhd8ed1ab_0    conda-forge\r\n",
      "wheel                     0.41.0             pyhd8ed1ab_0    conda-forge\r\n",
      "widgetsnbextension        4.0.8              pyhd8ed1ab_0    conda-forge\r\n",
      "wrapt                     1.12.1           py37h5e8e339_3    conda-forge\r\n",
      "wurlitzer                 3.0.3              pyhd8ed1ab_0    conda-forge\r\n",
      "xlrd                      2.0.1              pyhd8ed1ab_3    conda-forge\r\n",
      "xlsxwriter                3.1.2              pyhd8ed1ab_0    conda-forge\r\n",
      "xlwt                      1.3.0                      py_1    conda-forge\r\n",
      "xmltodict                 0.13.0             pyhd8ed1ab_0    conda-forge\r\n",
      "xorg-kbproto              1.0.7             h7f98852_1002    conda-forge\r\n",
      "xorg-libice               1.1.1                hd590300_0    conda-forge\r\n",
      "xorg-libsm                1.2.4                h7391055_0    conda-forge\r\n",
      "xorg-libx11               1.7.2                h7f98852_0    conda-forge\r\n",
      "xorg-libxau               1.0.11               hd590300_0    conda-forge\r\n",
      "xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge\r\n",
      "xorg-libxext              1.3.4                h0b41bf4_2    conda-forge\r\n",
      "xorg-libxrender           0.9.10            h7f98852_1003    conda-forge\r\n",
      "xorg-renderproto          0.11.1            h7f98852_1002    conda-forge\r\n",
      "xorg-xextproto            7.3.0             h0b41bf4_1003    conda-forge\r\n",
      "xorg-xproto               7.0.31            h7f98852_1007    conda-forge\r\n",
      "xz                        5.2.6                h166bdaf_0    conda-forge\r\n",
      "yaml                      0.2.5                h7f98852_2    conda-forge\r\n",
      "yapf                      0.33.0             pyhd8ed1ab_1    conda-forge\r\n",
      "zeromq                    4.3.4                h9c3ff4c_1    conda-forge\r\n",
      "zict                      2.2.0              pyhd8ed1ab_0    conda-forge\r\n",
      "zipp                      3.10.0             pyhd8ed1ab_0    conda-forge\r\n",
      "zlib                      1.2.13               hd590300_5    conda-forge\r\n",
      "zope                      1.0                      py37_1  \r\n",
      "zope.event                5.0                pyhd8ed1ab_0    conda-forge\r\n",
      "zope.interface            5.4.0            py37h540881e_2    conda-forge\r\n",
      "zstd                      1.5.2                hfc55251_7    conda-forge\r\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs = '6yrs'\n",
    "progression = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final_Samples_path = './Final Samples/Final_Samples_' + yrs + '.json'\n",
    "# Final_Samples = json.load(open(Final_Samples_path, 'r')) \n",
    "# usable_samples_ADNI = json.load(open('./usable_samples_ADNI.json'))\n",
    "# PRS_orig_feature_matrix = np.load('./PRS_feature_matrix.npy').astype(np.float32)\n",
    "# # normalize feature matrix\n",
    "# PRS_orig_feature_matrix = (PRS_orig_feature_matrix - PRS_orig_feature_matrix.mean(0))/PRS_orig_feature_matrix.std(0)\n",
    "# # PRS_orig_feature_matrix.shape[1], len(usable_samples_ADNI), usable_samples_ADNI\n",
    "# num_features=PRS_orig_feature_matrix.shape[1]\n",
    "# print(len( usable_samples_ADNI ) )\n",
    "# print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # covar_df = pd.read_csv('./COVAR_FILE_bigger_dataset.txt', ' ') \n",
    "# covar_df = pd.read_csv('./COVAR_FILE_bigger_dataset.txt', ' ') \n",
    "# print(\"shape\",covar_df.shape)\n",
    "# print( covar_df[['AGE', 'PTGENDER']].shape, covar_df[['AGE', 'PTGENDER']].dropna().shape ) \n",
    "# # PC - Principal Component\n",
    "\n",
    "# # trying to normalize AGE with having max age of 100\n",
    "# covar_df['AGE'] = covar_df['AGE'] / 100.0\n",
    "# print( covar_df.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covar for ADNI Plink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter parameters :\n",
    "    1. Number of features\n",
    "    2. Number of Hidden Layers \n",
    "    3. Dimension of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = 52\n",
    "hidden = 4\n",
    "hidden_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take the first num_features column from PRS_feature_matrix\n",
    "# PRS_feature_matrix = PRS_orig_feature_matrix\n",
    "# PRS_feature_matrix = PRS_feature_matrix[:, :num_features]\n",
    "# print(PRS_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Samples has two columns of data :\n",
    "    1. ID \n",
    "    2. output - true / false\n",
    "    \n",
    "Get the length of positive and negative samples of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # positive samples - output true\n",
    "# # negative samples - output false\n",
    "# len_positive_samples = 0\n",
    "# len_negative_samples = 0\n",
    "# for x in Final_Samples:\n",
    "#     if x[1] == 1 :\n",
    "#         len_positive_samples += 1\n",
    "#     else :\n",
    "#         len_negative_samples += 1\n",
    "        \n",
    "# print(len(Final_Samples))\n",
    "# print(len_positive_samples)\n",
    "# print(len_negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining covar data with PRS Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # cnt = number of missing IDs for which covar data doesn't exist\n",
    "# cnt = 0\n",
    "# print(\"Before adding covar_df columns, shape : \",PRS_feature_matrix.shape)\n",
    "# # adding ( total columns - 2 ) of covar_df , excluding FID, IID\n",
    "# FEATURE_MATRIX = np.concatenate([PRS_feature_matrix, np.zeros([PRS_feature_matrix.shape[0], covar_df.shape[1] - 2 ])], 1).astype(np.float32)\n",
    "# print(\"Before adding covar_df columns, shape : \",FEATURE_MATRIX.shape)\n",
    "# for sample in usable_samples_ADNI:\n",
    "#     # taking from the PCs, skipping the first two columns of IID, FID\n",
    "#     covar = covar_df[covar_df['IID'] == sample].to_numpy()[:, 2:].astype(np.float32) \n",
    "#     # shape[0] = 1 means a row is found in covar for the following sample ID\n",
    "#     # if not, that means no covar data exists for the sample in usable_samples_ADNI\n",
    "#     if covar.shape[0] != 1:\n",
    "# #         print(covar.shape)\n",
    "#         print(sample)\n",
    "#         cnt += 1\n",
    "#         continue\n",
    "#     # Adding the covar values to the feature matrix\n",
    "#     FEATURE_MATRIX[usable_samples_ADNI[sample], num_features:] = covar\n",
    "\n",
    "\n",
    "# print(\"Count of missing samples for covar data : \", cnt)\n",
    "# #     FEATURE_MATRIX[usable_samples_ADNI[sample], num_features:] = covar # naeem's modification\n",
    "# # cnt/FEATURE_MATRIX.shape[0], FEATURE_MATRIX[:2], PRS_feature_matrix[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directory for saving shap figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./shap/\" + str(num_features)\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the index with the Age = 0\n",
    "\n",
    "age is zero for the rows that the covar data was not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_zero = 0\n",
    "# age_zero_idx = []\n",
    "# for i in range( len(FEATURE_MATRIX) ):\n",
    "#     if FEATURE_MATRIX[i, -1] == 0.00:\n",
    "#         age_zero += 1\n",
    "#         age_zero_idx.append(i)\n",
    "        \n",
    "# print(age_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indices of features to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # feature_indices_to_consider = list(range(23))#list(range(35)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "# # naeem modified\n",
    "# print(FEATURE_MATRIX.shape[1])\n",
    "# last_idx = FEATURE_MATRIX.shape[1] - 1\n",
    "# feature_indices_to_consider = list(range(num_features))  + [last_idx - 1, last_idx] #list(range(35)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "# # remove_indices = [1, 2, 3]\n",
    "# # for i in remove_indices:\n",
    "# #     feature_indices_to_consider.remove(i)\n",
    "\n",
    "# # feature_indices_to_consider = list(range(23, 36))\n",
    "\n",
    "# # feature_indices_to_consider = [ 4, 11, 14, 21, 23, 26, 32, 34, 46]\n",
    "\n",
    "# # feature_indices_to_consider = [ 9, 10, 11, 14, 21, 23, 26, 28, 32, 34, 46]\n",
    "# # feature_indices_to_consider = [9, 10, 28, 34, 46]\n",
    "\n",
    "# print(feature_indices_to_consider)\n",
    "# # feature_indices_to_consider = [1, 2, 3, 11, 14, 21, 23, 26, 32, 45]\n",
    "# # feature_indices_to_consider = [2, 26, 32, 45]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_in_file: saves model accuracy in a text file\n",
    "#     args : model_name : name of model with layers and dimensions\n",
    "#            accuracy : accuracy  score\n",
    "def save_in_file(model_name, accuracy):\n",
    "    model_file = open(\"model_details.txt\",\"a\")\n",
    "    model_file.write(model_name + \" -> accuracy : \" + str(accuracy) + \"\\n\" )\n",
    "    model_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications**\n",
    "1. Added relu in the hidden layers and sigmoid in the output layer as activation functions\n",
    "2. Added dropout in the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim= hidden_dimension, drop_probab=.5):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        num_hidden = hidden\n",
    "        hidden_dim = hidden_dimension\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "#         self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.drop_probab = drop_probab\n",
    "        self.dropout = nn.functional.dropout\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        for i in range(self.num_hidden):\n",
    "            features = self.fc_hidden[i](features)\n",
    "            # added by Mashiat\n",
    "            features = self.dropout(features, p=self.drop_probab)\n",
    "            features = self.relu( features )\n",
    "            ####################\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_model(num_features=len(feature_indices_to_consider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataSet class \n",
    "combines usable_samples_ADNI, Final_Samples, feature_matrix to one dataset with features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dataSet(data.Dataset):\n",
    "#     def __init__(self, Final_Samples, feature_matrix, usable_samples_ADNI, feature_indices_to_consider=feature_indices_to_consider):\n",
    "#         super(dataSet, self).__init__()  \n",
    "#         self.data_len = len(Final_Samples)\n",
    "#         self.usable_samples_ADNI = usable_samples_ADNI\n",
    "#         self.Final_Samples = Final_Samples\n",
    "#         self.feature_indices_to_consider = feature_indices_to_consider\n",
    "#         self.feature_matrix = feature_matrix[:, self.feature_indices_to_consider]\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         features = torch.from_numpy(self.feature_matrix[self.usable_samples_ADNI[self.Final_Samples[index][0]]]).float()\n",
    "#         label = torch.tensor([float(self.Final_Samples[index][1])]).float()\n",
    "#         return features, label\n",
    "    \n",
    "#     def update_prs_features(self, mean, std):\n",
    "#         self.feature_matrix = (self.feature_matrix - mean) / std\n",
    "        \n",
    "#     def get_mean_std(self):\n",
    "#         mean = self.feature_matrix.mean(0)\n",
    "#         std = self.feature_matrix.std(0)\n",
    "#         return mean, std\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Pandas Dataframe to Dataset class\n",
    "\n",
    "overriding the constructor, getitem, len function of the original class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class df_dataSet(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.features = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y.values, dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch function : runs an epoch of a model\n",
    "#                 args :\n",
    "#                         model : neural network model\n",
    "#                         optimizer :\n",
    "#                         criterion :\n",
    "#                         is_training : train - true or test - false\n",
    "#                         loader : torch dataset\n",
    "#                 returns :\n",
    "#                         different accuracy score for the dataset of per epoch\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "#     print(loader)\n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "        label = torch.reshape(label, (label.shape[0], 1))\n",
    "        probab = model(features)\n",
    "        if is_training:  \n",
    "#             print(probab.shape, label.shape)\n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "    return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "#     return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**epoch function for LOOCV**\n",
    "\n",
    "Without precision, recall, ROC, AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loocv_epoch(model, optimizer, criterion, is_training, loader):\n",
    "#     pred = []\n",
    "#     true = []\n",
    "#     total_loss = 0.\n",
    "# #     print(loader)\n",
    "#     for batch_idx, (features, label) in enumerate(loader):\n",
    "#         features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "#         label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "#         label = torch.reshape(label, (label.shape[0], 1))\n",
    "#         probab = model(features)\n",
    "#         if is_training:  \n",
    "# #             print(probab.shape, label.shape)\n",
    "#             loss = criterion(probab, label)\n",
    "#             ## compute gradient and do SGD step \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "# #             print(batch_idx, ':', loss) \n",
    "#         pred += probab.detach().cpu().numpy().tolist()\n",
    "#         true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "#     pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "#     pred_binary = (pred > .5).astype(float)\n",
    "# #     precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "# #     auroc = roc_auc_score(true, pred)\n",
    "# #     p, r, thresholds = precision_recall_curve(true, pred)\n",
    "# #     auprc = auc(r, p)\n",
    "#     acc = (pred_binary==true).mean()\n",
    "    \n",
    "# #     return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "#     return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usable_samples_ADNI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Analysis to oversample data\n",
    "\n",
    "https://towardsdatascience.com/smote-synthetic-data-augmentation-for-tabular-data-1ce28090debc#:~:text=SMOTE%20is%20an%20over%2Dsampling,its%20%E2%80%9Ck%E2%80%9D%20nearest%20neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def smote(x, y):\n",
    "#     # Synthetic Minority Over-samping Technique\n",
    "#     # \n",
    "#     # sampling_strategy: determines the portion of samples to \n",
    "#     #                    generate with respect to the majority class\n",
    "#     # k_neighbors : number of neighbors to be considered for each sample\n",
    "    \n",
    "#     # For this example, only 1% of minoirty samples are considered\n",
    "#     k_neighbors = math.ceil(sum(y) * 0.01)\n",
    "      \n",
    "#     smote = SMOTE(sampling_strategy=1, \n",
    "#                   k_neighbors=k_neighbors)\n",
    "#     x, y = smote.fit_resample(x, y)\n",
    "    \n",
    "#     return x, y\n",
    "    \n",
    "# def bordersmote(x, y):\n",
    "#     # Borderline-SMOTE\n",
    "#     # \n",
    "#     # sampling_strategy: determines the portion of samples to \n",
    "#     #                    generate with respect to the majority class\n",
    "#     # k_neighbors : number of neighbors to be considered for each sample\n",
    "#     # m_neighbors : number of neighbors to consider to determine if a sample is danger\n",
    "    \n",
    "#     # For this example, only 1% of minoirty samples are considered\n",
    "#     k_neighbors = math.ceil(sum(y) * 0.01)\n",
    "#     m_neighbors = math.ceil(sum(y) * 0.01)\n",
    "    \n",
    "#     bordersmote = BorderlineSMOTE(sampling_strategy=1, \n",
    "#                                   k_neighbors=k_neighbors, \n",
    "#                                   m_neighbors=m_neighbors)\n",
    "    \n",
    "#     x, y = bordersmote.fit_resample(x, y)\n",
    "    \n",
    "#     return x, y\n",
    "    \n",
    "# def adasyn(x, y):\n",
    "#     # Adaptive Synthetic\n",
    "#     # \n",
    "#     # sampling_strategy: determines the portion of samples to \n",
    "#     #                    generate with respect to the majority class\n",
    "#     # n_neighbors : number of neighbors to be considered for each sample\n",
    "    \n",
    "#     # For this example, only 1% of minoirty samples are considered\n",
    "#     n_neighbors = math.ceil(sum(y) * 0.01)\n",
    "    \n",
    "#     adasyn = ADASYN(sampling_strategy=1,\n",
    "#                    n_neighbors=n_neighbors)\n",
    "#     x, y = adasyn.fit_resample(x, y)\n",
    "\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropping last / output column in df\n",
    "# shuffled_X = shuffled.iloc[: , :-1]\n",
    "# # taking the output column of df\n",
    "# shuffled_Y = shuffled.iloc[: , -1]\n",
    "# print(df.shape)\n",
    "# df_X_new, df_Y_new = bordersmote(df_X, df_Y)\n",
    "# print( df_X_new.shape, df_Y_new.shape )\n",
    "\n",
    "# df = df_X_new\n",
    "# df['output'] = df_Y_new\n",
    "# print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( df['output'].value_counts() )\n",
    "# print(df.head())\n",
    "\n",
    "# # ones = df[df['output'] == 1]\n",
    "# # zeros = df[df['output'] == 0]\n",
    "# # min_len = min( len(ones), len(zeros) ) \n",
    "\n",
    "# # ones = ones.iloc[:min_len, :]\n",
    "# # zeros = zeros.iloc[:min_len, :]\n",
    "\n",
    "# # df = ones.append(zeros, ignore_index=True)\n",
    "# # print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling dataframe\n",
    "\n",
    "**Dropping alcohol recommended columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # keep the index same\n",
    "# # shuffled = df.sample(frac=1)\n",
    "# # print( shuffled.head() )\n",
    "\n",
    "# # reset the index\n",
    "# shuffled = df.sample(frac=1, random_state = 1).reset_index()\n",
    "# # print(shuffled.columns)\n",
    "# shuffled = shuffled.drop( ['index'], axis = 1 )\n",
    "\n",
    "\n",
    "# # #  dropping alcohol recommended columns\n",
    "# # shuffled = shuffled.drop(['Ever had known person concerned about, or recommend reduction of, alcohol consumption: No', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, but not in the last year', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, during the last year'], axis = 1)\n",
    "# # shuffled = shuffled.drop(['Non-cancer illness code, self-reported: anxiety/panic attacks', 'Sleeplessness / insomnia', 'Non-cancer illness code, self-reported: type 2 diabetes', \"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\", \"Alcohol intake frequency\", 'Other meat intake' ], axis = 1)\n",
    "\n",
    "# print( shuffled.head() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled.to_csv('adni_shuffled_balanced_'+yrs+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features based on all methods\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/python-counter-python-collections-counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_dict = {k: v for k, v in sorted(dict_features.items(), key=lambda item: item[1], reverse=True)}\n",
    "# print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output keys to a file\n",
    "# output_file = \"column_importance_ADNI.txt\"\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for key in sorted_dict.keys():\n",
    "#         f.write(key + \"\\n\")\n",
    "\n",
    "# print(\"Sorted keys have been written to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # print(selected_features)\n",
    "# c = Counter( selected_features )\n",
    "# # print( c )\n",
    "# # print( c.most_common(20) )\n",
    "# most_common = c.most_common(15)\n",
    "# print( most_common )\n",
    "# selected_col = list( list(zip(*most_common))[0] )\n",
    "# print(selected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop extra features from Shuffled Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review select features\n",
    "\n",
    "Features related to : \n",
    "**Alcohol Consumption,\n",
    "Hearing Problem,\n",
    "Smoking / Cigarettes,\n",
    "Cholesterol,\n",
    "Blood Pressure,\n",
    "Meat,\n",
    "Depression,\n",
    "Insomnia /  Sleep Schedule,\n",
    "Education,\n",
    "Hypertension,\n",
    "Physical Inactivity,\n",
    "Brain Injury,\n",
    "Father / Mother,\n",
    "Obesity,\n",
    "Diabetes,\n",
    "Age**\n",
    "\n",
    "\n",
    "'Non-cancer illness code, self-reported: type 2 diabetes'\n",
    "'Total cholesterol', 'HDL cholesterol', 'LDL cholesterol'\n",
    "'Cigarettes per Day', 'systolic blood pressure', 'diastolic blood pressure'\n",
    "'Hearing difficulty/problems: Yes', 'Non-cancer illness code, self-reported: depression\n",
    "'Hearing difficulty/problems with background noise'\n",
    "'Sleeplessness / insomnia', 'Sleep duration', 'Age completed full time education'\n",
    "'Types of physical activity in last 4 weeks: Strenuous sports', 'Other meat intake', 'Loneliness, isolation', \"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\", 'Mood swings'\n",
    "'Non-cancer illness code, self-reported: hypertension'\n",
    "'Diagnoses - secondary ICD10: I10 Essential (primary) hypertension'\n",
    "'Non-cancer illness code, self-reported: head injury', 'Alcohol intake frequency', 'Diagnoses - secondary ICD10: E66.9 Obesity, unspecified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selected_col += ['output']\n",
    "# selected_col = list( shuffled.columns.values )\n",
    "# # selected_col.remove(\"Parkinson's disease\")\n",
    "# print(selected_col)\n",
    "# # selected_col.remove('age')\n",
    "# # selected_col.remove('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and column priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys read from the file: ['Types of physical activity in last 4 weeks: Light DIY (eg: pruning, watering the lawn)', 'Illness, injury, bereavement, stress in last 2 years: Serious illness, injury or assault to yourself', \"Parkinson's disease\", 'Hearing difficulty/problems with background noise', 'Illness, injury, bereavement, stress in last 2 years: Financial difficulties', 'Non-cancer illness code, self-reported: hypothyroidism/myxoedema', 'Diagnoses - secondary ICD10: I10 Essential (primary) hypertension', 'age', 'systolic blood pressure', 'Mood swings', 'Types of physical activity in last 4 weeks: Strenuous sports', 'HDL cholesterol', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: No', 'Loneliness, isolation', 'Body mass index (BMI)', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, during the last year', 'Non-oily fish intake', 'Leisure/social activities: Pub or social club', 'Mental health problems ever diagnosed by a professional: Anxiety, nerves or generalized anxiety disorder', \"Alzheimer's disease\", 'Hearing difficulty/problems: Yes', 'Non-cancer illness code, self-reported: depression', 'gender', 'Non-cancer illness code, self-reported: hypertension', 'Operation code: brain surgery', 'ADHD', 'Non-cancer illness code, self-reported: stroke', 'Oily fish intake', 'diastolic blood pressure', 'Microalbumin in urine', 'Leisure/social activities: Sports club or gym', 'Sleeplessness / insomnia', 'Mental health problems ever diagnosed by a professional: Social anxiety or social phobia', 'Cigarettes per Day', 'Nitrogen oxides air pollution; 2010', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, but not in the last year', 'Processed meat intake', 'Alcohol intake frequency', 'Non-cancer illness code, self-reported: type 2 diabetes', 'Non-cancer illness code, self-reported: hyperthyroidism/thyrotoxicosis', 'Non-cancer illness code, self-reported: head injury', 'triglycerides', 'Total cholesterol', 'Ever had prolonged loss of interest in normal activities', 'Sleep duration', 'Prospective memory result', 'Other meat intake', 'LDL cholesterol', 'Diagnoses - secondary ICD10: E66.9 Obesity, unspecified', 'Illness, injury, bereavement, stress in last 2 years: Marital separation/divorce', 'Non-cancer illness code, self-reported: anxiety/panic attacks', \"Illnesses of mother: Alzheimer's disease/dementia\", 'Particulate matter air pollution 2.5-10um; 2010', \"Illnesses of father: Alzheimer's disease/dementia\", 'Age completed full time education']\n"
     ]
    }
   ],
   "source": [
    "shuffled = pd.read_csv('adni_shuffled_balanced_' + yrs + '.csv')\n",
    "\n",
    "\n",
    "# Read from the output file and store keys in a list\n",
    "input_file = \"column_importance_ADNI.txt\"\n",
    "if progression != \"\":\n",
    "    shuffled = pd.read_csv('adni_shuffled_balanced_' + yrs + '_' + progression + '.csv')\n",
    "    input_file = \"column_importance_ADNI\" + progression + \".txt\"\n",
    "col_imp = []\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        key = line.strip()  # Remove newline character\n",
    "        col_imp.append(key)\n",
    "\n",
    "print(\"Keys read from the file:\", col_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Column Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['Types of physical activity in last 4 weeks: Light DIY (eg: pruning, watering the lawn)', 'Illness, injury, bereavement, stress in last 2 years: Serious illness, injury or assault to yourself', \"Parkinson's disease\", 'Hearing difficulty/problems with background noise', 'Illness, injury, bereavement, stress in last 2 years: Financial difficulties', 'Non-cancer illness code, self-reported: hypothyroidism/myxoedema', 'Diagnoses - secondary ICD10: I10 Essential (primary) hypertension', 'age', 'systolic blood pressure']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"ADNI_feature_count.txt\", \"r\")\n",
    "if progression != \"\":\n",
    "    f = open(\"ADNI_feature_count_\" + progression + '.txt', \"r\")\n",
    "count = f.read()\n",
    "count = 9\n",
    "print( int(count) )\n",
    "cols_to_take = int(count)\n",
    "selected_col = col_imp[:cols_to_take]\n",
    "# selected_col.append( 'output' )\n",
    "print(selected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add selected columns of ROSMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read from the output file and store keys in a list\n",
    "# input_file = \"../ROSMAP analysis/column_importance_ROSMAP.txt\"\n",
    "# if progression != \"\":\n",
    "#     input_file = \"../ROSMAP analysis/column_importance_ROSMAP\" + progression + \".txt\"\n",
    "# col_imp = []\n",
    "\n",
    "# with open(input_file, \"r\") as f:\n",
    "#     for line in f:\n",
    "#         key = line.strip()  # Remove newline character\n",
    "#         col_imp.append(key)\n",
    "\n",
    "# print(\"Keys read from the file:\", col_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"../ROSMAP analysis/rosmap_feat_count.txt\", \"r\")\n",
    "# if progression != \"\":\n",
    "#     f = open(\"../ROSMAP analysis/rosmap_feat_count_\" + progression + \".txt\", \"r\")\n",
    "# count = f.read()\n",
    "# print( int(count) )\n",
    "# cols_to_take = int(count)\n",
    "# selected_col_ros = col_imp[:cols_to_take]\n",
    "# # selected_col.append( 'output' )\n",
    "# print(selected_col_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_col = selected_col + selected_col_ros\n",
    "# selected_col = list(set( selected_col ) )\n",
    "# print( selected_col )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Types of physical activity in last 4 weeks: Light DIY (eg: pruning, watering the lawn)', 'Illness, injury, bereavement, stress in last 2 years: Serious illness, injury or assault to yourself', \"Parkinson's disease\", 'Hearing difficulty/problems with background noise', 'Illness, injury, bereavement, stress in last 2 years: Financial difficulties', 'Non-cancer illness code, self-reported: hypothyroidism/myxoedema', 'Diagnoses - secondary ICD10: I10 Essential (primary) hypertension', 'age', 'systolic blood pressure', 'output']\n"
     ]
    }
   ],
   "source": [
    "# if 'Alzheimer\\'s Disease' not in selected_col:\n",
    "#     print(\"Not exists\")\n",
    "#     selected_col.append('Alzheimer\\'s disease' )\n",
    "\n",
    "selected_col.append( 'output' )\n",
    "print(selected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longitudinal Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_col = shuffled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_take = len(col_imp)\n",
    "# selected_col = col_imp[:cols_to_take]\n",
    "# selected_col.append( 'output' )\n",
    "# print(selected_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464, 10)\n",
      "   Types of physical activity in last 4 weeks: Light DIY (eg: pruning, watering the lawn)  \\\n",
      "0                                           0.139413                                        \n",
      "1                                          -1.058011                                        \n",
      "2                                           0.573958                                        \n",
      "3                                           0.233788                                        \n",
      "4                                           0.409575                                        \n",
      "\n",
      "   Illness, injury, bereavement, stress in last 2 years: Serious illness, injury or assault to yourself  \\\n",
      "0                                           0.788127                                                      \n",
      "1                                          -0.836299                                                      \n",
      "2                                          -0.099296                                                      \n",
      "3                                          -0.629129                                                      \n",
      "4                                           0.993390                                                      \n",
      "\n",
      "   Parkinson's disease  Hearing difficulty/problems with background noise  \\\n",
      "0             0.201119                                           0.802276   \n",
      "1            -0.774038                                          -1.171389   \n",
      "2            -0.367916                                           0.258010   \n",
      "3            -0.631882                                          -0.165733   \n",
      "4             0.320504                                           0.409332   \n",
      "\n",
      "   Illness, injury, bereavement, stress in last 2 years: Financial difficulties  \\\n",
      "0                                           1.128272                              \n",
      "1                                          -0.983379                              \n",
      "2                                           0.232975                              \n",
      "3                                           0.184508                              \n",
      "4                                           0.406567                              \n",
      "\n",
      "   Non-cancer illness code, self-reported: hypothyroidism/myxoedema  \\\n",
      "0                                           0.317127                  \n",
      "1                                          -0.769655                  \n",
      "2                                           0.389664                  \n",
      "3                                           0.002617                  \n",
      "4                                           0.118983                  \n",
      "\n",
      "   Diagnoses - secondary ICD10: I10 Essential (primary) hypertension    age  \\\n",
      "0                                           0.779473                  0.796   \n",
      "1                                          -1.415852                  0.723   \n",
      "2                                           0.579342                  0.702   \n",
      "3                                           0.031671                  0.724   \n",
      "4                                           0.665582                  0.833   \n",
      "\n",
      "   systolic blood pressure  output  \n",
      "0                 0.633621     1.0  \n",
      "1                 1.480495     1.0  \n",
      "2                -1.224046     0.0  \n",
      "3                -0.978939     0.0  \n",
      "4                -0.140194     1.0  \n"
     ]
    }
   ],
   "source": [
    "shuffled = shuffled[selected_col]\n",
    "\n",
    "# file = open('ROSMAP analysis/columns.txt','w')\n",
    "# for item in selected_col:\n",
    "#     file.write(item+\"\\n\")\n",
    "# file.close()\n",
    "\n",
    "print( shuffled.shape )\n",
    "print( shuffled.head() )\n",
    "\n",
    "# dropping last / output column in df\n",
    "shuffled_X = shuffled.iloc[: , :-1]\n",
    "shuffled_Y =  shuffled.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier with LOOCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Model with LOOCV**\n",
    "\n",
    "Epoch number is reduced to 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc):\n",
    "    print(\"accuracy:\",avg_acc)\n",
    "    print(\"precision:\",avg_prec)\n",
    "    print(\"recall:\",avg_rec)\n",
    "    print(\"fscore:\",avg_fsc)\n",
    "    print(\"auroc:\",avg_roc)\n",
    "    print(\"auprc:\",avg_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# CSV file path\n",
    "def write_out_to_csv(datatype, model, years, acc, prec, rec, auprc, auroc, fscore):\n",
    "    csv_file_path = \"scores.csv\"\n",
    "\n",
    "    # Check if the CSV file already exists\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        # Create a new CSV file and write header\n",
    "        with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([\"Datatype\", \"Model\", \"Year\", \"Size\", \"Features\",\"Average Accuracy\", \"Average Precision\", \"Average Recall\", \"Average F-Score\", \"Average ROC AUC\", \"Average PR AUC\"])\n",
    "\n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([datatype, model, years, shuffled.shape[0], shuffled_X.shape[1], acc, prec, rec, fscore, auroc, auprc])\n",
    "\n",
    "    print(\"Average scores have been appended to the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/09/a-comprehensive-guide-on-neural-networks-performance-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF:9\n",
      "\n",
      "#F10\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n",
      "10 :\n",
      "Split :\n"
     ]
    }
   ],
   "source": [
    "# tensor_X = torch.tensor(shuffled_X.values, dtype=torch.float32).to(DEVICE)\n",
    "# print( tensor_X[0].shape )\n",
    "\n",
    "GENERATE_SHAP = True\n",
    "total_epochs = 500 #250(ideal)\n",
    "num_features = shuffled_X.shape[1]\n",
    "# random_integers = [2, 6, 108, 90, 5]\n",
    "random_seed = random_seed#, 92, 0, 87, 73, 82, 54]\n",
    "\n",
    "total_folds = 10#[37*2]\n",
    "\n",
    "avg_val_acc = []\n",
    "\n",
    "shap_values_list = []\n",
    "# for num_features in num_features_list:\n",
    "print(f'NF:{num_features}')\n",
    "global_best_acc_val = 0.\n",
    "precision_avg = 0\n",
    "recall_avg = 0\n",
    "auprc_avg = 0\n",
    "auroc_avg = 0\n",
    "fscore_avg = 0\n",
    "# for total_folds in folds_list:\n",
    "print(f'\\n#F{total_folds}')\n",
    "# for random_seed in random_integers:\n",
    "accuracies = []\n",
    "accuracies_val = []\n",
    "temp_shap_values = np.zeros(shuffled_X.shape)\n",
    "\n",
    "kf = KFold(n_splits = total_folds, random_state=None)\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    print(\"Split :\")\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "\n",
    "#                 X_train, y_train = bordersmote( X_train, y_train )\n",
    "#                 print(\"train size: \", X_train.shape, y_train.shape)\n",
    "#                 print(\"test size: \", X_test.shape, y_test.shape)\n",
    "\n",
    "    train_dataset = df_dataSet( X_train, y_train )\n",
    "    valid_dataset = df_dataSet( X_test, y_test )\n",
    "\n",
    "    train_batch_size = train_dataset.__len__()\n",
    "    val_batch_size = valid_dataset.__len__()\n",
    "\n",
    "#                 print( train_batch_size, val_batch_size )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = False, num_workers = 0)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = val_batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "    model = simple_model(num_features = shuffled_X.shape[1], hidden_dim = hidden_dimension)\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss() \n",
    "    best_acc_val = 0.\n",
    "    model_best = None\n",
    "\n",
    "    for epoch_num in range(total_epochs):\n",
    "#         print(epoch_num)\n",
    "        model.train()\n",
    "#                     model.drop_probab=.8\n",
    "#                     print(\"model trained\")\n",
    "        precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                 criterion=criterion, is_training=True, \n",
    "                                                                               loader=train_loader)\n",
    "#                     print(\"model validated\")\n",
    "        model.eval()\n",
    "#                     model.drop_probab=.0\n",
    "        precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, \n",
    "                                                                                 optimizer=optimizer, \n",
    "                                                                                 criterion=criterion, is_training=False, \n",
    "                                                                                loader=valid_loader)\n",
    "#                     print(\"model kahini done\")\n",
    "        if acc_val > best_acc_val:\n",
    "            best_acc_val = acc_val\n",
    "            if acc_val > global_best_acc_val:\n",
    "                global_best_acc_val = acc_val\n",
    "#                         print('global updated!')\n",
    "            torch.save(model.state_dict(), 'PRS_model.pt')\n",
    "#                     print(f'#F:{total_folds}| seed:{random_seed}, fold:{fold_num}, epoch:{epoch_num} -> local:{best_acc_val}, global:{global_best_acc_val}')  \n",
    "#                     if epoch_num + 1 == total_epochs:\n",
    "#     #                     print(f'LAST_Epoch:{epoch_num}, train_acc:{acc_train}, val_acc:{acc_val}, local_best:{best_acc_val}, global_best:{global_best_acc_val}')\n",
    "#                         pass\n",
    "    model_best = simple_model(num_features= shuffled_X.shape[1], hidden_dim = hidden_dimension, drop_probab=.0)\n",
    "    model_best.load_state_dict(torch.load('PRS_model.pt'))\n",
    "    model_best = model_best.to(DEVICE)\n",
    "    model_best.eval()\n",
    "    precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true = epoch(model=model_best, \n",
    "                                                                             optimizer=optimizer, \n",
    "                                                                             criterion=criterion, is_training=False, \n",
    "                                                                             loader=valid_loader)\n",
    "    accuracies += [acc_test]\n",
    "    accuracies_val += [best_acc_val]\n",
    "#                 print(\"precision : \", precision, \" ; recall : \", recall)\n",
    "    precision_avg += precision\n",
    "    recall_avg += recall\n",
    "    auprc_avg += auprc\n",
    "    auroc_avg += auroc\n",
    "    fscore_avg += fscore\n",
    "\n",
    "#                 print(precision, recall, fscore, support, auroc, auprc, acc_test, total_loss)\n",
    "#                 print(\"pred\")\n",
    "#                 print(pred)\n",
    "#                 print(\"pred binary\")\n",
    "#                 print(type(pred_binary))\n",
    "#                 print(pred_binary)\n",
    "\n",
    "    print(total_folds, ':')\n",
    "#     if GENERATE_SHAP:\n",
    "#         explainer = shap.GradientExplainer(model_best.to(DEVICE), tensor_X,\n",
    "#                                            batch_size=shuffled_X.shape[0]) #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "# #                     print(\"usable features : \", usable_features.shape[0])\n",
    "# #                     print(usable_features.shape)\n",
    "# #                     print(usable_features)\n",
    "#         shap_values = explainer.shap_values(tensor_X, nsamples=500)\n",
    "# #                     print(\"shap values shape : \", shap_values.shape)\n",
    "# #                     print(\"Shap values : \", shap_values)\n",
    "# #                     print(\"shap values of 0 index\", shap_values[0, :])\n",
    "\n",
    "#         temp_shap_values += shap_values \n",
    "# if GENERATE_SHAP:\n",
    "#     temp_shap_values /= total_folds\n",
    "#     shap_values_list += [temp_shap_values] \n",
    "print(f'random_seed:{random_seed}:', np.mean(accuracies), np.std(accuracies), \n",
    "      np.mean(accuracies_val), np.std(accuracies_val), 'train acc:', acc_train)\n",
    "avg_val_acc += [np.mean(accuracies_val)]\n",
    "print(\"accuraacies of validation: \", accuracies_val)\n",
    "print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "precision_avg = precision_avg * 1.0 / total_folds\n",
    "recall_avg = recall_avg * 1.0 / total_folds\n",
    "auprc_avg = auprc_avg * 1.0 / total_folds\n",
    "auroc_avg = auroc_avg * 1.0 / total_folds\n",
    "fscore_avg = fscore_avg * 1.0 / total_folds\n",
    "print( \"precision avg : \", precision_avg )\n",
    "print( \"recall avg : \", recall_avg )\n",
    "print( \"AUPRC avg : \", auprc_avg )\n",
    "print( \"AUROC avg : \", auroc_avg )\n",
    "print( \"FScore avg : \", fscore_avg )\n",
    "avg_val_acc = np.array(avg_val_acc)\n",
    "printScores(avg_val_acc,precision_avg,recall_avg,fscore_avg,auroc_avg,auprc_avg)\n",
    "if progression == \"\":\n",
    "    write_out_to_csv(\"ADNI\", \"NN\", yrs, avg_val_acc, precision_avg,recall_avg,fscore_avg,auroc_avg,auprc_avg)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model accuracy in model_details.txt\n",
    "# save_in_file(\"Neural Network with \" + str(hidden)  + \" layers_4yrs_\" + str(len(feature_indices_to_consider)), global_best_acc_val)\n",
    "# # save_in_file(\"Neural Network with \" + str(hidden) + \" layers\", global_best_acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable_features = usable_features.cpu().detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap Analysis\n",
    "\n",
    "https://medium.com/dataman-in-ai/explain-your-model-with-the-shap-values-bc36aac4de3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print( shap_values_list )\n",
    "# # write shap_values_list to pkl file\n",
    "# pickle.dump(shap_values_list, open('shap_values_list.pkl', 'wb'))\n",
    "# shap_values = np.mean(shap_values_list, axis=0)\n",
    "# print( shap_values.shape )\n",
    "# # print(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force plot :\n",
    "\n",
    "\n",
    "The shap.force_plot() takes three values: \n",
    "\n",
    "(i) the base value (explainerModel.expected_value[0]),\n",
    "\n",
    "(ii) the SHAP values (shap_values_Model[j][0]) and \n",
    "\n",
    "(iii) the matrix of feature values (S.iloc[[j]]). The base value or the expected value is the average of the model output over the training data X_train. It is the base value used in the following plot.\n",
    "\n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195\n",
    "\n",
    "the bold 0.80 is the models score for this observation. Higher scores lead the model to predict 1 and lower scores lead the model to predict 0. The features that were important to making the prediction for this observation are shown in red and blue, with red representing features that pushed the model score higher, and blue representing features that pushed the score lower. Features that had more of an impact on the score are located closer to the dividing boundary between red and blue, and the size of that impact is represented by the size of the bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # print( shap_values_list )\n",
    "\n",
    "# # print the JS visualization code to the notebook\n",
    "# shap.initjs()\n",
    "\n",
    "# # print(shap_values[0, :])\n",
    "# # print(usable_features[0, :])\n",
    "# # shap.summary_plot(shap_values[:, :], usable_features[:, :])\n",
    "\n",
    "# # shap.force_plot(.5, shap_values[0,:], usable_features[0, :], link = \"logit\", matplotlib = True)  \n",
    "\n",
    "# # using pandas dataframe\n",
    "# shap.force_plot(.5, shap_values[0,:], shuffled_X.iloc[0, :], link = \"logit\", matplotlib = True  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # shap.force_plot(.5, shap_values[:,:], usable_features[:, :], link=\"logit\")\n",
    "\n",
    "# # using pandas dataframe\n",
    "# print(shap_values.shape)\n",
    "# print(shuffled_X.shape)\n",
    "# shap.force_plot(.5, shap_values[:,:], shuffled_X.iloc[:, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Plot\n",
    "This plot is made of all the dots in the train data. It delivers the following information:\n",
    "\n",
    "Feature importance: Variables are ranked in descending order.\n",
    "\n",
    "Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
    "\n",
    "Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n",
    "\n",
    "Correlation: A high level of the alcohol content has a high and positive impact on the quality rating. The high comes from the red color, and the positive impact is shown on the X-axis. Similarly, we will say the volatile acidity is negatively correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(zip(list(range(23)), abs(shap_values).mean(0))), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./shap/\" + str(num_features)\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "    \n",
    "# # for trait in traits:\n",
    "# #     print(trait)\n",
    "# # https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "# # usable_features_std = (usable_features - usable_features.mean(0))/usable_features.std(0)\n",
    "# # shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='dot', max_display=len(traits), show = False)\n",
    "# # plt.savefig('shap/' + str(num_features) + '/summary_plot_hidden_'+ str(hidden) + '_dim_' + str(hidden_dimension) + '.pdf',  bbox_inches='tight')\n",
    "# # shap.summary_plot(shap_values, features=usable_features, feature_names=list(range(23)), plot_type='dot', max_display=len(traits))\n",
    "\n",
    "# shap.summary_plot(shap_values, features = shuffled_X, feature_names = shuffled_X.columns, plot_type='dot', max_display=len(traits), show = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "# # shap.summary_plot(shap_values, features=usable_features, feature_names=list(range(23)), plot_type='bar', max_display=len(traits), show=False)\n",
    "# # plt.savefig('shap/summary_plot_hidden_'+ str(hidden) + '_bar.png', bbox_inches='tight')\n",
    "# # naeem modified\n",
    "# # shap.summary_plot(shap_values, features=usable_features, feature_names=list(range(num_features)), plot_type='bar', max_display=len(traits), show=False)\n",
    "# # shap.summary_plot(shap_values, features=usable_features, feature_names = traits, plot_type='bar', max_display=len(traits), show=False)\n",
    "# # plt.savefig('shap/' + str(num_features) + 'summary_plot_hidden_'+ str(hidden) + '_bar.png', bbox_inches='tight')\n",
    "\n",
    "# shap.summary_plot(shap_values, features = shuffled_X, feature_names = shuffled_X.columns, plot_type='bar', max_display=len(traits), show=False)\n",
    "# plt.savefig('shap/' + str(num_features) + 'summary_plot_hidden_'+ str(hidden) + '_bar.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(shap.force_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision, recall, fscore, auroc, auprc,accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScores(true,pred_binary,pred):\n",
    "    print(true.shape,pred_binary.shape,pred.shape)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "\n",
    "\n",
    "#     print(\"precision\",precision[1],\"recall\", recall[1], \"fscore\",fscore[1], \"auroc\", auroc,\"auprc\", auprc,\"accuracy\" ,acc)\n",
    "    return acc, precision[1], recall[1], fscore, auroc, auprc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of tpot for the sonar classification dataset\n",
    "# from pandas import read_csv\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from tpot import TPOTClassifier\n",
    "\n",
    "# # define model evaluation\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # define search\n",
    "# model = TPOTClassifier(generations=5, population_size=50, cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=-1)\n",
    "# # perform the search\n",
    "# model.fit(shuffled_X, shuffled_Y)\n",
    "# # export the best model\n",
    "# model.export('tpot_sonar_best_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1)\n",
    "# training_features, testing_features, training_target, testing_target = \\\n",
    "#             train_test_split(shuffled_X, shuffled_Y, random_state=1)\n",
    "# print(\"training_features\",X_train.shape)\n",
    "# print(\"testing_features\",X_test.shape)\n",
    "print( shuffled_X.shape)\n",
    "\n",
    "# Average CV score on the training set was: 0.6719885773624091\n",
    "exported_pipeline = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, max_features=1, min_samples_leaf=7, min_samples_split=6, n_estimators=100, subsample=0.45)\n",
    "# Fix random state in exported estimator\n",
    "accuracies=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "fscores=[]\n",
    "aurocs=[]\n",
    "auprcs=[]\n",
    "if hasattr(exported_pipeline, 'random_state'):\n",
    "    setattr(exported_pipeline, 'random_state', random_seed)\n",
    "\n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    y_pred = exported_pipeline.predict(X_test)\n",
    "\n",
    "    true=y_test\n",
    "    pred_binary=y_pred\n",
    "    pred=exported_pipeline.predict_proba(X_test).T\n",
    "    pred=pred[1]\n",
    "    acc,precision, recall,fscore, auroc, auprc=GetScores(true,pred_binary,pred)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "avg_acc=np.mean(accuracies)\n",
    "avg_prec=np.mean(precisions)\n",
    "avg_rec=np.mean(recalls)\n",
    "avg_fsc=np.mean(fscores)\n",
    "avg_roc=np.mean(aurocs)\n",
    "avg_prc=np.mean(auprcs)\n",
    "printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if progression  == \"\":\n",
    "    write_out_to_csv(\"ADNI\", \"XGBoost\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)\n",
    "else:\n",
    "    write_out_to_csv(\"ADNI_\" + progression, \"XGBoost\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.TreeExplainer(exported_pipeline)\n",
    "# shap_values = explainer.shap_values(training_features)\n",
    "\n",
    "# shap.force_plot(explainer.expected_value, shap_values[0,:], training_features.iloc[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shap_values)\n",
    "# print( training_features.shape )\n",
    "# shap.summary_plot(shap_values, training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train, X_test, y_train, y_test = train_test_split(shuffled_X, shuffled_Y, test_size=0.25, random_state=0)\n",
    "\n",
    "# # Preprocess the data\n",
    "# # from sklearn.preprocessing import StandardScaler\n",
    "# # sc = StandardScaler()\n",
    "# # X_train = sc.fit_transform(X_train)\n",
    "# # X_test = sc.transform(X_test)\n",
    "\n",
    "# # Create an SVM object\n",
    "# from sklearn.svm import SVC\n",
    "# classifier = SVC(kernel='linear', random_state=random_seed, probability=True)\n",
    "\n",
    "\n",
    "# accuracies=[]\n",
    "# precisions=[]\n",
    "# recalls=[]\n",
    "# fscores=[]\n",
    "# aurocs=[]\n",
    "# auprcs=[]\n",
    "# for train_index , test_index in kf.split(shuffled):\n",
    "#     X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "#     y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "   \n",
    "#     # Train the model\n",
    "#     classifier.fit(X_train, y_train)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     y_pred = classifier.predict(X_test)\n",
    "#     from sklearn.metrics import confusion_matrix\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "# #     print(cm)\n",
    "\n",
    "#     true=y_test\n",
    "#     pred_binary=y_pred\n",
    "#     pred=classifier.predict_proba(X_test).T\n",
    "#     pred=pred[1]\n",
    "#     acc,precision, recall,fscore, auroc, auprc=GetScores(true,pred_binary,pred)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     fscores.append(fscore)\n",
    "#     aurocs.append(auroc)\n",
    "#     auprcs.append(auprc)\n",
    "# avg_acc=np.mean(accuracies)\n",
    "# avg_prec=np.mean(precisions)\n",
    "# avg_rec=np.mean(recalls)\n",
    "# avg_fsc=np.mean(fscores)\n",
    "# avg_roc=np.mean(aurocs)\n",
    "# avg_prc=np.mean(auprcs)\n",
    "# printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc)\n",
    "# if progression == \"\":\n",
    "#     write_out_to_csv(\"ADNI\", \"SVM\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shap.summary_plot(shap_values, training_features, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,) (23,) (23,)\n",
      "(23,) (23,) (23,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "(22,) (22,) (22,)\n",
      "accuracy: 0.7351778656126482\n",
      "precision: 0.7591921966921967\n",
      "recall: 0.719409756909757\n",
      "fscore: 0.7284123722536224\n",
      "auroc: 0.7732836607836608\n",
      "auprc: 0.7810674584398054\n",
      "Average scores have been appended to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=random_seed)\n",
    "accuracies=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "fscores=[]\n",
    "aurocs=[]\n",
    "auprcs=[]\n",
    "for train_index , test_index in kf.split(shuffled):\n",
    "    X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "    y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred_bin = rfc.predict(X_test)\n",
    "    y_pred_frac= rfc.predict_proba(X_test).T[1]\n",
    "    acc,precision, recall,fscore, auroc, auprc=GetScores(y_test,y_pred_bin,y_pred_frac)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "avg_acc=np.mean(accuracies)\n",
    "avg_prec=np.mean(precisions)\n",
    "avg_rec=np.mean(recalls)\n",
    "avg_fsc=np.mean(fscores)\n",
    "avg_roc=np.mean(aurocs)\n",
    "avg_prc=np.mean(auprcs)\n",
    "printScores(avg_acc,avg_prec,avg_rec,avg_fsc,avg_roc,avg_prc)\n",
    "if progression == \"\":\n",
    "    write_out_to_csv(\"ADNI\", \"RF\", yrs, avg_acc, avg_prec, avg_rec, avg_fsc, avg_roc, avg_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad_venv_2",
   "language": "python",
   "name": "ad_venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
