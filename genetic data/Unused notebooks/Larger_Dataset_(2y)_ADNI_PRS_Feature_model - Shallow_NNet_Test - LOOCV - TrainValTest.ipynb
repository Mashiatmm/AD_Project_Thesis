{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1816, 23),\n",
       " 1816,\n",
       " {'ADNI3_036_S_6231': 0,\n",
       "  'ADNI3_006_S_6277': 1,\n",
       "  'ADNI3_129_S_6146': 2,\n",
       "  'ADNI3_033_S_6352': 3,\n",
       "  'ADNI3_027_S_6183': 4,\n",
       "  'ADNI3_005_S_6427': 5,\n",
       "  'ADNI3_127_S_6147': 6,\n",
       "  'ADNI3_114_S_6251': 7,\n",
       "  'ADNI3_129_S_6228': 8,\n",
       "  'ADNI3_114_S_6309': 9,\n",
       "  'ADNI3_135_S_6110': 10,\n",
       "  'ADNI3_020_S_6358': 11,\n",
       "  'ADNI3_135_S_6411': 12,\n",
       "  'ADNI3_024_S_6202': 13,\n",
       "  'ADNI3_018_S_6414': 14,\n",
       "  'ADNI3_002_S_6103': 15,\n",
       "  'ADNI3_177_S_6408': 16,\n",
       "  'ADNI3_014_S_6148': 17,\n",
       "  'ADNI3_036_S_6466': 18,\n",
       "  'ADNI3_036_S_6134': 19,\n",
       "  'ADNI3_007_S_6455': 20,\n",
       "  'ADNI3_037_S_6271': 21,\n",
       "  'ADNI3_116_S_6100': 22,\n",
       "  'ADNI3_027_S_6327': 23,\n",
       "  'ADNI3_099_S_6097': 24,\n",
       "  'ADNI3_127_S_6330': 25,\n",
       "  'ADNI3_127_S_6168': 26,\n",
       "  'ADNI3_018_S_6351': 27,\n",
       "  'ADNI3_009_S_6212': 28,\n",
       "  'ADNI3_168_S_6180': 29,\n",
       "  'ADNI3_116_S_6119': 30,\n",
       "  'ADNI3_023_S_6346': 31,\n",
       "  'ADNI3_168_S_6065': 32,\n",
       "  'ADNI3_035_S_6200': 33,\n",
       "  'ADNI3_023_S_6399': 34,\n",
       "  'ADNI3_037_S_6125': 35,\n",
       "  'ADNI3_116_S_6428': 36,\n",
       "  'ADNI3_041_S_6192': 37,\n",
       "  'ADNI3_941_S_6333': 38,\n",
       "  'ADNI3_006_S_6209': 39,\n",
       "  'ADNI3_033_S_6497': 40,\n",
       "  'ADNI3_127_S_6436': 41,\n",
       "  'ADNI3_141_S_6178': 42,\n",
       "  'ADNI3_002_S_6456': 43,\n",
       "  'ADNI3_168_S_6062': 44,\n",
       "  'ADNI3_033_S_6298': 45,\n",
       "  'ADNI3_002_S_6053': 46,\n",
       "  'ADNI3_135_S_6359': 47,\n",
       "  'ADNI3_141_S_6015': 48,\n",
       "  'ADNI3_027_S_6317': 49,\n",
       "  'ADNI3_002_S_6009': 50,\n",
       "  'ADNI3_098_S_6343': 51,\n",
       "  'ADNI3_024_S_6033': 52,\n",
       "  'ADNI3_941_S_6384': 53,\n",
       "  'ADNI3_141_S_6008': 54,\n",
       "  'ADNI3_023_S_6400': 55,\n",
       "  'ADNI3_027_S_6034': 56,\n",
       "  'ADNI3_002_S_6404': 57,\n",
       "  'ADNI3_037_S_6031': 58,\n",
       "  'ADNI3_941_S_6017': 59,\n",
       "  'ADNI3_130_S_6329': 60,\n",
       "  'ADNI3_941_S_6044': 61,\n",
       "  'ADNI3_020_S_6470': 62,\n",
       "  'ADNI3_941_S_6052': 63,\n",
       "  'ADNI3_082_S_6197': 64,\n",
       "  'ADNI3_130_S_6019': 65,\n",
       "  'ADNI3_023_S_6356': 66,\n",
       "  'ADNI3_002_S_6030': 67,\n",
       "  'ADNI3_094_S_6419': 68,\n",
       "  'ADNI3_130_S_6037': 69,\n",
       "  'ADNI3_007_S_6310': 70,\n",
       "  'ADNI3_941_S_6054': 71,\n",
       "  'ADNI3_041_S_6354': 72,\n",
       "  'ADNI3_141_S_6041': 73,\n",
       "  'ADNI3_135_S_6360': 74,\n",
       "  'ADNI3_067_S_6045': 75,\n",
       "  'ADNI3_011_S_6367': 76,\n",
       "  'ADNI3_099_S_6016': 77,\n",
       "  'ADNI3_035_S_6306': 78,\n",
       "  'ADNI3_024_S_6005': 79,\n",
       "  'ADNI3_168_S_6318': 80,\n",
       "  'ADNI3_130_S_6047': 81,\n",
       "  'ADNI3_082_S_6287': 82,\n",
       "  'ADNI3_941_S_6068': 83,\n",
       "  'ADNI3_037_S_6222': 84,\n",
       "  'ADNI3_022_S_6013': 85,\n",
       "  'ADNI3_941_S_6422': 86,\n",
       "  'ADNI3_114_S_6039': 87,\n",
       "  'ADNI3_006_S_6375': 88,\n",
       "  'ADNI3_002_S_6066': 89,\n",
       "  'ADNI3_094_S_6250': 90,\n",
       "  'ADNI3_305_S_6188': 91,\n",
       "  'ADNI3_941_S_6471': 92,\n",
       "  'ADNI3_003_S_6258': 93,\n",
       "  'ADNI3_041_S_6159': 94,\n",
       "  'ADNI3_100_S_6349': 95,\n",
       "  'ADNI3_141_S_6240': 96,\n",
       "  'ADNI3_131_S_6170': 97,\n",
       "  'ADNI3_018_S_6207': 98,\n",
       "  'ADNI3_003_S_6256': 99,\n",
       "  'ADNI3_301_S_6224': 100,\n",
       "  'ADNI3_023_S_6334': 101,\n",
       "  'ADNI3_007_S_6255': 102,\n",
       "  'ADNI3_003_S_6260': 103,\n",
       "  'ADNI3_100_S_6164': 104,\n",
       "  'ADNI3_070_S_6394': 105,\n",
       "  'ADNI3_014_S_6145': 106,\n",
       "  'ADNI3_099_S_6396': 107,\n",
       "  'ADNI3_109_S_6213': 108,\n",
       "  'ADNI3_129_S_6452': 109,\n",
       "  'ADNI3_009_S_6163': 110,\n",
       "  'ADNI3_116_S_6543': 111,\n",
       "  'ADNI3_003_S_6158': 112,\n",
       "  'ADNI3_130_S_6161': 113,\n",
       "  'ADNI3_006_S_6252': 114,\n",
       "  'ADNI3_135_S_6446': 115,\n",
       "  'ADNI3_014_S_6199': 116,\n",
       "  'ADNI3_130_S_6319': 117,\n",
       "  'ADNI3_129_S_6244': 118,\n",
       "  'ADNI3_032_S_6279': 119,\n",
       "  'ADNI3_006_S_6243': 120,\n",
       "  'ADNI3_301_S_6326': 121,\n",
       "  'ADNI3_114_S_6113': 122,\n",
       "  'ADNI3_007_S_6323': 123,\n",
       "  'ADNI3_153_S_6274': 124,\n",
       "  'ADNI3_094_S_6269': 125,\n",
       "  'ADNI3_037_S_6141': 126,\n",
       "  'ADNI3_037_S_6187': 127,\n",
       "  'ADNI3_168_S_6049': 128,\n",
       "  'ADNI3_177_S_6335': 129,\n",
       "  'ADNI3_033_S_6266': 130,\n",
       "  'ADNI3_129_S_6459': 131,\n",
       "  'ADNI3_130_S_6372': 132,\n",
       "  'ADNI3_141_S_6253': 133,\n",
       "  'ADNI3_006_S_6500': 134,\n",
       "  'ADNI3_130_S_6072': 135,\n",
       "  'ADNI3_082_S_6283': 136,\n",
       "  'ADNI3_141_S_6061': 137,\n",
       "  'ADNI3_007_S_6341': 138,\n",
       "  'ADNI3_007_S_6120': 139,\n",
       "  'ADNI3_094_S_6275': 140,\n",
       "  'ADNI3_941_S_6080': 141,\n",
       "  'ADNI3_016_S_6381': 142,\n",
       "  'ADNI3_941_S_6094': 143,\n",
       "  'ADNI3_941_S_6345': 144,\n",
       "  'ADNI3_003_S_6067': 145,\n",
       "  'ADNI3_023_S_6374': 146,\n",
       "  'ADNI3_012_S_6073': 147,\n",
       "  'ADNI3_037_S_6216': 148,\n",
       "  'ADNI3_135_S_6104': 149,\n",
       "  'ADNI3_168_S_6320': 150,\n",
       "  'ADNI3_067_S_6117': 151,\n",
       "  'ADNI3_041_S_6401': 152,\n",
       "  'ADNI3_014_S_6076': 153,\n",
       "  'ADNI3_011_S_6418': 154,\n",
       "  'ADNI3_036_S_6088': 155,\n",
       "  'ADNI3_003_S_6432': 156,\n",
       "  'ADNI3_037_S_6083': 157,\n",
       "  'ADNI3_116_S_6550': 158,\n",
       "  'ADNI3_130_S_6043': 159,\n",
       "  'ADNI3_070_S_6386': 160,\n",
       "  'ADNI3_127_S_6024': 161,\n",
       "  'ADNI3_027_S_6370': 162,\n",
       "  'ADNI3_023_S_6369': 163,\n",
       "  'ADNI3_130_S_6035': 164,\n",
       "  'ADNI3_168_S_6371': 165,\n",
       "  'ADNI3_099_S_6038': 166,\n",
       "  'ADNI3_168_S_6350': 167,\n",
       "  'ADNI3_301_S_6056': 168,\n",
       "  'ADNI3_099_S_6025': 169,\n",
       "  'ADNI3_135_S_6389': 170,\n",
       "  'ADNI3_130_S_6105': 171,\n",
       "  'ADNI3_127_S_6241': 172,\n",
       "  'ADNI3_022_S_6069': 173,\n",
       "  'ADNI3_009_S_6402': 174,\n",
       "  'ADNI3_941_S_6058': 175,\n",
       "  'ADNI3_035_S_6488': 176,\n",
       "  'ADNI3_141_S_6116': 177,\n",
       "  'ADNI3_116_S_6439': 178,\n",
       "  'ADNI3_116_S_6133': 179,\n",
       "  'ADNI3_037_S_6204': 180,\n",
       "  'ADNI3_114_S_6429': 181,\n",
       "  'ADNI3_041_S_6292': 182,\n",
       "  'ADNI3_013_S_6206': 183,\n",
       "  'ADNI3_153_S_6336': 184,\n",
       "  'ADNI3_014_S_6437': 185,\n",
       "  'ADNI3_100_S_6273': 186,\n",
       "  'ADNI3_027_S_6577': 187,\n",
       "  'ADNI3_941_S_6575': 188,\n",
       "  'ADNI3_036_S_6189': 189,\n",
       "  'ADNI3_135_S_6545': 190,\n",
       "  'ADNI3_036_S_6179': 191,\n",
       "  'ADNI3_135_S_6473': 192,\n",
       "  'ADNI3_029_S_6289': 193,\n",
       "  'ADNI3_014_S_6522': 194,\n",
       "  'ADNI3_019_S_6186': 195,\n",
       "  'ADNI3_014_S_6502': 196,\n",
       "  'ADNI3_070_S_6236': 197,\n",
       "  'ADNI3_027_S_6582': 198,\n",
       "  'ADNI3_168_S_6085': 199,\n",
       "  'ADNI3_019_S_6573': 200,\n",
       "  'ADNI3_003_S_6259': 201,\n",
       "  'ADNI3_305_S_6498': 202,\n",
       "  'ADNI3_003_S_6014': 203,\n",
       "  'ADNI3_100_S_6578': 204,\n",
       "  'ADNI3_021_S_6312': 205,\n",
       "  'ADNI3_135_S_6544': 206,\n",
       "  'ADNI3_014_S_6424': 207,\n",
       "  'ADNI3_305_S_6313': 208,\n",
       "  'ADNI3_127_S_6549': 209,\n",
       "  'ADNI3_131_S_6143': 210,\n",
       "  'ADNI3_127_S_6512': 211,\n",
       "  'ADNI3_127_S_6232': 212,\n",
       "  'ADNI3_033_S_6572': 213,\n",
       "  'ADNI3_041_S_6226': 214,\n",
       "  'ADNI3_003_S_6264': 215,\n",
       "  'ADNI3_032_S_6211': 216,\n",
       "  'ADNI3_023_S_6547': 217,\n",
       "  'ADNI3_130_S_6111': 218,\n",
       "  'ADNI3_141_S_6423': 219,\n",
       "  'ADNI3_020_S_6282': 220,\n",
       "  'ADNI3_067_S_6529': 221,\n",
       "  'ADNI3_011_S_6303': 222,\n",
       "  'ADNI3_020_S_6566': 223,\n",
       "  'ADNI3_099_S_6175': 224,\n",
       "  'ADNI3_037_S_6377': 225,\n",
       "  'ADNI3_002_S_6007': 226,\n",
       "  'ADNI3_003_S_6307': 227,\n",
       "  'ADNI3_037_S_6046': 228,\n",
       "  'ADNI3_003_S_6479': 229,\n",
       "  'ADNI3_305_S_6157': 230,\n",
       "  'ADNI3_007_S_6515': 231,\n",
       "  'ADNI3_941_S_6581': 232,\n",
       "  'ADNI3_127_S_6173': 233,\n",
       "  'ADNI3_094_S_6417': 234,\n",
       "  'ADNI3_024_S_6385': 235,\n",
       "  'ADNI3_020_S_6227': 236,\n",
       "  'ADNI3_168_S_6413': 237,\n",
       "  'ADNI3_135_S_6284': 238,\n",
       "  'ADNI3_024_S_6472': 239,\n",
       "  'ADNI3_020_S_6185': 240,\n",
       "  'ADNI3_123_S_6118': 241,\n",
       "  'ADNI3_941_S_6514': 242,\n",
       "  'ADNI3_019_S_6315': 243,\n",
       "  'ADNI3_141_S_6075': 244,\n",
       "  'ADNI3_012_S_6503': 245,\n",
       "  'ADNI3_100_S_6308': 246,\n",
       "  'ADNI3_135_S_6586': 247,\n",
       "  'ADNI3_005_S_6084': 248,\n",
       "  'ADNI3_027_S_6001': 249,\n",
       "  'ADNI3_177_S_6448': 250,\n",
       "  'ADNI3_053_S_6598': 251,\n",
       "  'ADNI3_027_S_6002': 252,\n",
       "  'ADNI3_168_S_6467': 253,\n",
       "  'ADNI3_006_S_6234': 254,\n",
       "  'ADNI3_003_S_6268': 255,\n",
       "  'ADNI3_168_S_6426': 256,\n",
       "  'ADNI3_067_S_6442': 257,\n",
       "  'ADNI3_127_S_6348': 258,\n",
       "  'ADNI3_020_S_6513': 259,\n",
       "  'ADNI3_067_S_6138': 260,\n",
       "  'ADNI3_135_S_6510': 261,\n",
       "  'ADNI3_168_S_6151': 262,\n",
       "  'ADNI3_011_S_6465': 263,\n",
       "  'ADNI3_019_S_6483': 264,\n",
       "  'ADNI3_941_S_6454': 265,\n",
       "  'ADNI3_094_S_6485': 266,\n",
       "  'ADNI3_168_S_6321': 267,\n",
       "  'ADNI3_067_S_6474': 268,\n",
       "  'ADNI3_007_S_6421': 269,\n",
       "  'ADNI3_114_S_6347': 270,\n",
       "  'ADNI3_141_S_6416': 271,\n",
       "  'ADNI3_129_S_6457': 272,\n",
       "  'ADNI3_027_S_6463': 273,\n",
       "  'ADNI3_094_S_6468': 274,\n",
       "  'ADNI3_177_S_6409': 275,\n",
       "  'ADNI3_032_S_6294': 276,\n",
       "  'ADNI3_341_S_6494': 277,\n",
       "  'ADNI3_036_S_6316': 278,\n",
       "  'ADNI3_035_S_6480': 279,\n",
       "  'ADNI3_006_S_6441': 280,\n",
       "  'ADNI3_127_S_6357': 281,\n",
       "  'ADNI3_027_S_6516': 282,\n",
       "  'ADNI3_941_S_6580': 283,\n",
       "  'ADNI3_941_S_6546': 284,\n",
       "  'ADNI3_127_S_6433': 285,\n",
       "  'ADNI3_100_S_6493': 286,\n",
       "  'ADNI3_019_S_6533': 287,\n",
       "  'ADNI3_168_S_6233': 288,\n",
       "  'ADNI3_126_S_6559': 289,\n",
       "  'ADNI3_168_S_6492': 290,\n",
       "  'ADNI3_131_S_6519': 291,\n",
       "  'ADNI3_099_S_6476': 292,\n",
       "  'ADNI3_020_S_6504': 293,\n",
       "  'ADNI3_098_S_6534': 294,\n",
       "  'ADNI3_041_S_6314': 295,\n",
       "  'ADNI3_941_S_6499': 296,\n",
       "  'ADNI3_023_S_6535': 297,\n",
       "  'ADNI3_305_S_6378': 298,\n",
       "  'ADNI3_082_S_6415': 299,\n",
       "  '4_024_S_0985': 300,\n",
       "  '6_131_S_0123': 301,\n",
       "  '7_098_S_0160': 302,\n",
       "  '8_027_S_0256': 303,\n",
       "  '9_116_S_1243': 304,\n",
       "  '17_011_S_0002': 305,\n",
       "  '18_003_S_0907': 306,\n",
       "  '26_052_S_1346': 307,\n",
       "  '27_012_S_4026': 308,\n",
       "  '28_037_S_4030': 309,\n",
       "  '29_073_S_2182': 310,\n",
       "  '30_116_S_4167': 311,\n",
       "  '32_073_S_0089': 312,\n",
       "  '33_082_S_2099': 313,\n",
       "  '36_021_S_2100': 314,\n",
       "  '37_127_S_1427': 315,\n",
       "  '38_023_S_0926': 316,\n",
       "  '39_137_S_4672': 317,\n",
       "  '41_033_S_0920': 318,\n",
       "  '43_137_S_1414': 319,\n",
       "  '45_128_S_1408': 320,\n",
       "  '47_072_S_2027': 321,\n",
       "  '48_128_S_0545': 322,\n",
       "  '50_021_S_0626': 323,\n",
       "  '51_016_S_0702': 324,\n",
       "  '52_136_S_0695': 325,\n",
       "  '53_051_S_1072': 326,\n",
       "  '54_014_S_0558': 327,\n",
       "  '55_136_S_0873': 328,\n",
       "  '57_002_S_0729': 329,\n",
       "  '58_131_S_0384': 330,\n",
       "  '61_014_S_0563': 331,\n",
       "  '62_029_S_0845': 332,\n",
       "  '63_007_S_0068': 333,\n",
       "  '64_027_S_2219': 334,\n",
       "  '65_021_S_2142': 335,\n",
       "  '67_011_S_1080': 336,\n",
       "  '69_127_S_1032': 337,\n",
       "  '70_014_S_0169': 338,\n",
       "  '71_027_S_1045': 339,\n",
       "  '72_082_S_0832': 340,\n",
       "  '74_082_S_4208': 341,\n",
       "  '75_068_S_4174': 342,\n",
       "  '77_051_S_1331': 343,\n",
       "  '78_029_S_2376': 344,\n",
       "  '79_031_S_4149': 345,\n",
       "  '80_153_S_4151': 346,\n",
       "  '81_098_S_2052': 347,\n",
       "  '82_007_S_2058': 348,\n",
       "  '83_007_S_0128': 349,\n",
       "  '84_100_S_1226': 350,\n",
       "  '85_127_S_0259': 351,\n",
       "  '86_100_S_0296': 352,\n",
       "  '87_031_S_4029': 353,\n",
       "  '88_041_S_4051': 354,\n",
       "  '90_098_S_4059': 355,\n",
       "  '91_016_S_2031': 356,\n",
       "  '92_941_S_4036': 357,\n",
       "  '93_068_S_4134': 358,\n",
       "  '94_003_S_4119': 359,\n",
       "  '95_141_S_4160': 360,\n",
       "  '96_041_S_1418': 361,\n",
       "  '97_033_S_4176': 362,\n",
       "  '98_941_S_4100': 363,\n",
       "  '99_153_S_2148': 364,\n",
       "  '134_014_S_4058': 365,\n",
       "  '135_099_S_4076': 366,\n",
       "  '137_027_S_0644': 367,\n",
       "  '153_002_S_2043': 368,\n",
       "  '174_037_S_0303': 369,\n",
       "  '176_041_S_4138': 370,\n",
       "  '189_023_S_4448': 371,\n",
       "  '190_128_S_4571': 372,\n",
       "  '191_012_S_4545': 373,\n",
       "  '192_035_S_4414': 374,\n",
       "  '194_109_S_4499': 375,\n",
       "  '195_126_S_4514': 376,\n",
       "  '200_014_S_0548': 377,\n",
       "  '205_153_S_4159': 378,\n",
       "  '217_051_S_1123': 379,\n",
       "  '219_098_S_4095': 380,\n",
       "  '222_141_S_0915': 381,\n",
       "  '223_126_S_2407': 382,\n",
       "  '226_037_S_4410': 383,\n",
       "  '227_006_S_4357': 384,\n",
       "  '229_018_S_2155': 385,\n",
       "  '230_013_S_4268': 386,\n",
       "  '231_021_S_4402': 387,\n",
       "  '232_141_S_4438': 388,\n",
       "  '233_019_S_4477': 389,\n",
       "  '234_068_S_4424': 390,\n",
       "  '235_141_S_4456': 391,\n",
       "  '238_130_S_4417': 392,\n",
       "  '239_153_S_4372': 393,\n",
       "  '240_130_S_4294': 394,\n",
       "  '242_021_S_4335': 395,\n",
       "  '243_073_S_4360': 396,\n",
       "  '244_021_S_4421': 397,\n",
       "  '245_053_S_2396': 398,\n",
       "  '247_032_S_0479': 399,\n",
       "  '249_018_S_4313': 400,\n",
       "  '250_018_S_2133': 401,\n",
       "  '254_006_S_4449': 402,\n",
       "  '259_072_S_4465': 403,\n",
       "  '260_011_S_4547': 404,\n",
       "  '262_053_S_4578': 405,\n",
       "  '264_037_S_4381': 406,\n",
       "  '265_041_S_4513': 407,\n",
       "  '266_109_S_4455': 408,\n",
       "  '267_007_S_4611': 409,\n",
       "  '268_100_S_4469': 410,\n",
       "  '269_022_S_4291': 411,\n",
       "  '270_036_S_4562': 412,\n",
       "  '271_130_S_4605': 413,\n",
       "  '272_153_S_4621': 414,\n",
       "  '273_130_S_4641': 415,\n",
       "  '275_023_S_4502': 416,\n",
       "  '282_067_S_4072': 417,\n",
       "  '283_005_S_2390': 418,\n",
       "  '284_023_S_4034': 419,\n",
       "  '285_022_S_4196': 420,\n",
       "  '286_099_S_4157': 421,\n",
       "  '287_002_S_2073': 422,\n",
       "  '289_128_S_2036': 423,\n",
       "  '291_014_S_4263': 424,\n",
       "  '292_009_S_4564': 425,\n",
       "  '294_127_S_1419': 426,\n",
       "  '300_073_S_2191': 427,\n",
       "  '302_127_S_0260': 428,\n",
       "  '307_136_S_0186': 429,\n",
       "  '309_023_S_4164': 430,\n",
       "  '313_068_S_4340': 431,\n",
       "  '314_011_S_4278': 432,\n",
       "  '315_129_S_4396': 433,\n",
       "  '317_116_S_4338': 434,\n",
       "  '320_130_S_4343': 435,\n",
       "  '324_129_S_4371': 436,\n",
       "  '326_127_S_0112': 437,\n",
       "  '329_099_S_4104': 438,\n",
       "  '330_014_S_4039': 439,\n",
       "  '333_016_S_4121': 440,\n",
       "  '337_003_S_4350': 441,\n",
       "  '338_013_S_4580': 442,\n",
       "  '339_126_S_4494': 443,\n",
       "  '341_141_S_2210': 444,\n",
       "  '343_032_S_4429': 445,\n",
       "  '344_009_S_4530': 446,\n",
       "  '346_002_S_4473': 447,\n",
       "  '347_007_S_1206': 448,\n",
       "  '348_073_S_4540': 449,\n",
       "  '350_053_S_4557': 450,\n",
       "  '351_036_S_4491': 451,\n",
       "  '352_007_S_0101': 452,\n",
       "  '353_019_S_4548': 453,\n",
       "  '354_137_S_4482': 454,\n",
       "  '355_082_S_4428': 455,\n",
       "  '356_006_S_4363': 456,\n",
       "  '357_135_S_4446': 457,\n",
       "  '358_009_S_4359': 458,\n",
       "  '362_018_S_4597': 459,\n",
       "  '365_130_S_4589': 460,\n",
       "  '366_009_S_4612': 461,\n",
       "  '369_003_S_0981': 462,\n",
       "  '370_007_S_4568': 463,\n",
       "  '371_135_S_4598': 464,\n",
       "  '372_137_S_4520': 465,\n",
       "  '374_072_S_4539': 466,\n",
       "  '375_073_S_4552': 467,\n",
       "  '376_128_S_4603': 468,\n",
       "  '377_135_S_4566': 469,\n",
       "  '378_014_S_4615': 470,\n",
       "  '379_037_S_4146': 471,\n",
       "  '380_016_S_2007': 472,\n",
       "  '381_035_S_4582': 473,\n",
       "  '382_135_S_4489': 474,\n",
       "  '383_007_S_4467': 475,\n",
       "  '387_130_S_4405': 476,\n",
       "  '388_126_S_4507': 477,\n",
       "  '389_941_S_4365': 478,\n",
       "  '392_130_S_4468': 479,\n",
       "  '393_036_S_4430': 480,\n",
       "  '394_005_S_4707': 481,\n",
       "  '397_022_S_1097': 482,\n",
       "  '398_041_S_4037': 483,\n",
       "  '399_141_S_4053': 484,\n",
       "  '401_023_S_4122': 485,\n",
       "  '402_052_S_4626': 486,\n",
       "  '403_023_S_4501': 487,\n",
       "  '404_037_S_4071': 488,\n",
       "  '405_094_S_4234': 489,\n",
       "  '406_137_S_4331': 490,\n",
       "  '409_032_S_4386': 491,\n",
       "  '410_022_S_4266': 492,\n",
       "  '411_094_S_4434': 493,\n",
       "  '414_068_S_4431': 494,\n",
       "  '415_007_S_4516': 495,\n",
       "  '416_016_S_4584': 496,\n",
       "  '417_006_S_4546': 497,\n",
       "  '418_114_S_4379': 498,\n",
       "  '420_116_S_4625': 499,\n",
       "  '421_022_S_4444': 500,\n",
       "  '422_041_S_4510': 501,\n",
       "  '423_099_S_4475': 502,\n",
       "  '424_037_S_4308': 503,\n",
       "  '425_029_S_4385': 504,\n",
       "  '426_072_S_4462': 505,\n",
       "  '427_007_S_4488': 506,\n",
       "  '428_002_S_4654': 507,\n",
       "  '429_037_S_4432': 508,\n",
       "  '430_073_S_4614': 509,\n",
       "  '431_073_S_4559': 510,\n",
       "  '432_136_S_4408': 511,\n",
       "  '434_014_S_4668': 512,\n",
       "  '435_013_S_4595': 513,\n",
       "  '436_041_S_4629': 514,\n",
       "  '437_128_S_4653': 515,\n",
       "  '440_020_S_1288': 516,\n",
       "  '442_116_S_0752': 517,\n",
       "  '444_012_S_4188': 518,\n",
       "  '445_021_S_4254': 519,\n",
       "  '446_137_S_4299': 520,\n",
       "  '447_031_S_4590': 521,\n",
       "  '449_006_S_0498': 522,\n",
       "  '451_072_S_2083': 523,\n",
       "  '453_068_S_2184': 524,\n",
       "  '455_094_S_2238': 525,\n",
       "  '457_127_S_2234': 526,\n",
       "  '458_031_S_2233': 527,\n",
       "  '459_022_S_2263': 528,\n",
       "  '461_022_S_0130': 529,\n",
       "  '462_098_S_4018': 530,\n",
       "  '463_072_S_4007': 531,\n",
       "  '464_037_S_4015': 532,\n",
       "  '465_037_S_4001': 533,\n",
       "  '466_130_S_2373': 534,\n",
       "  '468_002_S_4251': 535,\n",
       "  '469_023_S_2068': 536,\n",
       "  '470_072_S_4206': 537,\n",
       "  '472_099_S_4202': 538,\n",
       "  '474_129_S_4422': 539,\n",
       "  '476_029_S_4327': 540,\n",
       "  '477_082_S_1256': 541,\n",
       "  '478_099_S_4498': 542,\n",
       "  '479_006_S_4153': 543,\n",
       "  '480_098_S_2079': 544,\n",
       "  '481_137_S_0800': 545,\n",
       "  '483_098_S_2047': 546,\n",
       "  '484_941_S_4187': 547,\n",
       "  '485_002_S_4225': 548,\n",
       "  '487_002_S_4237': 549,\n",
       "  '488_070_S_4692': 550,\n",
       "  '489_018_S_4349': 551,\n",
       "  '490_137_S_4303': 552,\n",
       "  '491_035_S_2061': 553,\n",
       "  '492_041_S_4271': 554,\n",
       "  '493_135_S_4309': 555,\n",
       "  '495_009_S_4388': 556,\n",
       "  '498_024_S_4392': 557,\n",
       "  '499_035_S_4256': 558,\n",
       "  '500_098_S_0896': 559,\n",
       "  '501_016_S_4353': 560,\n",
       "  '503_130_S_4250': 561,\n",
       "  '504_130_S_4542': 562,\n",
       "  '505_003_S_0908': 563,\n",
       "  '508_127_S_2213': 564,\n",
       "  '509_018_S_2138': 565,\n",
       "  '510_027_S_0120': 566,\n",
       "  '511_128_S_2151': 567,\n",
       "  '512_032_S_2247': 568,\n",
       "  '513_027_S_1387': 569,\n",
       "  '514_098_S_0172': 570,\n",
       "  '515_014_S_2308': 571,\n",
       "  '516_023_S_1190': 572,\n",
       "  '517_005_S_0448': 573,\n",
       "  '518_068_S_2316': 574,\n",
       "  '520_007_S_4272': 575,\n",
       "  '521_098_S_4275': 576,\n",
       "  '522_068_S_4332': 577,\n",
       "  '523_018_S_4400': 578,\n",
       "  '524_072_S_4383': 579,\n",
       "  '525_153_S_4297': 580,\n",
       "  '526_006_S_4515': 581,\n",
       "  '528_023_S_4241': 582,\n",
       "  '529_016_S_4591': 583,\n",
       "  '530_128_S_4553': 584,\n",
       "  '531_009_S_4543': 585,\n",
       "  '532_127_S_4604': 586,\n",
       "  '533_127_S_4500': 587,\n",
       "  '534_941_S_4420': 588,\n",
       "  '535_099_S_4480': 589,\n",
       "  '536_094_S_4503': 590,\n",
       "  '537_098_S_4506': 591,\n",
       "  '538_072_S_4522': 592,\n",
       "  '539_128_S_4599': 593,\n",
       "  '541_116_S_4043': 594,\n",
       "  '542_073_S_0311': 595,\n",
       "  '543_019_S_4285': 596,\n",
       "  '544_032_S_4348': 597,\n",
       "  '545_021_S_4659': 598,\n",
       "  '546_027_S_2183': 599,\n",
       "  '547_014_S_2185': 600,\n",
       "  '548_022_S_1351': 601,\n",
       "  '549_068_S_0127': 602,\n",
       "  '550_068_S_0872': 603,\n",
       "  '551_014_S_4401': 604,\n",
       "  '552_006_S_4346': 605,\n",
       "  '553_114_S_0378': 606,\n",
       "  '554_141_S_4426': 607,\n",
       "  '555_099_S_4463': 608,\n",
       "  '556_073_S_4443': 609,\n",
       "  '557_127_S_4240': 610,\n",
       "  '558_094_S_4560': 611,\n",
       "  '559_007_S_4620': 612,\n",
       "  '560_019_S_4549': 613,\n",
       "  '561_067_S_4310': 614,\n",
       "  '562_141_S_4232': 615,\n",
       "  '564_006_S_1130': 616,\n",
       "  '565_057_S_2398': 617,\n",
       "  '568_136_S_4269': 618,\n",
       "  '569_126_S_4458': 619,\n",
       "  '570_072_S_4445': 620,\n",
       "  '571_116_S_4453': 621,\n",
       "  '572_114_S_4404': 622,\n",
       "  '573_072_S_4394': 623,\n",
       "  '574_031_S_4474': 624,\n",
       "  '575_072_S_4613': 625,\n",
       "  '576_116_S_4635': 626,\n",
       "  '577_127_S_4645': 627,\n",
       "  '579_136_S_4433': 628,\n",
       "  '581_128_S_4609': 629,\n",
       "  '582_137_S_4596': 630,\n",
       "  '583_141_S_2333': 631,\n",
       "  '585_007_S_4637': 632,\n",
       "  '588_032_S_1169': 633,\n",
       "  '589_099_S_1034': 634,\n",
       "  '591_116_S_4092': 635,\n",
       "  '592_136_S_0107': 636,\n",
       "  '594_002_S_4270': 637,\n",
       "  '595_035_S_2074': 638,\n",
       "  '596_127_S_4301': 639,\n",
       "  '597_099_S_4205': 640,\n",
       "  '598_082_S_4339': 641,\n",
       "  '599_032_S_2119': 642,\n",
       "  '603_009_S_4337': 643,\n",
       "  '604_012_S_1212': 644,\n",
       "  '605_027_S_2336': 645,\n",
       "  '606_068_S_2315': 646,\n",
       "  '607_022_S_1394': 647,\n",
       "  '609_057_S_1007': 648,\n",
       "  '610_082_S_4224': 649,\n",
       "  '612_135_S_4281': 650,\n",
       "  '613_130_S_4415': 651,\n",
       "  '615_137_S_4536': 652,\n",
       "  '616_135_S_4657': 653,\n",
       "  '617_023_S_0376': 654,\n",
       "  '618_012_S_4643': 655,\n",
       "  '619_099_S_4565': 656,\n",
       "  '623_018_S_0055': 657,\n",
       "  '624_023_S_0625': 658,\n",
       "  '626_021_S_2125': 659,\n",
       "  '627_007_S_2106': 660,\n",
       "  '630_022_S_2087': 661,\n",
       "  '631_036_S_0869': 662,\n",
       "  '634_033_S_1016': 663,\n",
       "  '635_005_S_0546': 664,\n",
       "  '636_033_S_1098': 665,\n",
       "  '637_033_S_0922': 666,\n",
       "  '641_072_S_2093': 667,\n",
       "  '642_072_S_2037': 668,\n",
       "  '643_033_S_0906': 669,\n",
       "  '644_094_S_1417': 670,\n",
       "  '646_005_S_0610': 671,\n",
       "  '647_127_S_0925': 672,\n",
       "  '648_137_S_0972': 673,\n",
       "  '649_036_S_0672': 674,\n",
       "  '650_128_S_2011': 675,\n",
       "  '651_033_S_1116': 676,\n",
       "  '652_005_S_0602': 677,\n",
       "  '653_052_S_2249': 678,\n",
       "  '654_035_S_2199': 679,\n",
       "  '655_021_S_2150': 680,\n",
       "  '656_072_S_0315': 681,\n",
       "  '657_082_S_2307': 682,\n",
       "  '658_002_S_1261': 683,\n",
       "  '659_123_S_1300': 684,\n",
       "  '660_037_S_4028': 685,\n",
       "  '661_072_S_4063': 686,\n",
       "  '662_036_S_1023': 687,\n",
       "  '663_011_S_4075': 688,\n",
       "  '665_027_S_0074': 689,\n",
       "  '666_128_S_2002': 690,\n",
       "  '667_072_S_2026': 691,\n",
       "  '670_016_S_0359': 692,\n",
       "  '671_031_S_0351': 693,\n",
       "  '672_127_S_0622': 694,\n",
       "  '673_014_S_0520': 695,\n",
       "  '674_135_S_4676': 696,\n",
       "  '675_014_S_0658': 697,\n",
       "  '676_012_S_1133': 698,\n",
       "  '678_128_S_2003': 699,\n",
       "  '679_003_S_1074': 700,\n",
       "  '680_016_S_1117': 701,\n",
       "  '682_099_S_2146': 702,\n",
       "  '683_037_S_0501': 703,\n",
       "  '684_031_S_2022': 704,\n",
       "  '687_073_S_2190': 705,\n",
       "  '689_068_S_2168': 706,\n",
       "  '691_037_S_0377': 707,\n",
       "  '692_067_S_2195': 708,\n",
       "  '695_057_S_1269': 709,\n",
       "  '696_129_S_1246': 710,\n",
       "  '697_031_S_4005': 711,\n",
       "  '698_067_S_2304': 712,\n",
       "  '699_129_S_2332': 713,\n",
       "  '700_022_S_4173': 714,\n",
       "  '701_006_S_4150': 715,\n",
       "  '702_002_S_0685': 716,\n",
       "  '703_031_S_0618': 717,\n",
       "  '704_031_S_4203': 718,\n",
       "  '709_003_S_4152': 719,\n",
       "  '710_002_S_4171': 720,\n",
       "  '711_009_S_4324': 721,\n",
       "  '713_128_S_2045': 722,\n",
       "  '714_009_S_0842': 723,\n",
       "  '715_128_S_2220': 724,\n",
       "  '716_068_S_2248': 725,\n",
       "  '717_022_S_2167': 726,\n",
       "  '718_094_S_4162': 727,\n",
       "  '719_023_S_4035': 728,\n",
       "  '721_031_S_0830': 729,\n",
       "  '723_031_S_4218': 730,\n",
       "  '725_127_S_4148': 731,\n",
       "  '726_072_S_4103': 732,\n",
       "  '727_033_S_0741': 733,\n",
       "  '728_037_S_0566': 734,\n",
       "  '729_024_S_4280': 735,\n",
       "  '730_082_S_2121': 736,\n",
       "  '731_013_S_1186': 737,\n",
       "  '732_011_S_1282': 738,\n",
       "  '734_036_S_0945': 739,\n",
       "  '735_100_S_0069': 740,\n",
       "  '736_123_S_0298': 741,\n",
       "  '737_032_S_0214': 742,\n",
       "  '738_041_S_4060': 743,\n",
       "  '739_126_S_4686': 744,\n",
       "  '740_137_S_0459': 745,\n",
       "  '741_041_S_0679': 746,\n",
       "  '742_072_S_4057': 747,\n",
       "  '743_116_S_4195': 748,\n",
       "  '744_019_S_4252': 749,\n",
       "  '745_073_S_4155': 750,\n",
       "  '746_002_S_4262': 751,\n",
       "  '747_041_S_1260': 752,\n",
       "  '748_053_S_2357': 753,\n",
       "  '749_123_S_2363': 754,\n",
       "  '750_099_S_0352': 755,\n",
       "  '751_009_S_2381': 756,\n",
       "  '752_098_S_4003': 757,\n",
       "  '753_016_S_1326': 758,\n",
       "  '755_126_S_2360': 759,\n",
       "  '756_018_S_4696': 760,\n",
       "  '757_130_S_0289': 761,\n",
       "  '758_051_S_1040': 762,\n",
       "  '759_031_S_4042': 763,\n",
       "  '760_007_S_2394': 764,\n",
       "  '761_137_S_0722': 765,\n",
       "  '762_116_S_0657': 766,\n",
       "  '763_100_S_0047': 767,\n",
       "  '764_018_S_2180': 768,\n",
       "  '765_137_S_0973': 769,\n",
       "  '766_098_S_0171': 770,\n",
       "  '767_131_S_0441': 771,\n",
       "  '768_129_S_4073': 772,\n",
       "  '769_068_S_0802': 773,\n",
       "  '770_023_S_4115': 774,\n",
       "  '771_037_S_0588': 775,\n",
       "  '772_037_S_0150': 776,\n",
       "  '773_051_S_1131': 777,\n",
       "  '774_011_S_4366': 778,\n",
       "  '775_072_S_2116': 779,\n",
       "  '776_035_S_0555': 780,\n",
       "  '777_126_S_0605': 781,\n",
       "  '778_068_S_4061': 782,\n",
       "  '780_033_S_4179': 783,\n",
       "  '781_024_S_4158': 784,\n",
       "  '782_029_S_2395': 785,\n",
       "  '783_153_S_4172': 786,\n",
       "  '784_005_S_4185': 787,\n",
       "  '785_067_S_4212': 788,\n",
       "  '787_006_S_4192': 789,\n",
       "  '788_041_S_4200': 790,\n",
       "  '789_128_S_0272': 791,\n",
       "  '790_100_S_1286': 792,\n",
       "  '791_041_S_4014': 793,\n",
       "  '792_031_S_4024': 794,\n",
       "  '793_052_S_0671': 795,\n",
       "  '794_073_S_4216': 796,\n",
       "  '795_082_S_4090': 797,\n",
       "  '797_114_S_2392': 798,\n",
       "  '798_041_S_4041': 799,\n",
       "  '799_037_S_4214': 800,\n",
       "  '800_035_S_4114': 801,\n",
       "  '801_123_S_4096': 802,\n",
       "  '802_005_S_4168': 803,\n",
       "  '803_072_S_4131': 804,\n",
       "  '804_032_S_2240': 805,\n",
       "  '806_002_S_1268': 806,\n",
       "  '807_021_S_0276': 807,\n",
       "  '808_098_S_4050': 808,\n",
       "  '809_123_S_4127': 809,\n",
       "  '810_123_S_4170': 810,\n",
       "  '811_099_S_0051': 811,\n",
       "  '813_099_S_2042': 812,\n",
       "  '814_073_S_4259': 813,\n",
       "  '815_037_S_4302': 814,\n",
       "  '817_082_S_4244': 815,\n",
       "  '818_135_S_4356': 816,\n",
       "  '101_018_S_0682': 817,\n",
       "  '103_021_S_0424': 818,\n",
       "  '106_006_S_0484': 819,\n",
       "  '107_133_S_0629': 820,\n",
       "  '108_128_S_1409': 821,\n",
       "  '109_099_S_0040': 822,\n",
       "  '10_005_S_1224': 823,\n",
       "  '110_068_S_0442': 824,\n",
       "  '114_073_S_0565': 825,\n",
       "  '115_131_S_0497': 826,\n",
       "  '116_033_S_0723': 827,\n",
       "  '117_130_S_0449': 828,\n",
       "  '118_033_S_0516': 829,\n",
       "  '121_137_S_0438': 830,\n",
       "  '123_018_S_0450': 831,\n",
       "  '124_041_S_0721': 832,\n",
       "  '126_012_S_1009': 833,\n",
       "  '128_022_S_0044': 834,\n",
       "  '12_010_S_0472': 835,\n",
       "  '130_941_S_1311': 836,\n",
       "  '131_136_S_0874': 837,\n",
       "  '132_053_S_0621': 838,\n",
       "  '134_029_S_1215': 839,\n",
       "  '135_067_S_1185': 840,\n",
       "  '136_067_S_0038': 841,\n",
       "  '138_067_S_0019': 842,\n",
       "  '141_007_S_0316': 843,\n",
       "  '142_141_S_1152': 844,\n",
       "  '143_141_S_0853': 845,\n",
       "  '145_126_S_0784': 846,\n",
       "  '146_100_S_0006': 847,\n",
       "  '147_013_S_0575': 848,\n",
       "  '153_137_S_0481': 849,\n",
       "  '154_128_S_0216': 850,\n",
       "  '156_128_S_0310': 851,\n",
       "  '157_100_S_0190': 852,\n",
       "  '158_130_S_0783': 853,\n",
       "  '159_098_S_0884': 854,\n",
       "  '15_099_S_0470': 855,\n",
       "  '162_041_S_1002': 856,\n",
       "  '163_027_S_0417': 857,\n",
       "  '165_023_S_1289': 858,\n",
       "  '167_033_S_1279': 859,\n",
       "  '169_067_S_0110': 860,\n",
       "  '16_128_S_0245': 861,\n",
       "  '170_073_S_0445': 862,\n",
       "  '172_072_S_1211': 863,\n",
       "  '173_022_S_0544': 864,\n",
       "  '176_035_S_0204': 865,\n",
       "  '178_941_S_1194': 866,\n",
       "  '179_007_S_0041': 867,\n",
       "  '17_137_S_0825': 868,\n",
       "  '180_011_S_0168': 869,\n",
       "  '181_006_S_0675': 870,\n",
       "  '182_016_S_1263': 871,\n",
       "  '183_041_S_1423': 872,\n",
       "  '184_027_S_0461': 873,\n",
       "  '185_052_S_1168': 874,\n",
       "  '187_126_S_0506': 875,\n",
       "  '189_023_S_0963': 876,\n",
       "  '18_128_S_0266': 877,\n",
       "  '190_067_S_0177': 878,\n",
       "  '191_029_S_1073': 879,\n",
       "  '192_067_S_0290': 880,\n",
       "  '194_033_S_1308': 881,\n",
       "  '196_037_S_0454': 882,\n",
       "  '198_126_S_1221': 883,\n",
       "  '199_012_S_0634': 884,\n",
       "  '19_067_S_0243': 885,\n",
       "  '203_068_S_1191': 886,\n",
       "  '204_136_S_0184': 887,\n",
       "  '205_141_S_1024': 888,\n",
       "  '206_136_S_0194': 889,\n",
       "  '208_013_S_1120': 890,\n",
       "  '209_137_S_0443': 891,\n",
       "  '210_036_S_0673': 892,\n",
       "  '211_053_S_0507': 893,\n",
       "  '212_023_S_1126': 894,\n",
       "  '215_116_S_0392': 895,\n",
       "  '216_082_S_1119': 896,\n",
       "  '217_029_S_1184': 897,\n",
       "  '218_130_S_0969': 898,\n",
       "  '21_016_S_1138': 899,\n",
       "  '221_062_S_0730': 900,\n",
       "  '222_098_S_0288': 901,\n",
       "  '223_133_S_0638': 902,\n",
       "  '224_027_S_0179': 903,\n",
       "  '227_123_S_0050': 904,\n",
       "  '228_130_S_0285': 905,\n",
       "  '229_099_S_0534': 906,\n",
       "  '230_002_S_0782': 907,\n",
       "  '231_067_S_0029': 908,\n",
       "  '232_007_S_1339': 909,\n",
       "  '234_006_S_0653': 910,\n",
       "  '238_023_S_0139': 911,\n",
       "  '239_029_S_1384': 912,\n",
       "  '240_027_S_1082': 913,\n",
       "  '241_128_S_0500': 914,\n",
       "  '244_057_S_0957': 915,\n",
       "  '246_082_S_0304': 916,\n",
       "  '248_141_S_0810': 917,\n",
       "  '249_041_S_1391': 918,\n",
       "  '24_007_S_0070': 919,\n",
       "  '250_016_S_0590': 920,\n",
       "  '251_007_S_0414': 921,\n",
       "  '253_007_S_0249': 922,\n",
       "  '255_031_S_0321': 923,\n",
       "  '256_082_S_0363': 924,\n",
       "  '257_062_S_0690': 925,\n",
       "  '258_007_S_1248': 926,\n",
       "  '259_011_S_0861': 927,\n",
       "  '25_133_S_1170': 928,\n",
       "  '262_052_S_1251': 929,\n",
       "  '263_072_S_1380': 930,\n",
       "  '265_141_S_0726': 931,\n",
       "  '267_130_S_0102': 932,\n",
       "  '268_141_S_1004': 933,\n",
       "  '269_130_S_0423': 934,\n",
       "  '26_128_S_0522': 935,\n",
       "  '271_082_S_1079': 936,\n",
       "  '272_141_S_1231': 937,\n",
       "  '273_062_S_0768': 938,\n",
       "  '274_136_S_0195': 939,\n",
       "  '275_018_S_0087': 940,\n",
       "  '277_018_S_0335': 941,\n",
       "  '279_016_S_0769': 942,\n",
       "  '27_141_S_1245': 943,\n",
       "  '281_141_S_1051': 944,\n",
       "  '282_022_S_0066': 945,\n",
       "  '285_062_S_1294': 946,\n",
       "  '286_100_S_0747': 947,\n",
       "  '287_136_S_0429': 948,\n",
       "  '289_023_S_0126': 949,\n",
       "  '290_941_S_1363': 950,\n",
       "  '291_031_S_1209': 951,\n",
       "  '292_037_S_0627': 952,\n",
       "  '294_036_S_0976': 953,\n",
       "  '296_029_S_0914': 954,\n",
       "  '297_073_S_0312': 955,\n",
       "  '298_021_S_0178': 956,\n",
       "  '299_021_S_0647': 957,\n",
       "  '29_007_S_1304': 958,\n",
       "  '2_005_S_1341': 959,\n",
       "  '300_021_S_0332': 960,\n",
       "  '301_027_S_0850': 961,\n",
       "  '302_131_S_0319': 962,\n",
       "  '303_032_S_1101': 963,\n",
       "  '304_002_S_0559': 964,\n",
       "  '305_032_S_0677': 965,\n",
       "  '309_041_S_1411': 966,\n",
       "  '310_137_S_0841': 967,\n",
       "  '311_013_S_1276': 968,\n",
       "  '312_029_S_0999': 969,\n",
       "  '313_022_S_0543': 970,\n",
       "  '314_082_S_0469': 971,\n",
       "  '315_012_S_1292': 972,\n",
       "  '316_082_S_1377': 973,\n",
       "  '319_002_S_0413': 974,\n",
       "  '31_006_S_0547': 975,\n",
       "  '320_032_S_0718': 976,\n",
       "  '321_141_S_1244': 977,\n",
       "  '323_126_S_0891': 978,\n",
       "  '324_013_S_1205': 979,\n",
       "  '325_005_S_0222': 980,\n",
       "  '32_009_S_0751': 981,\n",
       "  '330_099_S_0551': 982,\n",
       "  '333_002_S_0295': 983,\n",
       "  '334_100_S_1154': 984,\n",
       "  '336_128_S_0188': 985,\n",
       "  '337_027_S_1213': 986,\n",
       "  '339_029_S_1318': 987,\n",
       "  '33_123_S_0390': 988,\n",
       "  '341_037_S_0327': 989,\n",
       "  '342_033_S_1284': 990,\n",
       "  '343_068_S_0476': 991,\n",
       "  '344_029_S_1056': 992,\n",
       "  '345_067_S_0045': 993,\n",
       "  '348_023_S_0078': 994,\n",
       "  '349_100_S_0015': 995,\n",
       "  '350_012_S_0689': 996,\n",
       "  '352_114_S_0979': 997,\n",
       "  '353_128_S_1406': 998,\n",
       "  '354_137_S_1041': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final_Samples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "usable_samples_ADNI = json.load(open('./usable_samples_ADNI.json'))\n",
    "PRS_feature_matrix = np.load('./PRS_feature_matrix.npy').astype(np.float32)\n",
    "PRS_feature_matrix.shape, len(usable_samples_ADNI), usable_samples_ADNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['d', '4'], ['a', '1'], ['b', '2'], ['c', '3']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([['a',1],['b',2],['c',3],['d',4]]).tolist()\n",
    "random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>IID</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADNI3_036_S_6231</td>\n",
       "      <td>ADNI3_036_S_6231</td>\n",
       "      <td>-0.006724</td>\n",
       "      <td>-0.010617</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>-0.013131</td>\n",
       "      <td>-0.005855</td>\n",
       "      <td>-0.005142</td>\n",
       "      <td>-0.009063</td>\n",
       "      <td>-0.001739</td>\n",
       "      <td>-0.012863</td>\n",
       "      <td>1</td>\n",
       "      <td>69.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADNI3_006_S_6277</td>\n",
       "      <td>ADNI3_006_S_6277</td>\n",
       "      <td>-0.010432</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>0.012757</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>-0.014958</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.027775</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>0.054966</td>\n",
       "      <td>0.087390</td>\n",
       "      <td>1</td>\n",
       "      <td>70.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADNI3_129_S_6146</td>\n",
       "      <td>ADNI3_129_S_6146</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>-0.035521</td>\n",
       "      <td>0.064641</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>0.019736</td>\n",
       "      <td>-0.023304</td>\n",
       "      <td>1</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADNI3_033_S_6352</td>\n",
       "      <td>ADNI3_033_S_6352</td>\n",
       "      <td>-0.014069</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>0.053023</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>-0.002273</td>\n",
       "      <td>-0.030627</td>\n",
       "      <td>-0.053461</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>0</td>\n",
       "      <td>71.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADNI3_027_S_6183</td>\n",
       "      <td>ADNI3_027_S_6183</td>\n",
       "      <td>-0.010766</td>\n",
       "      <td>-0.012370</td>\n",
       "      <td>-0.010960</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>-0.019520</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>0.023844</td>\n",
       "      <td>0.079138</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>0</td>\n",
       "      <td>65.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>2_033_S_5198</td>\n",
       "      <td>2_033_S_5198</td>\n",
       "      <td>-0.002887</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.028780</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>-0.009613</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>-0.006607</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>1</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>2_003_S_5150</td>\n",
       "      <td>2_003_S_5150</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.015292</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>-0.024321</td>\n",
       "      <td>0.035067</td>\n",
       "      <td>0.010485</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>-0.037699</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>2_027_S_5170</td>\n",
       "      <td>2_027_S_5170</td>\n",
       "      <td>-0.009013</td>\n",
       "      <td>-0.017648</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>-0.005609</td>\n",
       "      <td>-0.018215</td>\n",
       "      <td>-0.047387</td>\n",
       "      <td>-0.006604</td>\n",
       "      <td>-0.004732</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>0</td>\n",
       "      <td>76.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>2_035_S_4785</td>\n",
       "      <td>2_035_S_4785</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>-0.012679</td>\n",
       "      <td>0.011654</td>\n",
       "      <td>-0.013708</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>-0.010816</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>-0.017140</td>\n",
       "      <td>-0.029010</td>\n",
       "      <td>1</td>\n",
       "      <td>66.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>2_100_S_5096</td>\n",
       "      <td>2_100_S_5096</td>\n",
       "      <td>0.074290</td>\n",
       "      <td>-0.011119</td>\n",
       "      <td>0.029897</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.016584</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.005885</td>\n",
       "      <td>-0.003966</td>\n",
       "      <td>-0.012851</td>\n",
       "      <td>0</td>\n",
       "      <td>80.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FID               IID       PC1       PC2       PC3  \\\n",
       "0     ADNI3_036_S_6231  ADNI3_036_S_6231 -0.006724 -0.010617  0.001596   \n",
       "1     ADNI3_006_S_6277  ADNI3_006_S_6277 -0.010432 -0.010269  0.012757   \n",
       "2     ADNI3_129_S_6146  ADNI3_129_S_6146 -0.004919 -0.011656 -0.035521   \n",
       "3     ADNI3_033_S_6352  ADNI3_033_S_6352 -0.014069 -0.010279  0.020014   \n",
       "4     ADNI3_027_S_6183  ADNI3_027_S_6183 -0.010766 -0.012370 -0.010960   \n",
       "...                ...               ...       ...       ...       ...   \n",
       "1812      2_033_S_5198      2_033_S_5198 -0.002887 -0.019102 -0.028780   \n",
       "1813      2_003_S_5150      2_003_S_5150 -0.010896 -0.015292  0.024823   \n",
       "1814      2_027_S_5170      2_027_S_5170 -0.009013 -0.017648  0.000018   \n",
       "1815      2_035_S_4785      2_035_S_4785 -0.009913 -0.012679  0.011654   \n",
       "1816      2_100_S_5096      2_100_S_5096  0.074290 -0.011119  0.029897   \n",
       "\n",
       "           PC4       PC5       PC6       PC7       PC8       PC9      PC10  \\\n",
       "0    -0.000460 -0.013131 -0.005855 -0.005142 -0.009063 -0.001739 -0.012863   \n",
       "1     0.006921 -0.014958 -0.005860 -0.027775 -0.009632  0.054966  0.087390   \n",
       "2     0.064641  0.012094  0.003860  0.035955  0.006561  0.019736 -0.023304   \n",
       "3     0.053023  0.023691  0.000247 -0.002273 -0.030627 -0.053461  0.049984   \n",
       "4     0.029830 -0.019520 -0.001955  0.023844  0.079138  0.002207  0.008892   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1812 -0.002316 -0.009613  0.008407  0.001315 -0.006607 -0.015636  0.014984   \n",
       "1813 -0.024321  0.035067  0.010485  0.024330 -0.037699  0.012961  0.012007   \n",
       "1814  0.003858 -0.005609 -0.018215 -0.047387 -0.006604 -0.004732 -0.018276   \n",
       "1815 -0.013708 -0.000056  0.003703 -0.010816 -0.001636 -0.017140 -0.029010   \n",
       "1816 -0.004294  0.043389  0.016584  0.021954  0.005885 -0.003966 -0.012851   \n",
       "\n",
       "      PTGENDER   AGE  \n",
       "0            1  69.1  \n",
       "1            1  70.7  \n",
       "2            1  65.5  \n",
       "3            0  71.4  \n",
       "4            0  65.6  \n",
       "...        ...   ...  \n",
       "1812         1  69.2  \n",
       "1813         0  66.4  \n",
       "1814         0  76.9  \n",
       "1815         1  66.7  \n",
       "1816         0  80.4  \n",
       "\n",
       "[1817 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../larger_dataset/COVAR_FILE_bigger_dataset.txt', ' ') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1288546255506608,\n",
       " array([[ 2.1124783e-01,  4.2774317e-01, -9.1682458e-01, ...,\n",
       "         -1.2863100e-02,  1.0000000e+00,  6.9099998e+01],\n",
       "        [-2.7879314e-02, -1.8597077e-01,  2.9249790e-01, ...,\n",
       "          8.7389797e-02,  1.0000000e+00,  7.0699997e+01],\n",
       "        [ 9.0986907e-01,  7.6243150e-01, -1.0507778e+00, ...,\n",
       "         -2.3304300e-02,  1.0000000e+00,  6.5500000e+01],\n",
       "        ...,\n",
       "        [-4.7291896e-01,  4.2175242e-01,  5.7615715e-01, ...,\n",
       "         -1.8275900e-02,  0.0000000e+00,  7.6900002e+01],\n",
       "        [-6.2698370e-01,  7.1663982e-01,  1.0141952e-01, ...,\n",
       "         -2.9009599e-02,  1.0000000e+00,  6.6699997e+01],\n",
       "        [-1.6327330e-01,  1.2807531e+00,  3.4276077e-01, ...,\n",
       "         -1.2850600e-02,  0.0000000e+00,  8.0400002e+01]], dtype=float32),\n",
       " array([[ 0.21124783,  0.42774317, -0.9168246 , ...,  1.4731058 ,\n",
       "          0.03602596, -0.5889694 ],\n",
       "        [-0.02787931, -0.18597077,  0.2924979 , ..., -0.3768664 ,\n",
       "         -0.05209647,  0.55992734],\n",
       "        [ 0.9098691 ,  0.7624315 , -1.0507778 , ...,  1.3416739 ,\n",
       "          0.344472  , -0.50282633],\n",
       "        ...,\n",
       "        [-0.47291896,  0.42175242,  0.57615715, ...,  0.27624285,\n",
       "          0.39533326, -0.14655346],\n",
       "        [-0.6269837 ,  0.7166398 ,  0.10141952, ...,  0.2710782 ,\n",
       "         -0.03582846,  0.99683714],\n",
       "        [-0.1632733 ,  1.2807531 ,  0.34276077, ...,  1.7514445 ,\n",
       "          1.3124045 ,  0.33931008]], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# COVAR_FILE = df.to_numpy()[:, 2:].astype(np.float32)\n",
    "cnt = 0\n",
    "FEATURE_MATRIX = np.concatenate([PRS_feature_matrix, np.zeros([PRS_feature_matrix.shape[0], 12])], 1).astype(np.float32)\n",
    "for sample in usable_samples_ADNI:\n",
    "    covar = df[df['IID'] == sample].to_numpy()[:, 2:].astype(np.float32)\n",
    "    if covar.shape[0] != 1:\n",
    "#         print(sample)\n",
    "        cnt += 1\n",
    "        continue\n",
    "    FEATURE_MATRIX[usable_samples_ADNI[sample], 23:] = covar\n",
    "cnt/FEATURE_MATRIX.shape[0], FEATURE_MATRIX, PRS_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "feature_indices_to_consider = list(range(23))#list(range(35)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "\n",
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features=FEATURE_MATRIX.shape[1], hidden_dim=32, drop_probab=.8):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        num_hidden = 4\n",
    "        self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "#         self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.drop_probab = drop_probab\n",
    "        self.dropout = nn.functional.dropout\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        for i in range(self.num_hidden):\n",
    "            features = self.fc_hidden[i](features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n",
      "650 650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1300, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "print(len(Final_Samples))\n",
    "positive_samples = Final_Samples[:654]\n",
    "negative_samples = Final_Samples[654:]\n",
    "random_seed = None\n",
    "if random_seed is not None: \n",
    "    random.seed(random_seed * 2)\n",
    "random.shuffle(positive_samples)\n",
    "random.shuffle(negative_samples)\n",
    "Final_Samples = positive_samples[:650] + negative_samples[:650]\n",
    "print(len([x[1] for x in Final_Samples if x[1] == 1]), len([x[1] for x in Final_Samples if x[1] == 0]))\n",
    "Final_Samples = np.array(Final_Samples)\n",
    "Final_Samples.shape\n",
    "# Final_Samples.reshape(10, -1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_model(\n",
       "  (fc1): Linear(in_features=23, out_features=32, bias=True)\n",
       "  (fc_hidden): ModuleList(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (outLayer): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model(num_features=len(feature_indices_to_consider))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class dataSet(data.Dataset):\n",
    "    def __init__(self, Final_Samples, feature_matrix, usable_samples_ADNI, feature_indices_to_consider=feature_indices_to_consider):\n",
    "        super(dataSet, self).__init__()  \n",
    "        self.data_len = len(Final_Samples)\n",
    "        self.usable_samples_ADNI = usable_samples_ADNI\n",
    "        self.Final_Samples = Final_Samples\n",
    "        self.feature_indices_to_consider = feature_indices_to_consider\n",
    "        self.feature_matrix = feature_matrix[:, self.feature_indices_to_consider]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        features = torch.from_numpy(self.feature_matrix[self.usable_samples_ADNI[self.Final_Samples[index][0]]]).float()\n",
    "        label = torch.tensor([float(self.Final_Samples[index][1])]).float()\n",
    "        return features, label\n",
    "    \n",
    "    def update_prs_features(self, mean, std):\n",
    "        self.feature_matrix = (self.feature_matrix - mean) / std\n",
    "        \n",
    "    def get_mean_std(self):\n",
    "        mean = self.feature_matrix.mean(0)\n",
    "        std = self.feature_matrix.std(0)\n",
    "        return mean, std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def random_samples(total_folds, random_seed=None):\n",
    "    Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "    positive_samples = Final_Samples[:654]\n",
    "    negative_samples = Final_Samples[654:]\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 2)\n",
    "    random.shuffle(positive_samples)\n",
    "    random.shuffle(negative_samples)\n",
    "    Final_Samples = positive_samples[:650] + negative_samples[:650]\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed)\n",
    "    random.shuffle(Final_Samples)\n",
    "    Final_Samples = np.array(Final_Samples)\n",
    "    N_splits = Final_Samples.reshape(total_folds, -1, 2)\n",
    "    return N_splits\n",
    "\n",
    "def generate_datasets(N_splits, fold_num, random_seed):\n",
    "    test_samples = N_splits[fold_num:fold_num+1].reshape([-1, 2])\n",
    "    train_samples = np.concatenate([N_splits[0:fold_num],N_splits[fold_num+1:]], 0).reshape([-1, 2]).tolist()\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 3)\n",
    "    random.shuffle(train_samples)\n",
    "    train_samples = np.array(train_samples)\n",
    "    split_pos = int(train_samples.shape[0] * 1.) \n",
    "    #split_pos = int(train_samples.shape[0] * .8) \n",
    "#     print(train_samples.shape, split_pos, train_samples.shape[0])\n",
    "    train_samples, val_samples = train_samples[:split_pos], train_samples[split_pos:]\n",
    "    train_set = dataSet(Final_Samples=train_samples, \n",
    "                        feature_matrix=FEATURE_MATRIX, \n",
    "                        usable_samples_ADNI=usable_samples_ADNI)\n",
    "    val_set = dataSet(Final_Samples=val_samples, \n",
    "                      feature_matrix=FEATURE_MATRIX, \n",
    "                      usable_samples_ADNI=usable_samples_ADNI)\n",
    "    test_set = dataSet(Final_Samples=test_samples, \n",
    "                      feature_matrix=FEATURE_MATRIX, \n",
    "                      usable_samples_ADNI=usable_samples_ADNI)\n",
    "    mean, std = train_set.get_mean_std()\n",
    "    train_set.update_prs_features(mean, std)\n",
    "    val_set.update_prs_features(mean, std)\n",
    "    test_set.update_prs_features(mean, std)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def generate_loader(train_set, val_set, test_set, num_workers):\n",
    "    train_batch_size = train_set.__len__()\n",
    "    val_batch_size = val_set.__len__()\n",
    "    test_batch_size = test_set.__len__()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                              batch_size=train_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              batch_size=test_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_set, val_set, test_set = generate_datasets(N_splits=random_samples(total_folds=10, random_seed=0), fold_num=0, random_seed=0)\n",
    "val_set.feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'036_S_6231': 0,\n",
       " '006_S_6277': 1,\n",
       " '129_S_6146': 2,\n",
       " '033_S_6352': 3,\n",
       " '027_S_6183': 4,\n",
       " '005_S_6427': 5,\n",
       " '127_S_6147': 6,\n",
       " '114_S_6251': 7,\n",
       " '129_S_6228': 8,\n",
       " '114_S_6309': 9,\n",
       " '135_S_6110': 10,\n",
       " '020_S_6358': 11,\n",
       " '135_S_6411': 12,\n",
       " '024_S_6202': 13,\n",
       " '018_S_6414': 14,\n",
       " '002_S_6103': 15,\n",
       " '177_S_6408': 16,\n",
       " '014_S_6148': 17,\n",
       " '036_S_6466': 18,\n",
       " '036_S_6134': 19,\n",
       " '007_S_6455': 20,\n",
       " '037_S_6271': 21,\n",
       " '116_S_6100': 22,\n",
       " '027_S_6327': 23,\n",
       " '099_S_6097': 24,\n",
       " '127_S_6330': 25,\n",
       " '127_S_6168': 26,\n",
       " '018_S_6351': 27,\n",
       " '009_S_6212': 28,\n",
       " '168_S_6180': 29,\n",
       " '116_S_6119': 30,\n",
       " '023_S_6346': 31,\n",
       " '168_S_6065': 32,\n",
       " '035_S_6200': 33,\n",
       " '023_S_6399': 34,\n",
       " '037_S_6125': 35,\n",
       " '116_S_6428': 36,\n",
       " '041_S_6192': 37,\n",
       " '941_S_6333': 38,\n",
       " '006_S_6209': 39,\n",
       " '033_S_6497': 40,\n",
       " '127_S_6436': 41,\n",
       " '141_S_6178': 42,\n",
       " '002_S_6456': 43,\n",
       " '168_S_6062': 44,\n",
       " '033_S_6298': 45,\n",
       " '002_S_6053': 46,\n",
       " '135_S_6359': 47,\n",
       " '141_S_6015': 48,\n",
       " '027_S_6317': 49,\n",
       " '002_S_6009': 50,\n",
       " '098_S_6343': 51,\n",
       " '024_S_6033': 52,\n",
       " '941_S_6384': 53,\n",
       " '141_S_6008': 54,\n",
       " '023_S_6400': 55,\n",
       " '027_S_6034': 56,\n",
       " '002_S_6404': 57,\n",
       " '037_S_6031': 58,\n",
       " '941_S_6017': 59,\n",
       " '130_S_6329': 60,\n",
       " '941_S_6044': 61,\n",
       " '020_S_6470': 62,\n",
       " '941_S_6052': 63,\n",
       " '082_S_6197': 64,\n",
       " '130_S_6019': 65,\n",
       " '023_S_6356': 66,\n",
       " '002_S_6030': 67,\n",
       " '094_S_6419': 68,\n",
       " '130_S_6037': 69,\n",
       " '007_S_6310': 70,\n",
       " '941_S_6054': 71,\n",
       " '041_S_6354': 72,\n",
       " '141_S_6041': 73,\n",
       " '135_S_6360': 74,\n",
       " '067_S_6045': 75,\n",
       " '011_S_6367': 76,\n",
       " '099_S_6016': 77,\n",
       " '035_S_6306': 78,\n",
       " '024_S_6005': 79,\n",
       " '168_S_6318': 80,\n",
       " '130_S_6047': 81,\n",
       " '082_S_6287': 82,\n",
       " '941_S_6068': 83,\n",
       " '037_S_6222': 84,\n",
       " '022_S_6013': 85,\n",
       " '941_S_6422': 86,\n",
       " '114_S_6039': 87,\n",
       " '006_S_6375': 88,\n",
       " '002_S_6066': 89,\n",
       " '094_S_6250': 90,\n",
       " '305_S_6188': 91,\n",
       " '941_S_6471': 92,\n",
       " '003_S_6258': 93,\n",
       " '041_S_6159': 94,\n",
       " '100_S_6349': 95,\n",
       " '141_S_6240': 96,\n",
       " '131_S_6170': 97,\n",
       " '018_S_6207': 98,\n",
       " '003_S_6256': 99,\n",
       " '301_S_6224': 100,\n",
       " '023_S_6334': 101,\n",
       " '007_S_6255': 102,\n",
       " '003_S_6260': 103,\n",
       " '100_S_6164': 104,\n",
       " '070_S_6394': 105,\n",
       " '014_S_6145': 106,\n",
       " '099_S_6396': 107,\n",
       " '109_S_6213': 108,\n",
       " '129_S_6452': 109,\n",
       " '009_S_6163': 110,\n",
       " '116_S_6543': 111,\n",
       " '003_S_6158': 112,\n",
       " '130_S_6161': 113,\n",
       " '006_S_6252': 114,\n",
       " '135_S_6446': 115,\n",
       " '014_S_6199': 116,\n",
       " '130_S_6319': 117,\n",
       " '129_S_6244': 118,\n",
       " '032_S_6279': 119,\n",
       " '006_S_6243': 120,\n",
       " '301_S_6326': 121,\n",
       " '114_S_6113': 122,\n",
       " '007_S_6323': 123,\n",
       " '153_S_6274': 124,\n",
       " '094_S_6269': 125,\n",
       " '037_S_6141': 126,\n",
       " '037_S_6187': 127,\n",
       " '168_S_6049': 128,\n",
       " '177_S_6335': 129,\n",
       " '033_S_6266': 130,\n",
       " '129_S_6459': 131,\n",
       " '130_S_6372': 132,\n",
       " '141_S_6253': 133,\n",
       " '006_S_6500': 134,\n",
       " '130_S_6072': 135,\n",
       " '082_S_6283': 136,\n",
       " '141_S_6061': 137,\n",
       " '007_S_6341': 138,\n",
       " '007_S_6120': 139,\n",
       " '094_S_6275': 140,\n",
       " '941_S_6080': 141,\n",
       " '016_S_6381': 142,\n",
       " '941_S_6094': 143,\n",
       " '941_S_6345': 144,\n",
       " '003_S_6067': 145,\n",
       " '023_S_6374': 146,\n",
       " '012_S_6073': 147,\n",
       " '037_S_6216': 148,\n",
       " '135_S_6104': 149,\n",
       " '168_S_6320': 150,\n",
       " '067_S_6117': 151,\n",
       " '041_S_6401': 152,\n",
       " '014_S_6076': 153,\n",
       " '011_S_6418': 154,\n",
       " '036_S_6088': 155,\n",
       " '003_S_6432': 156,\n",
       " '037_S_6083': 157,\n",
       " '116_S_6550': 158,\n",
       " '130_S_6043': 159,\n",
       " '070_S_6386': 160,\n",
       " '127_S_6024': 161,\n",
       " '027_S_6370': 162,\n",
       " '023_S_6369': 163,\n",
       " '130_S_6035': 164,\n",
       " '168_S_6371': 165,\n",
       " '099_S_6038': 166,\n",
       " '168_S_6350': 167,\n",
       " '301_S_6056': 168,\n",
       " '099_S_6025': 169,\n",
       " '135_S_6389': 170,\n",
       " '130_S_6105': 171,\n",
       " '127_S_6241': 172,\n",
       " '022_S_6069': 173,\n",
       " '009_S_6402': 174,\n",
       " '941_S_6058': 175,\n",
       " '035_S_6488': 176,\n",
       " '141_S_6116': 177,\n",
       " '116_S_6439': 178,\n",
       " '116_S_6133': 179,\n",
       " '037_S_6204': 180,\n",
       " '114_S_6429': 181,\n",
       " '041_S_6292': 182,\n",
       " '013_S_6206': 183,\n",
       " '153_S_6336': 184,\n",
       " '014_S_6437': 185,\n",
       " '100_S_6273': 186,\n",
       " '027_S_6577': 187,\n",
       " '941_S_6575': 188,\n",
       " '036_S_6189': 189,\n",
       " '135_S_6545': 190,\n",
       " '036_S_6179': 191,\n",
       " '135_S_6473': 192,\n",
       " '029_S_6289': 193,\n",
       " '014_S_6522': 194,\n",
       " '019_S_6186': 195,\n",
       " '014_S_6502': 196,\n",
       " '070_S_6236': 197,\n",
       " '027_S_6582': 198,\n",
       " '168_S_6085': 199,\n",
       " '019_S_6573': 200,\n",
       " '003_S_6259': 201,\n",
       " '305_S_6498': 202,\n",
       " '003_S_6014': 203,\n",
       " '100_S_6578': 204,\n",
       " '021_S_6312': 205,\n",
       " '135_S_6544': 206,\n",
       " '014_S_6424': 207,\n",
       " '305_S_6313': 208,\n",
       " '127_S_6549': 209,\n",
       " '131_S_6143': 210,\n",
       " '127_S_6512': 211,\n",
       " '127_S_6232': 212,\n",
       " '033_S_6572': 213,\n",
       " '041_S_6226': 214,\n",
       " '003_S_6264': 215,\n",
       " '032_S_6211': 216,\n",
       " '023_S_6547': 217,\n",
       " '130_S_6111': 218,\n",
       " '141_S_6423': 219,\n",
       " '020_S_6282': 220,\n",
       " '067_S_6529': 221,\n",
       " '011_S_6303': 222,\n",
       " '020_S_6566': 223,\n",
       " '099_S_6175': 224,\n",
       " '037_S_6377': 225,\n",
       " '002_S_6007': 226,\n",
       " '003_S_6307': 227,\n",
       " '037_S_6046': 228,\n",
       " '003_S_6479': 229,\n",
       " '305_S_6157': 230,\n",
       " '007_S_6515': 231,\n",
       " '941_S_6581': 232,\n",
       " '127_S_6173': 233,\n",
       " '094_S_6417': 234,\n",
       " '024_S_6385': 235,\n",
       " '020_S_6227': 236,\n",
       " '168_S_6413': 237,\n",
       " '135_S_6284': 238,\n",
       " '024_S_6472': 239,\n",
       " '020_S_6185': 240,\n",
       " '123_S_6118': 241,\n",
       " '941_S_6514': 242,\n",
       " '019_S_6315': 243,\n",
       " '141_S_6075': 244,\n",
       " '012_S_6503': 245,\n",
       " '100_S_6308': 246,\n",
       " '135_S_6586': 247,\n",
       " '005_S_6084': 248,\n",
       " '027_S_6001': 249,\n",
       " '177_S_6448': 250,\n",
       " '053_S_6598': 251,\n",
       " '027_S_6002': 252,\n",
       " '168_S_6467': 253,\n",
       " '006_S_6234': 254,\n",
       " '003_S_6268': 255,\n",
       " '168_S_6426': 256,\n",
       " '067_S_6442': 257,\n",
       " '127_S_6348': 258,\n",
       " '020_S_6513': 259,\n",
       " '067_S_6138': 260,\n",
       " '135_S_6510': 261,\n",
       " '168_S_6151': 262,\n",
       " '011_S_6465': 263,\n",
       " '019_S_6483': 264,\n",
       " '941_S_6454': 265,\n",
       " '094_S_6485': 266,\n",
       " '168_S_6321': 267,\n",
       " '067_S_6474': 268,\n",
       " '007_S_6421': 269,\n",
       " '114_S_6347': 270,\n",
       " '141_S_6416': 271,\n",
       " '129_S_6457': 272,\n",
       " '027_S_6463': 273,\n",
       " '094_S_6468': 274,\n",
       " '177_S_6409': 275,\n",
       " '032_S_6294': 276,\n",
       " '341_S_6494': 277,\n",
       " '036_S_6316': 278,\n",
       " '035_S_6480': 279,\n",
       " '006_S_6441': 280,\n",
       " '127_S_6357': 281,\n",
       " '027_S_6516': 282,\n",
       " '941_S_6580': 283,\n",
       " '941_S_6546': 284,\n",
       " '127_S_6433': 285,\n",
       " '100_S_6493': 286,\n",
       " '019_S_6533': 287,\n",
       " '168_S_6233': 288,\n",
       " '126_S_6559': 289,\n",
       " '168_S_6492': 290,\n",
       " '131_S_6519': 291,\n",
       " '099_S_6476': 292,\n",
       " '020_S_6504': 293,\n",
       " '098_S_6534': 294,\n",
       " '041_S_6314': 295,\n",
       " '941_S_6499': 296,\n",
       " '023_S_6535': 297,\n",
       " '305_S_6378': 298,\n",
       " '082_S_6415': 299,\n",
       " '024_S_0985': 300,\n",
       " '131_S_0123': 301,\n",
       " '098_S_0160': 302,\n",
       " '027_S_0256': 303,\n",
       " '116_S_1243': 304,\n",
       " '011_S_0002': 305,\n",
       " '003_S_0907': 306,\n",
       " '052_S_1346': 307,\n",
       " '012_S_4026': 308,\n",
       " '037_S_4030': 309,\n",
       " '073_S_2182': 310,\n",
       " '116_S_4167': 311,\n",
       " '073_S_0089': 312,\n",
       " '082_S_2099': 313,\n",
       " '021_S_2100': 314,\n",
       " '127_S_1427': 315,\n",
       " '023_S_0926': 316,\n",
       " '137_S_4672': 317,\n",
       " '033_S_0920': 318,\n",
       " '137_S_1414': 319,\n",
       " '128_S_1408': 320,\n",
       " '072_S_2027': 321,\n",
       " '128_S_0545': 322,\n",
       " '021_S_0626': 323,\n",
       " '016_S_0702': 324,\n",
       " '136_S_0695': 325,\n",
       " '051_S_1072': 326,\n",
       " '014_S_0558': 327,\n",
       " '136_S_0873': 328,\n",
       " '002_S_0729': 329,\n",
       " '131_S_0384': 330,\n",
       " '014_S_0563': 331,\n",
       " '029_S_0845': 332,\n",
       " '007_S_0068': 333,\n",
       " '027_S_2219': 334,\n",
       " '021_S_2142': 335,\n",
       " '011_S_1080': 336,\n",
       " '127_S_1032': 337,\n",
       " '014_S_0169': 338,\n",
       " '027_S_1045': 339,\n",
       " '082_S_0832': 340,\n",
       " '082_S_4208': 341,\n",
       " '068_S_4174': 342,\n",
       " '051_S_1331': 343,\n",
       " '029_S_2376': 344,\n",
       " '031_S_4149': 345,\n",
       " '153_S_4151': 346,\n",
       " '098_S_2052': 347,\n",
       " '007_S_2058': 348,\n",
       " '007_S_0128': 349,\n",
       " '100_S_1226': 350,\n",
       " '127_S_0259': 351,\n",
       " '100_S_0296': 352,\n",
       " '031_S_4029': 353,\n",
       " '041_S_4051': 354,\n",
       " '098_S_4059': 355,\n",
       " '016_S_2031': 356,\n",
       " '941_S_4036': 357,\n",
       " '068_S_4134': 358,\n",
       " '003_S_4119': 359,\n",
       " '141_S_4160': 360,\n",
       " '041_S_1418': 361,\n",
       " '033_S_4176': 362,\n",
       " '941_S_4100': 363,\n",
       " '153_S_2148': 364,\n",
       " '014_S_4058': 365,\n",
       " '099_S_4076': 366,\n",
       " '027_S_0644': 367,\n",
       " '002_S_2043': 368,\n",
       " '037_S_0303': 369,\n",
       " '041_S_4138': 370,\n",
       " '023_S_4448': 371,\n",
       " '128_S_4571': 372,\n",
       " '012_S_4545': 373,\n",
       " '035_S_4414': 374,\n",
       " '109_S_4499': 375,\n",
       " '126_S_4514': 376,\n",
       " '014_S_0548': 377,\n",
       " '153_S_4159': 378,\n",
       " '051_S_1123': 379,\n",
       " '098_S_4095': 380,\n",
       " '141_S_0915': 381,\n",
       " '126_S_2407': 382,\n",
       " '037_S_4410': 383,\n",
       " '006_S_4357': 384,\n",
       " '018_S_2155': 385,\n",
       " '013_S_4268': 386,\n",
       " '021_S_4402': 387,\n",
       " '141_S_4438': 388,\n",
       " '019_S_4477': 389,\n",
       " '068_S_4424': 390,\n",
       " '141_S_4456': 391,\n",
       " '130_S_4417': 392,\n",
       " '153_S_4372': 393,\n",
       " '130_S_4294': 394,\n",
       " '021_S_4335': 395,\n",
       " '073_S_4360': 396,\n",
       " '021_S_4421': 397,\n",
       " '053_S_2396': 398,\n",
       " '032_S_0479': 399,\n",
       " '018_S_4313': 400,\n",
       " '018_S_2133': 401,\n",
       " '006_S_4449': 402,\n",
       " '072_S_4465': 403,\n",
       " '011_S_4547': 404,\n",
       " '053_S_4578': 405,\n",
       " '037_S_4381': 406,\n",
       " '041_S_4513': 407,\n",
       " '109_S_4455': 408,\n",
       " '007_S_4611': 409,\n",
       " '100_S_4469': 410,\n",
       " '022_S_4291': 411,\n",
       " '036_S_4562': 412,\n",
       " '130_S_4605': 413,\n",
       " '153_S_4621': 414,\n",
       " '130_S_4641': 415,\n",
       " '023_S_4502': 416,\n",
       " '067_S_4072': 417,\n",
       " '005_S_2390': 418,\n",
       " '023_S_4034': 419,\n",
       " '022_S_4196': 420,\n",
       " '099_S_4157': 421,\n",
       " '002_S_2073': 422,\n",
       " '128_S_2036': 423,\n",
       " '014_S_4263': 424,\n",
       " '009_S_4564': 425,\n",
       " '127_S_1419': 426,\n",
       " '073_S_2191': 427,\n",
       " '127_S_0260': 428,\n",
       " '136_S_0186': 429,\n",
       " '023_S_4164': 430,\n",
       " '068_S_4340': 431,\n",
       " '011_S_4278': 432,\n",
       " '129_S_4396': 433,\n",
       " '116_S_4338': 434,\n",
       " '130_S_4343': 435,\n",
       " '129_S_4371': 436,\n",
       " '127_S_0112': 437,\n",
       " '099_S_4104': 438,\n",
       " '014_S_4039': 439,\n",
       " '016_S_4121': 440,\n",
       " '003_S_4350': 441,\n",
       " '013_S_4580': 442,\n",
       " '126_S_4494': 443,\n",
       " '141_S_2210': 444,\n",
       " '032_S_4429': 445,\n",
       " '009_S_4530': 446,\n",
       " '002_S_4473': 447,\n",
       " '007_S_1206': 448,\n",
       " '073_S_4540': 449,\n",
       " '053_S_4557': 450,\n",
       " '036_S_4491': 451,\n",
       " '007_S_0101': 452,\n",
       " '019_S_4548': 453,\n",
       " '137_S_4482': 454,\n",
       " '082_S_4428': 455,\n",
       " '006_S_4363': 456,\n",
       " '135_S_4446': 457,\n",
       " '009_S_4359': 458,\n",
       " '018_S_4597': 459,\n",
       " '130_S_4589': 460,\n",
       " '009_S_4612': 461,\n",
       " '003_S_0981': 462,\n",
       " '007_S_4568': 463,\n",
       " '135_S_4598': 464,\n",
       " '137_S_4520': 465,\n",
       " '072_S_4539': 466,\n",
       " '073_S_4552': 467,\n",
       " '128_S_4603': 468,\n",
       " '135_S_4566': 469,\n",
       " '014_S_4615': 470,\n",
       " '037_S_4146': 471,\n",
       " '016_S_2007': 472,\n",
       " '035_S_4582': 473,\n",
       " '135_S_4489': 474,\n",
       " '007_S_4467': 475,\n",
       " '130_S_4405': 476,\n",
       " '126_S_4507': 477,\n",
       " '941_S_4365': 478,\n",
       " '130_S_4468': 479,\n",
       " '036_S_4430': 480,\n",
       " '005_S_4707': 481,\n",
       " '022_S_1097': 482,\n",
       " '041_S_4037': 483,\n",
       " '141_S_4053': 484,\n",
       " '023_S_4122': 485,\n",
       " '052_S_4626': 486,\n",
       " '023_S_4501': 487,\n",
       " '037_S_4071': 488,\n",
       " '094_S_4234': 489,\n",
       " '137_S_4331': 490,\n",
       " '032_S_4386': 491,\n",
       " '022_S_4266': 492,\n",
       " '094_S_4434': 493,\n",
       " '068_S_4431': 494,\n",
       " '007_S_4516': 495,\n",
       " '016_S_4584': 496,\n",
       " '006_S_4546': 497,\n",
       " '114_S_4379': 498,\n",
       " '116_S_4625': 499,\n",
       " '022_S_4444': 500,\n",
       " '041_S_4510': 501,\n",
       " '099_S_4475': 502,\n",
       " '037_S_4308': 503,\n",
       " '029_S_4385': 504,\n",
       " '072_S_4462': 505,\n",
       " '007_S_4488': 506,\n",
       " '002_S_4654': 507,\n",
       " '037_S_4432': 508,\n",
       " '073_S_4614': 509,\n",
       " '073_S_4559': 510,\n",
       " '136_S_4408': 511,\n",
       " '014_S_4668': 512,\n",
       " '013_S_4595': 513,\n",
       " '041_S_4629': 514,\n",
       " '128_S_4653': 515,\n",
       " '020_S_1288': 516,\n",
       " '116_S_0752': 517,\n",
       " '012_S_4188': 518,\n",
       " '021_S_4254': 519,\n",
       " '137_S_4299': 520,\n",
       " '031_S_4590': 521,\n",
       " '006_S_0498': 522,\n",
       " '072_S_2083': 523,\n",
       " '068_S_2184': 524,\n",
       " '094_S_2238': 525,\n",
       " '127_S_2234': 526,\n",
       " '031_S_2233': 527,\n",
       " '022_S_2263': 528,\n",
       " '022_S_0130': 529,\n",
       " '098_S_4018': 530,\n",
       " '072_S_4007': 531,\n",
       " '037_S_4015': 532,\n",
       " '037_S_4001': 533,\n",
       " '130_S_2373': 534,\n",
       " '002_S_4251': 535,\n",
       " '023_S_2068': 536,\n",
       " '072_S_4206': 537,\n",
       " '099_S_4202': 538,\n",
       " '129_S_4422': 539,\n",
       " '029_S_4327': 540,\n",
       " '082_S_1256': 541,\n",
       " '099_S_4498': 542,\n",
       " '006_S_4153': 543,\n",
       " '098_S_2079': 544,\n",
       " '137_S_0800': 545,\n",
       " '098_S_2047': 546,\n",
       " '941_S_4187': 547,\n",
       " '002_S_4225': 548,\n",
       " '002_S_4237': 549,\n",
       " '070_S_4692': 550,\n",
       " '018_S_4349': 551,\n",
       " '137_S_4303': 552,\n",
       " '035_S_2061': 553,\n",
       " '041_S_4271': 554,\n",
       " '135_S_4309': 555,\n",
       " '009_S_4388': 556,\n",
       " '024_S_4392': 557,\n",
       " '035_S_4256': 558,\n",
       " '098_S_0896': 559,\n",
       " '016_S_4353': 560,\n",
       " '130_S_4250': 561,\n",
       " '130_S_4542': 562,\n",
       " '003_S_0908': 563,\n",
       " '127_S_2213': 564,\n",
       " '018_S_2138': 565,\n",
       " '027_S_0120': 566,\n",
       " '128_S_2151': 567,\n",
       " '032_S_2247': 568,\n",
       " '027_S_1387': 569,\n",
       " '098_S_0172': 570,\n",
       " '014_S_2308': 571,\n",
       " '023_S_1190': 572,\n",
       " '005_S_0448': 573,\n",
       " '068_S_2316': 574,\n",
       " '007_S_4272': 575,\n",
       " '098_S_4275': 576,\n",
       " '068_S_4332': 577,\n",
       " '018_S_4400': 578,\n",
       " '072_S_4383': 579,\n",
       " '153_S_4297': 580,\n",
       " '006_S_4515': 581,\n",
       " '023_S_4241': 582,\n",
       " '016_S_4591': 583,\n",
       " '128_S_4553': 584,\n",
       " '009_S_4543': 585,\n",
       " '127_S_4604': 586,\n",
       " '127_S_4500': 587,\n",
       " '941_S_4420': 588,\n",
       " '099_S_4480': 589,\n",
       " '094_S_4503': 590,\n",
       " '098_S_4506': 591,\n",
       " '072_S_4522': 592,\n",
       " '128_S_4599': 593,\n",
       " '116_S_4043': 594,\n",
       " '073_S_0311': 595,\n",
       " '019_S_4285': 596,\n",
       " '032_S_4348': 597,\n",
       " '021_S_4659': 598,\n",
       " '027_S_2183': 599,\n",
       " '014_S_2185': 600,\n",
       " '022_S_1351': 601,\n",
       " '068_S_0127': 602,\n",
       " '068_S_0872': 603,\n",
       " '014_S_4401': 604,\n",
       " '006_S_4346': 605,\n",
       " '114_S_0378': 606,\n",
       " '141_S_4426': 607,\n",
       " '099_S_4463': 608,\n",
       " '073_S_4443': 609,\n",
       " '127_S_4240': 610,\n",
       " '094_S_4560': 611,\n",
       " '007_S_4620': 612,\n",
       " '019_S_4549': 613,\n",
       " '067_S_4310': 614,\n",
       " '141_S_4232': 615,\n",
       " '006_S_1130': 616,\n",
       " '057_S_2398': 617,\n",
       " '136_S_4269': 618,\n",
       " '126_S_4458': 619,\n",
       " '072_S_4445': 620,\n",
       " '116_S_4453': 621,\n",
       " '114_S_4404': 622,\n",
       " '072_S_4394': 623,\n",
       " '031_S_4474': 624,\n",
       " '072_S_4613': 625,\n",
       " '116_S_4635': 626,\n",
       " '127_S_4645': 627,\n",
       " '136_S_4433': 628,\n",
       " '128_S_4609': 629,\n",
       " '137_S_4596': 630,\n",
       " '141_S_2333': 631,\n",
       " '007_S_4637': 632,\n",
       " '032_S_1169': 633,\n",
       " '099_S_1034': 634,\n",
       " '116_S_4092': 635,\n",
       " '136_S_0107': 636,\n",
       " '002_S_4270': 637,\n",
       " '035_S_2074': 638,\n",
       " '127_S_4301': 639,\n",
       " '099_S_4205': 640,\n",
       " '082_S_4339': 641,\n",
       " '032_S_2119': 642,\n",
       " '009_S_4337': 643,\n",
       " '012_S_1212': 644,\n",
       " '027_S_2336': 645,\n",
       " '068_S_2315': 646,\n",
       " '022_S_1394': 647,\n",
       " '057_S_1007': 648,\n",
       " '082_S_4224': 649,\n",
       " '135_S_4281': 650,\n",
       " '130_S_4415': 651,\n",
       " '137_S_4536': 652,\n",
       " '135_S_4657': 653,\n",
       " '023_S_0376': 654,\n",
       " '012_S_4643': 655,\n",
       " '099_S_4565': 656,\n",
       " '018_S_0055': 657,\n",
       " '023_S_0625': 658,\n",
       " '021_S_2125': 659,\n",
       " '007_S_2106': 660,\n",
       " '022_S_2087': 661,\n",
       " '036_S_0869': 662,\n",
       " '033_S_1016': 663,\n",
       " '005_S_0546': 664,\n",
       " '033_S_1098': 665,\n",
       " '033_S_0922': 666,\n",
       " '072_S_2093': 667,\n",
       " '072_S_2037': 668,\n",
       " '033_S_0906': 669,\n",
       " '094_S_1417': 670,\n",
       " '005_S_0610': 671,\n",
       " '127_S_0925': 672,\n",
       " '137_S_0972': 673,\n",
       " '036_S_0672': 674,\n",
       " '128_S_2011': 675,\n",
       " '033_S_1116': 676,\n",
       " '005_S_0602': 677,\n",
       " '052_S_2249': 678,\n",
       " '035_S_2199': 679,\n",
       " '021_S_2150': 680,\n",
       " '072_S_0315': 681,\n",
       " '082_S_2307': 682,\n",
       " '002_S_1261': 683,\n",
       " '123_S_1300': 684,\n",
       " '037_S_4028': 685,\n",
       " '072_S_4063': 686,\n",
       " '036_S_1023': 687,\n",
       " '011_S_4075': 688,\n",
       " '027_S_0074': 689,\n",
       " '128_S_2002': 690,\n",
       " '072_S_2026': 691,\n",
       " '016_S_0359': 692,\n",
       " '031_S_0351': 693,\n",
       " '127_S_0622': 694,\n",
       " '014_S_0520': 695,\n",
       " '135_S_4676': 696,\n",
       " '014_S_0658': 697,\n",
       " '012_S_1133': 698,\n",
       " '128_S_2003': 699,\n",
       " '003_S_1074': 700,\n",
       " '016_S_1117': 701,\n",
       " '099_S_2146': 702,\n",
       " '037_S_0501': 703,\n",
       " '031_S_2022': 704,\n",
       " '073_S_2190': 705,\n",
       " '068_S_2168': 706,\n",
       " '037_S_0377': 707,\n",
       " '067_S_2195': 708,\n",
       " '057_S_1269': 709,\n",
       " '129_S_1246': 710,\n",
       " '031_S_4005': 711,\n",
       " '067_S_2304': 712,\n",
       " '129_S_2332': 713,\n",
       " '022_S_4173': 714,\n",
       " '006_S_4150': 715,\n",
       " '002_S_0685': 716,\n",
       " '031_S_0618': 717,\n",
       " '031_S_4203': 718,\n",
       " '003_S_4152': 719,\n",
       " '002_S_4171': 720,\n",
       " '009_S_4324': 721,\n",
       " '128_S_2045': 722,\n",
       " '009_S_0842': 723,\n",
       " '128_S_2220': 724,\n",
       " '068_S_2248': 725,\n",
       " '022_S_2167': 726,\n",
       " '094_S_4162': 727,\n",
       " '023_S_4035': 728,\n",
       " '031_S_0830': 729,\n",
       " '031_S_4218': 730,\n",
       " '127_S_4148': 731,\n",
       " '072_S_4103': 732,\n",
       " '033_S_0741': 733,\n",
       " '037_S_0566': 734,\n",
       " '024_S_4280': 735,\n",
       " '082_S_2121': 736,\n",
       " '013_S_1186': 737,\n",
       " '011_S_1282': 738,\n",
       " '036_S_0945': 739,\n",
       " '100_S_0069': 740,\n",
       " '123_S_0298': 741,\n",
       " '032_S_0214': 742,\n",
       " '041_S_4060': 743,\n",
       " '126_S_4686': 744,\n",
       " '137_S_0459': 745,\n",
       " '041_S_0679': 746,\n",
       " '072_S_4057': 747,\n",
       " '116_S_4195': 748,\n",
       " '019_S_4252': 749,\n",
       " '073_S_4155': 750,\n",
       " '002_S_4262': 751,\n",
       " '041_S_1260': 752,\n",
       " '053_S_2357': 753,\n",
       " '123_S_2363': 754,\n",
       " '099_S_0352': 755,\n",
       " '009_S_2381': 756,\n",
       " '098_S_4003': 757,\n",
       " '016_S_1326': 758,\n",
       " '126_S_2360': 759,\n",
       " '018_S_4696': 760,\n",
       " '130_S_0289': 761,\n",
       " '051_S_1040': 762,\n",
       " '031_S_4042': 763,\n",
       " '007_S_2394': 764,\n",
       " '137_S_0722': 765,\n",
       " '116_S_0657': 766,\n",
       " '100_S_0047': 767,\n",
       " '018_S_2180': 768,\n",
       " '137_S_0973': 769,\n",
       " '098_S_0171': 770,\n",
       " '131_S_0441': 771,\n",
       " '129_S_4073': 772,\n",
       " '068_S_0802': 773,\n",
       " '023_S_4115': 774,\n",
       " '037_S_0588': 775,\n",
       " '037_S_0150': 776,\n",
       " '051_S_1131': 777,\n",
       " '011_S_4366': 778,\n",
       " '072_S_2116': 779,\n",
       " '035_S_0555': 780,\n",
       " '126_S_0605': 781,\n",
       " '068_S_4061': 782,\n",
       " '033_S_4179': 783,\n",
       " '024_S_4158': 784,\n",
       " '029_S_2395': 785,\n",
       " '153_S_4172': 786,\n",
       " '005_S_4185': 787,\n",
       " '067_S_4212': 788,\n",
       " '006_S_4192': 789,\n",
       " '041_S_4200': 790,\n",
       " '128_S_0272': 791,\n",
       " '100_S_1286': 792,\n",
       " '041_S_4014': 793,\n",
       " '031_S_4024': 794,\n",
       " '052_S_0671': 795,\n",
       " '073_S_4216': 796,\n",
       " '082_S_4090': 797,\n",
       " '114_S_2392': 798,\n",
       " '041_S_4041': 799,\n",
       " '037_S_4214': 800,\n",
       " '035_S_4114': 801,\n",
       " '123_S_4096': 802,\n",
       " '005_S_4168': 803,\n",
       " '072_S_4131': 804,\n",
       " '032_S_2240': 805,\n",
       " '002_S_1268': 806,\n",
       " '021_S_0276': 807,\n",
       " '098_S_4050': 808,\n",
       " '123_S_4127': 809,\n",
       " '123_S_4170': 810,\n",
       " '099_S_0051': 811,\n",
       " '099_S_2042': 812,\n",
       " '073_S_4259': 813,\n",
       " '037_S_4302': 814,\n",
       " '082_S_4244': 815,\n",
       " '135_S_4356': 816,\n",
       " '018_S_0682': 817,\n",
       " '021_S_0424': 818,\n",
       " '006_S_0484': 819,\n",
       " '133_S_0629': 820,\n",
       " '128_S_1409': 821,\n",
       " '099_S_0040': 822,\n",
       " '005_S_1224': 823,\n",
       " '068_S_0442': 824,\n",
       " '073_S_0565': 825,\n",
       " '131_S_0497': 826,\n",
       " '033_S_0723': 827,\n",
       " '130_S_0449': 828,\n",
       " '033_S_0516': 829,\n",
       " '137_S_0438': 830,\n",
       " '018_S_0450': 831,\n",
       " '041_S_0721': 832,\n",
       " '012_S_1009': 833,\n",
       " '022_S_0044': 834,\n",
       " '010_S_0472': 835,\n",
       " '941_S_1311': 836,\n",
       " '136_S_0874': 837,\n",
       " '053_S_0621': 838,\n",
       " '029_S_1215': 839,\n",
       " '067_S_1185': 840,\n",
       " '067_S_0038': 841,\n",
       " '067_S_0019': 842,\n",
       " '007_S_0316': 843,\n",
       " '141_S_1152': 844,\n",
       " '141_S_0853': 845,\n",
       " '126_S_0784': 846,\n",
       " '100_S_0006': 847,\n",
       " '013_S_0575': 848,\n",
       " '137_S_0481': 849,\n",
       " '128_S_0216': 850,\n",
       " '128_S_0310': 851,\n",
       " '100_S_0190': 852,\n",
       " '130_S_0783': 853,\n",
       " '098_S_0884': 854,\n",
       " '099_S_0470': 855,\n",
       " '041_S_1002': 856,\n",
       " '027_S_0417': 857,\n",
       " '023_S_1289': 858,\n",
       " '033_S_1279': 859,\n",
       " '067_S_0110': 860,\n",
       " '128_S_0245': 861,\n",
       " '073_S_0445': 862,\n",
       " '072_S_1211': 863,\n",
       " '022_S_0544': 864,\n",
       " '035_S_0204': 865,\n",
       " '941_S_1194': 866,\n",
       " '007_S_0041': 867,\n",
       " '137_S_0825': 868,\n",
       " '011_S_0168': 869,\n",
       " '006_S_0675': 870,\n",
       " '016_S_1263': 871,\n",
       " '041_S_1423': 872,\n",
       " '027_S_0461': 873,\n",
       " '052_S_1168': 874,\n",
       " '126_S_0506': 875,\n",
       " '023_S_0963': 876,\n",
       " '128_S_0266': 877,\n",
       " '067_S_0177': 878,\n",
       " '029_S_1073': 879,\n",
       " '067_S_0290': 880,\n",
       " '033_S_1308': 881,\n",
       " '037_S_0454': 882,\n",
       " '126_S_1221': 883,\n",
       " '012_S_0634': 884,\n",
       " '067_S_0243': 885,\n",
       " '068_S_1191': 886,\n",
       " '136_S_0184': 887,\n",
       " '141_S_1024': 888,\n",
       " '136_S_0194': 889,\n",
       " '013_S_1120': 890,\n",
       " '137_S_0443': 891,\n",
       " '036_S_0673': 892,\n",
       " '053_S_0507': 893,\n",
       " '023_S_1126': 894,\n",
       " '116_S_0392': 895,\n",
       " '082_S_1119': 896,\n",
       " '029_S_1184': 897,\n",
       " '130_S_0969': 898,\n",
       " '016_S_1138': 899,\n",
       " '062_S_0730': 900,\n",
       " '098_S_0288': 901,\n",
       " '133_S_0638': 902,\n",
       " '027_S_0179': 903,\n",
       " '123_S_0050': 904,\n",
       " '130_S_0285': 905,\n",
       " '099_S_0534': 906,\n",
       " '002_S_0782': 907,\n",
       " '067_S_0029': 908,\n",
       " '007_S_1339': 909,\n",
       " '006_S_0653': 910,\n",
       " '023_S_0139': 911,\n",
       " '029_S_1384': 912,\n",
       " '027_S_1082': 913,\n",
       " '128_S_0500': 914,\n",
       " '057_S_0957': 915,\n",
       " '082_S_0304': 916,\n",
       " '141_S_0810': 917,\n",
       " '041_S_1391': 918,\n",
       " '007_S_0070': 919,\n",
       " '016_S_0590': 920,\n",
       " '007_S_0414': 921,\n",
       " '007_S_0249': 922,\n",
       " '031_S_0321': 923,\n",
       " '082_S_0363': 924,\n",
       " '062_S_0690': 925,\n",
       " '007_S_1248': 926,\n",
       " '011_S_0861': 927,\n",
       " '133_S_1170': 928,\n",
       " '052_S_1251': 929,\n",
       " '072_S_1380': 930,\n",
       " '141_S_0726': 931,\n",
       " '130_S_0102': 932,\n",
       " '141_S_1004': 933,\n",
       " '130_S_0423': 934,\n",
       " '128_S_0522': 935,\n",
       " '082_S_1079': 936,\n",
       " '141_S_1231': 937,\n",
       " '062_S_0768': 938,\n",
       " '136_S_0195': 939,\n",
       " '018_S_0087': 940,\n",
       " '018_S_0335': 941,\n",
       " '016_S_0769': 942,\n",
       " '141_S_1245': 943,\n",
       " '141_S_1051': 944,\n",
       " '022_S_0066': 945,\n",
       " '062_S_1294': 946,\n",
       " '100_S_0747': 947,\n",
       " '136_S_0429': 948,\n",
       " '023_S_0126': 949,\n",
       " '941_S_1363': 950,\n",
       " '031_S_1209': 951,\n",
       " '037_S_0627': 952,\n",
       " '036_S_0976': 953,\n",
       " '029_S_0914': 954,\n",
       " '073_S_0312': 955,\n",
       " '021_S_0178': 956,\n",
       " '021_S_0647': 957,\n",
       " '007_S_1304': 958,\n",
       " '005_S_1341': 959,\n",
       " '021_S_0332': 960,\n",
       " '027_S_0850': 961,\n",
       " '131_S_0319': 962,\n",
       " '032_S_1101': 963,\n",
       " '002_S_0559': 964,\n",
       " '032_S_0677': 965,\n",
       " '041_S_1411': 966,\n",
       " '137_S_0841': 967,\n",
       " '013_S_1276': 968,\n",
       " '029_S_0999': 969,\n",
       " '022_S_0543': 970,\n",
       " '082_S_0469': 971,\n",
       " '012_S_1292': 972,\n",
       " '082_S_1377': 973,\n",
       " '002_S_0413': 974,\n",
       " '006_S_0547': 975,\n",
       " '032_S_0718': 976,\n",
       " '141_S_1244': 977,\n",
       " '126_S_0891': 978,\n",
       " '013_S_1205': 979,\n",
       " '005_S_0222': 980,\n",
       " '009_S_0751': 981,\n",
       " '099_S_0551': 982,\n",
       " '002_S_0295': 983,\n",
       " '100_S_1154': 984,\n",
       " '128_S_0188': 985,\n",
       " '027_S_1213': 986,\n",
       " '029_S_1318': 987,\n",
       " '123_S_0390': 988,\n",
       " '037_S_0327': 989,\n",
       " '033_S_1284': 990,\n",
       " '068_S_0476': 991,\n",
       " '029_S_1056': 992,\n",
       " '067_S_0045': 993,\n",
       " '023_S_0078': 994,\n",
       " '100_S_0015': 995,\n",
       " '012_S_0689': 996,\n",
       " '114_S_0979': 997,\n",
       " '128_S_1406': 998,\n",
       " '137_S_1041': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "#         print(features.shape, label.shape)\n",
    "        probab = model(features)\n",
    "    \n",
    "        if is_training:  \n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "#     precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "#     auroc = roc_auc_score(true, pred)\n",
    "#     p, r, thresholds = precision_recall_curve(true, pred)\n",
    "#     auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "#     return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "    return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    \n",
    "import json\n",
    "import pandas as pd\n",
    "GWAS_ID = 'ieu-b-25'\n",
    "usable_samples_ADNI = pd.read_csv(f'./prsice_output/{GWAS_ID}/{GWAS_ID}.best', ' ')['IID'].to_numpy().tolist()\n",
    "usable_samples_ADNI = {'_'.join(sample.split('_')[1:]):idx for idx,sample in enumerate(usable_samples_ADNI)}\n",
    "# usable_samples_ADNI = {sample:idx for idx,sample in enumerate(usable_samples_ADNI)}\n",
    "usable_samples_ADNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "usable_indices = [usable_samples_ADNI[Final_Samples[i][0]] for i in range(len(Final_Samples))]\n",
    "usable_features = FEATURE_MATRIX[usable_indices][:, feature_indices_to_consider]\n",
    "usable_labels = np.array([float(Final_Samples[i][1]) for i in range(len(Final_Samples))])\n",
    "usable_features = torch.autograd.Variable(torch.from_numpy(usable_features)).to(DEVICE).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF:23\n",
      "\n",
      "#F10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.39s/it]\n",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:90: 0.6453846153846154 0.024675407588223754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.35s/it]\n",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:92: 0.6592307692307692 0.04564462676660554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.35s/it]\n",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:0: 0.6346153846153846 0.035459786374203404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.35s/it]\n",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:87: 0.6461538461538463 0.02664693550105964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.30s/it]\n",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:73: 0.6453846153846154 0.03759799818415367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.37s/it]\n",
      "  0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:82: 0.6423076923076922 0.05665738408713935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:54: 0.6592307692307692 0.02899704126918578\n",
      "global_best_acc_val:0.7461538461538462\n",
      "0.6474725274725275 0.008271740336948911 0.6592307692307692 0.6346153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "GENERATE_SHAP = False\n",
    "total_epochs = 250 #150 \n",
    "num_features_list = [usable_features.shape[1]]\n",
    "# random_integers = [2, 6, 108, 90, 5]\n",
    "random_integers = [90, 92, 0, 87, 73, 82, 54]\n",
    "\n",
    "folds_list = [10]#[37*2]\n",
    "\n",
    "avg_val_acc = []\n",
    "\n",
    "shap_values_list = []\n",
    "for num_features in num_features_list:\n",
    "    print(f'NF:{num_features}')\n",
    "    global_best_acc_val = 0.\n",
    "    for total_folds in folds_list:\n",
    "        print(f'\\n#F{total_folds}')\n",
    "        for random_seed in random_integers:\n",
    "            N_splits = random_samples(total_folds=total_folds, random_seed=random_seed)\n",
    "            accuracies = []\n",
    "            accuracies_val = []\n",
    "            temp_shap_values = np.zeros(usable_features.shape)\n",
    "            for fold_num in tqdm(range(total_folds)):\n",
    "    #             print(f'fold-{fold_num}:')\n",
    "#                 train_set, val_set, test_set = generate_datasets(N_splits=N_splits, fold_num=fold_num, random_seed=random_seed)\n",
    "                train_set, _, test_set = generate_datasets(N_splits=N_splits, fold_num=fold_num, random_seed=random_seed)\n",
    "                val_set = test_set        \n",
    "                train_loader, val_loader, test_loader = generate_loader(train_set=train_set, val_set=val_set, \n",
    "                                                                        test_set=test_set, num_workers=0)\n",
    "                model = simple_model(num_features=usable_features.shape[1], hidden_dim=32, drop_probab=.8)\n",
    "                model = model.to(DEVICE)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                criterion = torch.nn.BCEWithLogitsLoss() \n",
    "                best_acc_val = 0.\n",
    "                model_best = None\n",
    "                for epoch_num in range(total_epochs):\n",
    "                    model.train()\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=True, \n",
    "                                                                                             loader=train_loader)\n",
    "                    model.eval()\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=False, \n",
    "                                                                                             loader=val_loader)\n",
    "                    if acc_val > best_acc_val:\n",
    "                        best_acc_val = acc_val\n",
    "                        if acc_val > global_best_acc_val:\n",
    "                            global_best_acc_val = acc_val\n",
    "    #                         print('global updated!')\n",
    "                        model_best = deepcopy(model)\n",
    "    #                     print(f'#F:{total_folds}| seed:{random_seed}, fold:{fold_num}, epoch:{epoch_num} -> local:{best_acc_val}, global:{global_best_acc_val}')  \n",
    "#                     if epoch_num + 1 == total_epochs:\n",
    "#     #                     print(f'LAST_Epoch:{epoch_num}, train_acc:{acc_train}, val_acc:{acc_val}, local_best:{best_acc_val}, global_best:{global_best_acc_val}')\n",
    "#                         pass\n",
    "                model_best = model_best.to(DEVICE)\n",
    "                model_best.eval()\n",
    "                precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true = epoch(model=model_best, \n",
    "                                                                                         optimizer=optimizer, \n",
    "                                                                                         criterion=criterion, is_training=False, \n",
    "                                                                                         loader=val_loader)\n",
    "                accuracies += [acc_test]\n",
    "                accuracies_val += [best_acc_val]\n",
    "                \n",
    "#                 print(fold_num, ':', accuracies)\n",
    "                if GENERATE_SHAP:\n",
    "                    explainer = shap.GradientExplainer(model_best.to(DEVICE), usable_features,\n",
    "                                                       batch_size=usable_features.shape[0]) #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "                    shap_values = explainer.shap_values(usable_features, nsamples=100)\n",
    "                    temp_shap_values += shap_values\n",
    "            if GENERATE_SHAP:\n",
    "                temp_shap_values /= total_folds\n",
    "                shap_values_list += [temp_shap_values] \n",
    "            print(f'random_seed:{random_seed}:', #np.mean(accuracies), np.std(accuracies), \n",
    "                  np.mean(accuracies_val), np.std(accuracies_val))\n",
    "            avg_val_acc += [np.mean(accuracies_val)]\n",
    "            \n",
    "    print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "# usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n",
    "avg_val_acc = np.array(avg_val_acc)\n",
    "print(avg_val_acc.mean(), avg_val_acc.std(), avg_val_acc.max(), avg_val_acc.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6873737375\n",
      "0.6895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simple_model(\n",
       "  (fc1): Linear(in_features=23, out_features=32, bias=True)\n",
       "  (fc_hidden): ModuleList(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (outLayer): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n",
    "print(np.array([.67699, 0.685, 0.694, 0.687, 0.695, 0.693, 0.6849999, 0.683]).mean())\n",
    "print(np.array([0.6809999999999999, 0.6889999999999998, 0.7060000000000001, 0.6910000000000001, 0.701, 0.6740000000000002, 0.6869999999999999, 0.6869999999999999]).mean())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4510de96c341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshap_values_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_values_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shap_values_list.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_values_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "shap_values_list[0].shape\n",
    "\n",
    "import pickle\n",
    "pickle.dump(shap_values_list, open('shap_values_list.pkl', 'wb'))\n",
    "shap_values = np.sum(shap_values_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap_values_list\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "shap.force_plot(.5, shap_values[0,:], usable_features[0, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(.5, shap_values[:,:], usable_features[:, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traits[20]\n",
    "shap_values = np.sum(shap_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GWAS_IDS = ['ieu-b-109', 'ukb-b-12064', 'ukb-b-13806', 'ukb-d-20405_0', 'ieu-b-38', 'ukb-b-6134', 'ieu-b-110', 'ukb-b-17627', 'ukb-b-19953', 'ukb-b-8476', 'ukb-d-20405_1', 'ukb-d-20405_2', 'ukb-b-2209', 'ukb-b-4424', 'ukb-b-7663', 'ukb-b-18275', 'ukb-b-770', 'met-d-Total_C', 'ieu-b-25', 'ieu-b-111', 'ukb-b-3957', 'ieu-b-39', 'ukb-b-6324']\n",
    "traits = ['HDL cholesterol', 'Non-cancer illness code, self-reported: depression', 'Non-cancer illness code, self-reported: type 2 diabetes', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: No', 'systolic blood pressure', 'Age completed full time education', 'LDL cholesterol', 'Non-oily fish intake', 'Body mass index (BMI)', 'Loneliness, isolation', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, but not in the last year', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, during the last year', 'Oily fish intake', 'Sleep duration', 'Types of physical activity in last 4 weeks: Strenuous sports', 'Hearing difficulty/problems with background noise', 'Other meat intake', 'Total cholesterol', 'Cigarettes per Day', 'triglycerides', 'Sleeplessness / insomnia', 'diastolic blood pressure', 'Processed meat intake']\n",
    "\n",
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n",
      "500.0 1000\n",
      "43.0 (100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print(Final_Samples)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor, BernoulliRBM\n",
    "\n",
    "feature_indices_to_consider = list(range(0, 23)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "\n",
    "usable_indices = [usable_samples_ADNI[Final_Samples[i][0]] for i in range(len(Final_Samples))]\n",
    "usable_features = FEATURE_MATRIX[usable_indices][:, feature_indices_to_consider]\n",
    "usable_labels = np.array([float(Final_Samples[i][1]) for i in range(len(Final_Samples))])\n",
    "\n",
    "Final_Samples = positive_samples[:500] + negative_samples[:500]\n",
    "random.seed(2);random.shuffle(Final_Samples)\n",
    "# Final_Samples = np.array(Final_Samples)\n",
    "print(len([x[1] for x in Final_Samples if x[1] == 1]), len([x[1] for x in Final_Samples if x[1] == 0]))\n",
    "print(sum(usable_labels), len(usable_labels))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    usable_features, usable_labels, test_size=0.1)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "print(y_test.sum(), y_test.shape)\n",
    "clf = svm.SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam').fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n",
      "0.611 0.04437341546466758 0.69 0.53\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(Final_Samples)\n",
    "usable_indices = [usable_samples_ADNI[Final_Samples[i][0]] for i in range(len(Final_Samples))]\n",
    "usable_features = FEATURE_MATRIX[usable_indices][:, feature_indices_to_consider]\n",
    "usable_labels = np.array([float(Final_Samples[i][1]) for i in range(len(Final_Samples))])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "X = usable_features[:, :23]\n",
    "y = usable_labels\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)\n",
    "# print(y_test)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = svm.SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "#     clf = MLPClassifier(hidden_layer_sizes=(32,8), activation='relu', solver='adam', max_iter=100, learning_rate='invscaling').fit(X_train, y_train)\n",
    "#     print(y_test.sum(), y_test.shape)\n",
    "    accuracies += [clf.score(X_test, y_test)]\n",
    "print(np.mean(accuracies), np.std(accuracies), np.max(accuracies), np.min(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.636 0.041999999999999996\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
