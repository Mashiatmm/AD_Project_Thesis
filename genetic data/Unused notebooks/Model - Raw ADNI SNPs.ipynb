{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from datetime import datetime\n",
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features=1260680*3, num_hidden=0, hidden_dim=32, drop_probab=.5):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        self.drop_probab = drop_probab\n",
    "        self.num_hidden = num_hidden\n",
    "        self.dropout0 = nn.Dropout(p=self.drop_probab)\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=self.drop_probab)\n",
    "        self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(self.num_hidden)])\n",
    "        self.dropout_hidden = nn.ModuleList([nn.Dropout(p=self.drop_probab) for i in range(self.num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.dropout2 = nn.Dropout(p=self.drop_probab)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.dropout0(features)\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout1(features)\n",
    "        for i in range(self.num_hidden):\n",
    "            features = self.fc_hidden[i](features)\n",
    "            features = self.dropout_hidden[i](features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout2(features)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotEncoder(categorical_features=None, categories='auto',\n",
      "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
      "       n_values=None, sparse=True)\n",
      "[array([0, 1, 2])]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-dab78123f936>\", line 30, in <module>\n",
      "    pickle.dump(all_snp_samples, open('F:/all_snp_samples.pkl', 'wb'))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "snps_to_consider = pickle.load(open('snps_to_consider.pkl', 'rb'))\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "X = [[0], [1], [2]]\n",
    "print(enc.fit(X))\n",
    "print(enc.categories_)\n",
    "temp = [[0], [1], [0], [2]]\n",
    "encoded = enc.transform(temp).toarray().astype(np.float32)\n",
    "print(encoded)\n",
    "\n",
    "\n",
    "get_onehot = True\n",
    "all_snp_samples = {} \n",
    "with open('ADNI_TSV_FILE.raw', 'r') as f:\n",
    "    f.readline()\n",
    "    for i in range(847):\n",
    "        line = f.readline()\n",
    "        line = line.replace('NA', '0').split()\n",
    "        FID, IID, PAT,  MAT, SEX, SNPs = line[0], line[1], line[2], line[3], line[4], line[6:]\n",
    "#         SNPs = np.array(SNPs).astype(int)[snps_to_consider]\n",
    "#         if get_onehot:\n",
    "#             SNPs = enc.transform(SNPs.reshape([-1, 1])).toarray().astype(np.float32)\n",
    "        all_snp_samples[IID] = [FID, IID, PAT,  MAT, SEX, SNPs]\n",
    "\n",
    "# enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
    "\n",
    "import pickle\n",
    "pickle.dump(all_snp_samples, open('F:/all_snp_samples.pkl', 'wb'))\n",
    "\n",
    "feature_names = enc.get_feature_names(['SNP'])\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_snp_samples(IID):\n",
    "#     f = open(f'./RAW_SNPs/{IID}.snp', 'r')\n",
    "#     line = f.readline()\n",
    "    FID, IID, PAT,  MAT, SEX, SNPs = all_snp_samples[IID]\n",
    "    return FID, IID, PAT,  MAT, SEX, SNPs\n",
    "\n",
    "# FID, IID, PAT,  MAT, SEX, SNPs = generate_raw_snp_samples(IID='018_S_0633')\n",
    "# FID, IID, PAT,  MAT, SEX, SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "#         print('checkpoint');break\n",
    "        features = torch.autograd.Variable(features.view(features.shape[0], -1).to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "#         print(features.shape, label.shape)\n",
    "        if is_training:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        probab = model(features)\n",
    "        if is_training:  \n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "#     precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "#     auroc = roc_auc_score(true, pred)\n",
    "#     p, r, thresholds = precision_recall_curve(true, pred)\n",
    "#     auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "#     return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "    return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from time import time\n",
    "class dataSet(data.Dataset):\n",
    "    def __init__(self, samples_list):\n",
    "        super(dataSet, self).__init__()  \n",
    "        self.data_len = len(samples_list)\n",
    "        self.samples_list = samples_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        t0 = time()\n",
    "        IID = self.samples_list[index][0]\n",
    "        FID, IID, PAT,  MAT, SEX, SNPs = generate_raw_snp_samples(IID=IID)\n",
    "        features = torch.from_numpy(SNPs).float()\n",
    "        label = torch.tensor([float(self.samples_list[index][1])]).float()\n",
    "        return features, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(train_indices, test_indices, random_seed):\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 3)\n",
    "    random.shuffle(train_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    split_pos = int(train_indices.shape[0] * 0.8)\n",
    "    train_indices, val_indices = train_indices[:split_pos], train_indices[split_pos:]\n",
    "    train_set = dataSet(samples_list=Final_Samples[train_indices])\n",
    "    val_set = dataSet(samples_list=Final_Samples[val_indices])\n",
    "    test_set = dataSet(samples_list=Final_Samples[test_indices])\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def generate_loader(train_set, val_set, test_set, num_workers, CUSTOM_BATCH_SIZE=128):\n",
    "    train_batch_size = CUSTOM_BATCH_SIZE if CUSTOM_BATCH_SIZE else train_set.__len__()\n",
    "    val_batch_size = CUSTOM_BATCH_SIZE if CUSTOM_BATCH_SIZE else val_set.__len__()\n",
    "    test_batch_size = CUSTOM_BATCH_SIZE if CUSTOM_BATCH_SIZE else test_set.__len__()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                              batch_size=train_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              batch_size=test_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "negative_samples = Final_Samples[370:]\n",
    "random.seed(7)\n",
    "random.shuffle(negative_samples)\n",
    "Final_Samples = Final_Samples[:370] + negative_samples[:370] \n",
    "random.seed(0)\n",
    "random.shuffle(Final_Samples)\n",
    "# Final_Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "Final_Samples = np.array(Final_Samples)\n",
    "\n",
    "\n",
    "def train_val_test(train_indices, test_indices):\n",
    "    global accuracies\n",
    "    global accuracies_val\n",
    "    global global_best_acc_val\n",
    "    train_set, val_set, test_set = generate_datasets(train_indices, test_indices, random_seed=None)\n",
    "    train_loader, val_loader, test_loader = generate_loader(train_set, val_set, test_set, num_workers=0)\n",
    "    model = simple_model()\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss() \n",
    "    \n",
    "    best_acc_val = [0., None]\n",
    "    model_best = None\n",
    "    for epoch_num in range(total_epochs):\n",
    "        precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, \n",
    "                                                                                 optimizer=optimizer, \n",
    "                                                                                 criterion=criterion, is_training=True, \n",
    "                                                                                 loader=train_loader)\n",
    "        precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, \n",
    "                                                                                 optimizer=optimizer, \n",
    "                                                                                 criterion=criterion, is_training=False, \n",
    "                                                                                 loader=val_loader)\n",
    "        if acc_val > best_acc_val[0] and True:\n",
    "            model_best = deepcopy(model)\n",
    "            best_acc_val[0] = acc_val\n",
    "            best_acc_val[1] = epoch_num\n",
    "            if acc_val > global_best_acc_val: global_best_acc_val = acc_val\n",
    "        print('acc_val:', acc_val, 'best_acc_val:', best_acc_val)\n",
    "    del model\n",
    "#     model_best = model_best.to(DEVICE)\n",
    "    precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true = epoch(model=model_best, \n",
    "                                                                             optimizer=optimizer, \n",
    "                                                                             criterion=criterion, is_training=False, \n",
    "                                                                             loader=val_loader)\n",
    "    accuracies += [acc_test]\n",
    "    accuracies_val += [best_acc_val[0]]\n",
    "    print(fold_num, ':', accuracies)\n",
    "    return\n",
    "\n",
    "    \n",
    "kf = KFold(n_splits=10)\n",
    "# kf.get_n_splits(Final_Samples)\n",
    "print(kf)\n",
    "global_best_acc_val = 0.\n",
    "total_epochs = 100\n",
    "for fold_num, (train_indices, test_indices) in enumerate(kf.split(Final_Samples)):\n",
    "    accuracies = []\n",
    "    accuracies_val = []\n",
    "    train_val_test(train_indices, test_indices)\n",
    "    print(np.mean(accuracies), np.std(accuracies), \n",
    "          np.mean(accuracies_val), np.std(accuracies_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# X = Final_Samples\n",
    "# kf = KFold(n_splits=10)\n",
    "# print(kf.get_n_splits(X))\n",
    "\n",
    "# print(kf)\n",
    "# # print(y_test)\n",
    "# accuracies = []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     for i in train_index:\n",
    "#         Final_Samples[i][0]\n",
    "#     FID, IID, PAT,  MAT, SEX, SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
