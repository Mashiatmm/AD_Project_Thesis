{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((847, 23), 847)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final_Samples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "usable_samples_ADNI = json.load(open('./usable_samples_ADNI.json'))\n",
    "PRS_feature_matrix = np.load('./PRS_feature_matrix.npy').astype(np.float32)\n",
    "PRS_feature_matrix.shape, len(usable_samples_ADNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4.62746948e-01,  6.30172491e-02, -8.30398262e-01, ...,\n",
       "         -7.39835994e-03,  0.00000000e+00,  7.76999969e+01],\n",
       "        [-2.31216118e-01,  1.43889165e+00, -2.49065900e+00, ...,\n",
       "         -1.35235004e-02,  0.00000000e+00,  7.15999985e+01],\n",
       "        [-1.57160297e-01,  1.92461002e+00,  1.47627056e+00, ...,\n",
       "         -2.79313000e-03,  0.00000000e+00,  8.46999969e+01],\n",
       "        ...,\n",
       "        [-6.73138738e-01,  1.42115796e+00,  2.21374497e-01, ...,\n",
       "          6.16690004e-03,  1.00000000e+00,  8.30000000e+01],\n",
       "        [-3.59309107e-01,  1.09130554e-01,  1.25354218e+00, ...,\n",
       "         -1.22331996e-02,  1.00000000e+00,  8.40000000e+01],\n",
       "        [-1.45961285e-01,  3.16558570e-01, -2.64395952e-01, ...,\n",
       "          7.09875999e-03,  1.00000000e+00,  6.95000000e+01]], dtype=float32),\n",
       " array([[ 0.46274695,  0.06301725, -0.83039826, ...,  0.9141369 ,\n",
       "          0.29846928,  0.02692245],\n",
       "        [-0.23121612,  1.4388916 , -2.490659  , ...,  1.302844  ,\n",
       "          0.3564516 , -0.15681076],\n",
       "        [-0.1571603 ,  1.92461   ,  1.4762706 , ...,  0.2751855 ,\n",
       "         -0.04920464,  0.14408997],\n",
       "        ...,\n",
       "        [-0.67313874,  1.421158  ,  0.2213745 , ...,  0.16947116,\n",
       "         -0.30607244,  0.97224677],\n",
       "        [-0.3593091 ,  0.10913055,  1.2535422 , ..., -0.83123094,\n",
       "         -0.82132477,  0.35422948],\n",
       "        [-0.14596128,  0.31655857, -0.26439595, ...,  1.9197366 ,\n",
       "          1.0122726 ,  0.5347796 ]], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('./COVAR_FILE.txt', ' ') \n",
    "# COVAR_FILE = df.to_numpy()[:, 2:].astype(np.float32)\n",
    "\n",
    "FEATURE_MATRIX = np.concatenate([PRS_feature_matrix, np.zeros([PRS_feature_matrix.shape[0], 12])], 1).astype(np.float32)\n",
    "for sample in usable_samples_ADNI:\n",
    "    covar = df[df['IID'] == sample].to_numpy()[:, 2:].astype(np.float32)\n",
    "    FEATURE_MATRIX[usable_samples_ADNI[sample], 23:] = covar\n",
    "FEATURE_MATRIX, PRS_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "feature_indices_to_consider = list(range(23, 35)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "\n",
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features=FEATURE_MATRIX.shape[1], hidden_dim=32, drop_probab=.8):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "#         self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "#         self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         self.num_hidden = num_hidden\n",
    "        self.drop_probab = drop_probab\n",
    "        self.dropout = nn.functional.dropout\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "#         for i in range(self.num_hidden):\n",
    "#             features = self.fc_hidden[i](features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['018_S_0633', '1'],\n",
       "        ['037_S_0467', '1'],\n",
       "        ['128_S_0517', '1'],\n",
       "        ...,\n",
       "        ['099_S_4202', '0'],\n",
       "        ['072_S_4445', '0'],\n",
       "        ['029_S_2395', '0']], dtype='<U10'),\n",
       " (10, 74, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "negative_samples = Final_Samples[370:]\n",
    "random.seed(7)\n",
    "random.shuffle(negative_samples)\n",
    "Final_Samples = Final_Samples[:370] + negative_samples[:370]\n",
    "len([x[1] for x in Final_Samples if x[1] == 1]), len([x[1] for x in Final_Samples if x[1] == 0])\n",
    "Final_Samples = np.array(Final_Samples)\n",
    "Final_Samples, Final_Samples.reshape(10, -1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_model(\n",
       "  (fc1): Linear(in_features=18, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (outLayer): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model(num_features=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class dataSet(data.Dataset):\n",
    "    def __init__(self, Final_Samples, feature_matrix, usable_samples_ADNI, feature_indices_to_consider=feature_indices_to_consider):\n",
    "        super(dataSet, self).__init__()  \n",
    "        self.data_len = len(Final_Samples)\n",
    "        self.usable_samples_ADNI = usable_samples_ADNI\n",
    "        self.Final_Samples = Final_Samples\n",
    "        self.feature_indices_to_consider = feature_indices_to_consider\n",
    "        self.feature_matrix = feature_matrix[:, self.feature_indices_to_consider]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        features = torch.from_numpy(self.feature_matrix[self.usable_samples_ADNI[self.Final_Samples[index][0]]]).float()\n",
    "        label = torch.tensor([float(self.Final_Samples[index][1])]).float()\n",
    "        return features, label\n",
    "    \n",
    "    def update_prs_features(self, mean, std):\n",
    "        self.feature_matrix = (self.feature_matrix - mean) / std\n",
    "        \n",
    "    def get_mean_std(self):\n",
    "        mean = self.feature_matrix.mean(0)\n",
    "        std = self.feature_matrix.std(0)\n",
    "        return mean, std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def random_samples(total_folds, random_seed=None):\n",
    "    Final_Samples = json.load(open('Final_Samples.json', 'r')) \n",
    "    negative_samples = Final_Samples[370:]\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 2)\n",
    "    random.shuffle(negative_samples)\n",
    "    Final_Samples = Final_Samples[:370] + negative_samples[:370]   \n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed)\n",
    "    random.shuffle(Final_Samples)\n",
    "    Final_Samples = np.array(Final_Samples)\n",
    "    N_splits = Final_Samples.reshape(total_folds, -1, 2)\n",
    "    return N_splits\n",
    "\n",
    "def generate_datasets(N_splits, fold_num):\n",
    "    test_samples = N_splits[fold_num:fold_num+1].reshape([-1, 2])\n",
    "    train_samples = np.concatenate([N_splits[0:fold_num],N_splits[fold_num+1:]], 0).reshape([-1, 2])\n",
    "    train_set = dataSet(Final_Samples=train_samples, \n",
    "                        feature_matrix=FEATURE_MATRIX, \n",
    "                        usable_samples_ADNI=usable_samples_ADNI)\n",
    "    val_set = dataSet(Final_Samples=test_samples, \n",
    "                      feature_matrix=FEATURE_MATRIX, \n",
    "                      usable_samples_ADNI=usable_samples_ADNI)\n",
    "    mean, std = train_set.get_mean_std()\n",
    "    train_set.update_prs_features(mean, std)\n",
    "    val_set.update_prs_features(mean, std)\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def generate_loader(train_set, val_set, num_workers):\n",
    "    train_batch_size = train_set.__len__()\n",
    "    val_batch_size = val_set.__len__()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                              batch_size=train_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_set, val_set = generate_datasets(N_splits=random_samples(total_folds=37), fold_num=0)\n",
    "val_set.feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "#         print(features.shape, label.shape)\n",
    "        probab = model(features)\n",
    "    \n",
    "        if is_training:  \n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "#     precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "#     auroc = roc_auc_score(true, pred)\n",
    "#     p, r, thresholds = precision_recall_curve(true, pred)\n",
    "#     auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "#     return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "    return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "usable_indices = [usable_samples_ADNI[Final_Samples[i][0]] for i in range(len(Final_Samples))]\n",
    "usable_features = FEATURE_MATRIX[usable_indices][:, feature_indices_to_consider]\n",
    "usable_labels = np.array([float(Final_Samples[i][1]) for i in range(len(Final_Samples))])\n",
    "usable_features = torch.autograd.Variable(torch.from_numpy(usable_features)).to(DEVICE).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                    | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF:12\n",
      "\n",
      "#F740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 740/740 [15:18<00:00,  1.24s/it]\n",
      "  0%|                                                                                                                    | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:2: 1.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 740/740 [15:19<00:00,  1.24s/it]\n",
      "  0%|                                                                                                                    | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:6: 1.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 740/740 [15:25<00:00,  1.25s/it]\n",
      "  0%|                                                                                                                    | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed:108: 1.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████▏               | 630/740 [13:08<02:17,  1.25s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-65e8caf65a3f>\", line 37, in <module>\n",
      "    loader=val_loader)\n",
      "  File \"<ipython-input-9-9f392057ea16>\", line 9, in epoch\n",
      "    for batch_idx, (features, label) in enumerate(loader):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 289, in __iter__\n",
      "    return _SingleProcessDataLoaderIter(self)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 394, in __init__\n",
      "    super(_SingleProcessDataLoaderIter, self).__init__(loader)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 350, in __init__\n",
      "    self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\HP\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-65e8caf65a3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                                                                                              \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                                                                                              loader=val_loader)\n\u001b[0m\u001b[0;32m     38\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0macc_val\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_acc_val\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-9f392057ea16>\u001b[0m in \u001b[0;36mepoch\u001b[1;34m(model, optimizer, criterion, is_training, loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_sampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2047\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1436\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             )\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1193\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "GENERATE_SHAP = False\n",
    "total_epochs = 100 #150 \n",
    "num_features_list = [usable_features.shape[1]]\n",
    "random_integers = [2, 6, 108, 90, 5]\n",
    "folds_list = [37*20]#[37*2]\n",
    "\n",
    "shap_values_list = []\n",
    "for num_features in num_features_list:\n",
    "    print(f'NF:{num_features}')\n",
    "    global_best_acc_val = 0.\n",
    "    for total_folds in folds_list:\n",
    "        print(f'\\n#F{total_folds}')\n",
    "        for random_seed in random_integers:\n",
    "            N_splits = random_samples(total_folds=total_folds, random_seed=random_seed)\n",
    "            accuracies = []\n",
    "            temp_shap_values = np.zeros(usable_features.shape)\n",
    "            for fold_num in tqdm(range(total_folds)):\n",
    "    #             print(f'fold-{fold_num}:')\n",
    "                train_set, val_set = generate_datasets(N_splits=N_splits, fold_num=fold_num)\n",
    "                train_loader, val_loader = generate_loader(train_set=train_set, val_set=val_set, num_workers=0)\n",
    "                model = simple_model(num_features=usable_features.shape[1], hidden_dim=32, drop_probab=.8)\n",
    "                model = model.to(DEVICE)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                criterion = torch.nn.BCEWithLogitsLoss() \n",
    "                best_acc_val = 0.\n",
    "                model_best = None\n",
    "                for epoch_num in range(total_epochs):\n",
    "                    model.train()\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=True, \n",
    "                                                                                             loader=train_loader)\n",
    "                    model.eval()\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=False, \n",
    "                                                                                             loader=val_loader)\n",
    "                    if acc_val > best_acc_val:\n",
    "                        best_acc_val = acc_val\n",
    "                        if acc_val > global_best_acc_val:\n",
    "                            global_best_acc_val = acc_val\n",
    "    #                         print('global updated!')\n",
    "                        model_best = deepcopy(model)\n",
    "    #                     print(f'#F:{total_folds}| seed:{random_seed}, fold:{fold_num}, epoch:{epoch_num} -> local:{best_acc_val}, global:{global_best_acc_val}')  \n",
    "                    if epoch_num + 1 == total_epochs:\n",
    "    #                     print(f'LAST_Epoch:{epoch_num}, train_acc:{acc_train}, val_acc:{acc_val}, local_best:{best_acc_val}, global_best:{global_best_acc_val}')\n",
    "                        pass\n",
    "                accuracies += [best_acc_val]\n",
    "#                 print(fold_num, ':', accuracies)\n",
    "                if GENERATE_SHAP:\n",
    "                    explainer = shap.GradientExplainer(model_best.to(DEVICE), usable_features,\n",
    "                                                       batch_size=usable_features.shape[0]) #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "                    shap_values = explainer.shap_values(usable_features, nsamples=100)\n",
    "                    temp_shap_values += shap_values\n",
    "            if GENERATE_SHAP:\n",
    "                temp_shap_values /= total_folds\n",
    "                shap_values_list += [temp_shap_values] \n",
    "            print(f'random_seed:{random_seed}:', np.mean(accuracies), np.std(accuracies))\n",
    "            \n",
    "    print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "usable_features = usable_features.cpu().detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_list[0].shape\n",
    "\n",
    "import pickle\n",
    "pickle.dump(shap_values_list, open('shap_values_list.pkl', 'wb'))\n",
    "shap_values = np.sum(shap_values_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap_values_list\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "shap.force_plot(.5, shap_values[0,:], usable_features[0, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(.5, shap_values[:,:], usable_features[:, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traits[20]\n",
    "shap_values = np.sum(shap_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GWAS_IDS = ['ieu-b-109', 'ukb-b-12064', 'ukb-b-13806', 'ukb-d-20405_0', 'ieu-b-38', 'ukb-b-6134', 'ieu-b-110', 'ukb-b-17627', 'ukb-b-19953', 'ukb-b-8476', 'ukb-d-20405_1', 'ukb-d-20405_2', 'ukb-b-2209', 'ukb-b-4424', 'ukb-b-7663', 'ukb-b-18275', 'ukb-b-770', 'met-d-Total_C', 'ieu-b-25', 'ieu-b-111', 'ukb-b-3957', 'ieu-b-39', 'ukb-b-6324']\n",
    "traits = ['HDL cholesterol', 'Non-cancer illness code, self-reported: depression', 'Non-cancer illness code, self-reported: type 2 diabetes', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: No', 'systolic blood pressure', 'Age completed full time education', 'LDL cholesterol', 'Non-oily fish intake', 'Body mass index (BMI)', 'Loneliness, isolation', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, but not in the last year', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, during the last year', 'Oily fish intake', 'Sleep duration', 'Types of physical activity in last 4 weeks: Strenuous sports', 'Hearing difficulty/problems with background noise', 'Other meat intake', 'Total cholesterol', 'Cigarettes per Day', 'triglycerides', 'Sleeplessness / insomnia', 'diastolic blood pressure', 'Processed meat intake']\n",
    "\n",
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 100\n",
    "# #F37\n",
    "# random_seed:2: 0.7743243243243244 0.0776527453976143\n",
    "# random_seed:6: 0.7418918918918919 0.05636933409355193\n",
    "# random_seed:108: 0.7648648648648649 0.06136385156617693\n",
    "# random_seed:90: 0.7527027027027028 0.0646674925081373\n",
    "# random_seed:5: 0.7648648648648649 0.06352788796509815\n",
    "# global_best_acc_val:0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 100\n",
    "# #F74\n",
    "# random_seed:2: 0.845945945945946 0.09752226244685794\n",
    "# random_seed:6: 0.8405405405405405 0.1064398306352174\n",
    "# random_seed:108: 0.8472972972972973 0.07749973491318739\n",
    "# random_seed:90: 0.8391891891891893 0.08513513513513514\n",
    "# random_seed:5: 0.8527027027027028 0.07017925498702046\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 100\n",
    "# #F74\n",
    "# random_seed:2: 0.677027027027027 0.144771298948616\n",
    "# random_seed:6: 0.677027027027027 0.13208033724122614\n",
    "# random_seed:108: 0.6716216216216216 0.14843350524208282\n",
    "# random_seed:90: 0.6783783783783782 0.13682724387942852\n",
    "# random_seed:5: 0.6500000000000001 0.13177581289775778\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# Learning rate: 0.001, epochs 200\n",
    "# #F74\n",
    "# random_seed:2: 0.6716216216216216 0.12683280148488987\n",
    "# random_seed:6: 0.6972972972972974 0.13653330746286582\n",
    "# random_seed:108: 0.6756756756756758 0.12925024001317129\n",
    "# random_seed:90: 0.681081081081081 0.12910887472968852\n",
    "# random_seed:5: 0.681081081081081 0.12156154672458931\n",
    "# global_best_acc_val:1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# # Learning rate: 0.001, epochs 150\n",
    "# #F74\n",
    "# random_seed:2: 0.6864864864864865 0.14825500704759437\n",
    "# random_seed:6: 0.6945945945945947 0.13545907120153156\n",
    "# random_seed:108: 0.7027027027027029 0.1173732473005863\n",
    "# random_seed:90: 0.6783783783783784 0.1265661564753308\n",
    "# random_seed:5: 0.6918918918918918 0.13126902102249613\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 3\n",
    "# # Learning rate: 0.001, epochs 100\n",
    "# #F74\n",
    "# random_seed:2: 0.6824324324324323 0.13592337677361732\n",
    "# random_seed:6: 0.7040540540540541 0.12886111231612285\n",
    "# random_seed:108: 0.672972972972973 0.12975787982873305\n",
    "# random_seed:90: 0.6716216216216215 0.10593250052576224\n",
    "# random_seed:5: 0.677027027027027 0.12030063388354634\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers: 2\n",
    "# #F74\n",
    "# random_seed:2: 0.677027027027027 0.1466262921164212\n",
    "# random_seed:6: 0.6756756756756755 0.14594594594594595\n",
    "# random_seed:108: 0.6837837837837837 0.13658679749722463\n",
    "# random_seed:90: 0.6783783783783784 0.1378113382052104\n",
    "# random_seed:5: 0.6756756756756758 0.13336680855649005\n",
    "# global_best_acc_val:1.0\n",
    "\n",
    "# Hidden Layers: 1\n",
    "# #F74\n",
    "# random_seed:2: 0.6445945945945946 0.14059900031543793\n",
    "# random_seed:6: 0.6702702702702703 0.138261141215445\n",
    "# random_seed:108: 0.6270270270270272 0.14265560238266645\n",
    "# random_seed:90: 0.6486486486486486 0.11768400571393105\n",
    "# random_seed:5: 0.6378378378378377 0.13426748484728887\n",
    "# global_best_acc_val:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
