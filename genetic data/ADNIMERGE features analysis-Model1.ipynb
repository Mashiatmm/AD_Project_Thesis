{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class simple_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        self.fc1 = nn.Linear(32, 24)\n",
    "        self.fc1_1 = nn.Linear(24, 24)\n",
    "        self.fc1_2 = nn.Linear(24, 24)\n",
    "        self.fc2 = nn.Linear(24, 16)\n",
    "        self.outLayer = nn.Linear(16, 3)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.fc1_1(features)\n",
    "        features = self.fc1_2(features)\n",
    "        features = self.fc2(features)\n",
    "        logits = self.outLayer(features)\n",
    "        # print('output size', features.shape)\n",
    "        probabs = self.softmax(logits)\n",
    "        return logits, probabs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod2_df = pd.read_csv('mod2_df_v2.csv')\n",
    "\n",
    "temp_csv = []\n",
    "cnt = 0\n",
    "for var in mod2_df.keys():\n",
    "#     print(var, ':', mod2_df[var].isna().sum())\n",
    "    if var not in {'PTID', 'DX', 'Last_DX', 'Years_bl', 'DX_bl', 'SITE', 'COLPROT', 'ORIGPROT'}:\n",
    "        temp_csv += [var.split('_DUMMY_')[0]]\n",
    "        cnt += 1\n",
    "# print(mod2_df.keys())\n",
    "temp_csv = set(temp_csv)\n",
    "temp_csv, len(temp_csv)\n",
    "temp_csv = ','.join(temp_csv)\n",
    "temp_csv\n",
    "with open('temp_csv.csv', 'w') as f:\n",
    "    f.write(temp_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRES_SITE: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6782, 37), (761, 37))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SITEs = mod2_df['SITE'].unique()\n",
    "\n",
    "THRES_SITE = int(SITEs.shape[0] * .2)\n",
    "print('THRES_SITE:', THRES_SITE)\n",
    "\n",
    "temp = (mod2_df['SITE'] == SITEs[0])\n",
    "for site in SITEs[1:-THRES_SITE]:\n",
    "    temp = temp | (mod2_df['SITE'] == site)\n",
    "mod2_df_train = mod2_df[temp]\n",
    "\n",
    "temp = (mod2_df['SITE'] == SITEs[-THRES_SITE])\n",
    "for site in SITEs[-THRES_SITE+1:]:\n",
    "    temp = temp | (mod2_df['SITE'] == site)\n",
    "mod2_df_test = mod2_df[temp]\n",
    "\n",
    "mod2_df_train.shape, mod2_df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRES_PTID: 192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5952, 37), (830, 37))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PTID_list = mod2_df_train['PTID'].unique()\n",
    "import random\n",
    "random.shuffle(PTID_list)\n",
    "\n",
    "THRES_PTID = int(PTID_list.shape[0] * .12)\n",
    "print('THRES_PTID:', THRES_PTID)\n",
    "\n",
    "temp = (mod2_df_train['PTID'] == PTID_list[0])\n",
    "for PTID in PTID_list[1:-THRES_PTID]:\n",
    "    temp = temp | (mod2_df_train['PTID'] == PTID)\n",
    "mod2_df_train_1 = mod2_df_train[temp]\n",
    "\n",
    "temp = (mod2_df_train['PTID'] == PTID_list[-THRES_PTID])\n",
    "for PTID in PTID_list[-THRES_PTID+1:]:\n",
    "    temp = temp | (mod2_df_train['PTID'] == PTID)\n",
    "mod2_df_val = mod2_df_train[temp]\n",
    "\n",
    "mod2_df_train = mod2_df_train_1\n",
    "\n",
    "mod2_df_train.shape, mod2_df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': array([  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         73.4703629 ,   0.        ,  16.10147849,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.52402554,\n",
       "          1.51495296,   9.77813172,  15.37423387,   4.77436156,\n",
       "         27.48655914,  36.77620968,   4.35584677,   4.30829973,\n",
       "         56.16484695, 111.00352823,   3.87147177,  -4.70350463,\n",
       "         -4.42900069]),\n",
       " 'std': array([ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  6.95962662,  1.        ,\n",
       "         2.77246426,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.64874067,  1.8528305 ,\n",
       "         6.05593456,  9.01627745,  2.97516657,  2.74095646, 12.82056872,\n",
       "         2.77711549,  2.57031089, 37.88564453, 71.0519381 ,  6.12995263,\n",
       "         6.56225689,  6.1275678 ])}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = False\n",
    "means_std = {\n",
    "    'mean' : np.zeros([mod2_df.shape[1]]),\n",
    "    'std' : np.ones([mod2_df.shape[1]])\n",
    "}\n",
    "for i, var in enumerate(mod2_df.keys()):\n",
    "    if var not in {'PTID', 'DX', 'Last_DX', 'DX_bl', 'COLPROT', 'ORIGPROT', 'FSVERSION', 'SITE', 'Years_bl'}:\n",
    "        if DEBUG: \n",
    "            print('var:', var, mod2_df[var].count())\n",
    "#         mod2_df[var] = mod2_df[var].fillna(mod2_df[var].mean())\n",
    "\n",
    "        if mod2_df[var].unique().shape[0]>2:\n",
    "            means_std['mean'][i] = mod2_df_train[var].mean()\n",
    "            means_std['std'][i] = mod2_df_train[var].std()\n",
    "        else: pass\n",
    "#             print(mod2_df[var].unique())\n",
    "#             means_std['mean'] += [0.]\n",
    "#             means_std['std'] += [1.]\n",
    "    else: pass\n",
    "#             means_std['mean'] += [0.]\n",
    "#             means_std['std'] += [1.]\n",
    "            \n",
    "means_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CN --> CN :  1695\n",
      "CN --> MCI :  331\n",
      "CN --> Dementia :  141\n",
      "\n",
      "MCI --> CN :  203\n",
      "MCI --> MCI :  1738\n",
      "MCI --> Dementia :  910\n",
      "\n",
      "Dementia --> CN :  1\n",
      "Dementia --> MCI :  18\n",
      "Dementia --> Dementia :  915\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'CN': {'CN': 1695, 'Dementia': 141, 'MCI': 331},\n",
       "  'Dementia': {'CN': 1, 'Dementia': 915, 'MCI': 18},\n",
       "  'MCI': {'CN': 203, 'Dementia': 910, 'MCI': 1738}},\n",
       " {'CN': array([0.00058997, 0.00302115, 0.0070922 ]),\n",
       "  'Dementia': array([1.        , 0.05555556, 0.0010929 ]),\n",
       "  'MCI': array([0.00492611, 0.00057537, 0.0010989 ])})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "            'CN': 0,\n",
    "            'MCI': 1,\n",
    "            'Dementia': 2\n",
    "        }\n",
    "print()\n",
    "per_stage_count = {}\n",
    "loss_weight_all = {}\n",
    "for var1 in label_map:\n",
    "    per_stage_count[var1] = {}\n",
    "    loss_weight_all[var1] = np.zeros(3)\n",
    "#     print('loss_weight_all[var1]:', loss_weight_all[var1].shape)\n",
    "    for var2 in label_map:\n",
    "        per_stage_count[var1][var2] = mod2_df_train[(mod2_df_train['DX']==var1) & (mod2_df_train['Last_DX']==var2)].shape[0]\n",
    "        print(var1, '-->', var2, ': ', per_stage_count[var1][var2])\n",
    "        loss_weight_all[var1][label_map[var2]] = 1. / per_stage_count[var1][var2]\n",
    "    print()\n",
    "    \n",
    "per_stage_count, loss_weight_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# patient_list = mod2_df_train['PTID'].unique()\n",
    "\n",
    "# def get_sample(FEATURE_INDEX, df, STANDARDIZE, label_map, per_stage_count, means_std):\n",
    "#     ## PTID patient_id for a patient who had multiple diagnosis \n",
    "#     FEATURE_STARTING = 8\n",
    "# #     PTID = patient_list[_idx_]\n",
    "# #     temp_df = mod2_df_train[mod2_df_train['PTID'] == PTID]\n",
    "# #     assert temp_df.shape[0] > 1\n",
    "# #     FEATURE_INDEX = np.random.choice(temp_df.shape[0])\n",
    "#     features = df.iloc[FEATURE_INDEX, FEATURE_STARTING:]\n",
    "#     features = features.to_numpy().astype(float)\n",
    "#     if STANDARDIZE: \n",
    "#         features = (features - means_std['mean'][FEATURE_STARTING:]) / means_std['std'][FEATURE_STARTING:]\n",
    "#     current_DX, last_DX = np.zeros([3]), np.zeros([3])\n",
    "#     current_DX[label_map[df.iloc[FEATURE_INDEX, 1]]] = 1.        \n",
    "#     last_DX[label_map[df.iloc[FEATURE_INDEX, 2]]] = 1.\n",
    "#     loss_weight = 1. / per_stage_count[df.iloc[FEATURE_INDEX, 1]][df.iloc[FEATURE_INDEX, 2]] \n",
    "#     features = np.concatenate([current_DX, features])\n",
    "#     return features, current_DX, last_DX, loss_weight\n",
    "\n",
    "# features, current_DX, last_DX, loss_weight = get_sample(FEATURE_INDEX=1, \n",
    "#                                             df=mod2_df_train, \n",
    "#                                             STANDARDIZE=True, \n",
    "#                                             label_map=label_map, \n",
    "#                                             per_stage_count=per_stage_count,\n",
    "#                                             means_std=means_std)\n",
    "# features, current_DX, last_DX, features.shape, loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class dataSet(data.Dataset):\n",
    "    def __init__(self, df, loss_weight_all, means_std, dset_type='train'):\n",
    "        super(dataSet, self).__init__()\n",
    "        self.label_map = {\n",
    "                          'CN': 0,\n",
    "                          'MCI': 1,\n",
    "                          'Dementia': 2\n",
    "                         }\n",
    "        self.df = df\n",
    "        self.means_std = means_std\n",
    "        self.loss_weight_all = loss_weight_all\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        features, _, label, loss_weight = self.get_sample(FEATURE_INDEX=index, \n",
    "                                                        df=self.df, \n",
    "                                                        STANDARDIZE=True, \n",
    "                                                        label_map=self.label_map, \n",
    "                                                        loss_weight_all=self.loss_weight_all,\n",
    "                                                        means_std=self.means_std)\n",
    "        features = torch.from_numpy(features).type(torch.FloatTensor)\n",
    "        label = torch.from_numpy(label).type(torch.LongTensor)   \n",
    "        loss_weight = torch.from_numpy(loss_weight).type(torch.FloatTensor)   \n",
    "        return features, label, loss_weight\n",
    "    \n",
    "    def get_sample(self, FEATURE_INDEX, df, STANDARDIZE, label_map, loss_weight_all, means_std):\n",
    "    ## PTID patient_id for a patient who had multiple diagnosis \n",
    "        FEATURE_STARTING = 8\n",
    "        features = df.iloc[FEATURE_INDEX, FEATURE_STARTING:]\n",
    "        features = features.to_numpy().astype(float)\n",
    "        if STANDARDIZE: \n",
    "            features = (features - means_std['mean'][FEATURE_STARTING:]) / means_std['std'][FEATURE_STARTING:]\n",
    "        current_DX, last_DX = np.zeros([3]), np.zeros([3])\n",
    "        current_DX[label_map[df.iloc[FEATURE_INDEX, 1]]] = 1.        \n",
    "        last_DX = np.array(label_map[df.iloc[FEATURE_INDEX, 2]]).astype(int)\n",
    "        loss_weight = loss_weight_all[df.iloc[FEATURE_INDEX, 1]][label_map[df.iloc[FEATURE_INDEX, 2]]]\n",
    "        features = np.concatenate([current_DX, features])\n",
    "        return features, current_DX, last_DX, np.array([loss_weight])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = dataSet(df=mod2_df_train, means_std=means_std, loss_weight_all=loss_weight_all, dset_type='train')\n",
    "testset = dataSet(df=mod2_df_test, means_std=means_std, loss_weight_all=loss_weight_all, dset_type='test')\n",
    "valset = dataSet(df=mod2_df_val, means_std=means_std, loss_weight_all=loss_weight_all, dset_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 0.0000,  1.0000,  0.0000,  1.5273,  0.0000, -0.0366,  0.0000,\n",
       "           1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           0.0000,  1.0000,  0.0000,  0.0000, -0.8078, -0.0081,  0.0366,\n",
       "           0.4021,  1.0842, -1.2720, -1.1525, -1.5685, -0.1199,  1.1570,\n",
       "           0.7881, -0.3053, -0.9768, -1.1380]), tensor(2), tensor(1.00000e-03 *\n",
       "         [ 1.0989])),\n",
       " {'CN': array([0.00058997, 0.00302115, 0.0070922 ]),\n",
       "  'Dementia': array([1.        , 0.05555556, 0.0010929 ]),\n",
       "  'MCI': array([0.00492611, 0.00057537, 0.0010989 ])})"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.__getitem__(739), loss_weight_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for s in [trainset, valset, testset]:\n",
    "#     print(s.df.groupby('DX')['PTID'].nunique(), '\\n->', s.df.groupby('Last_DX')['PTID'].nunique())\n",
    "#     print()\n",
    "batch_size = 16*8\n",
    "num_workers = 0\n",
    "train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          num_workers=num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(valset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()),\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sazan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "loader = train_loader\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# DEVICE = 'cpu'\n",
    "\n",
    "print(trainset.df.shape)\n",
    "\n",
    "model = simple_model()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for batch_idx, (features, label, loss_weight) in enumerate(loader):\n",
    "    #     print(features.shape, label.shape, loss_weight.shape)\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).long())\n",
    "        loss_weight = torch.autograd.Variable(loss_weight.to(DEVICE).float())\n",
    "\n",
    "    #     print(features.shape, label.shape, loss_weight.shape)\n",
    "        logits, probabs = model(features)\n",
    "        log_logits = nn.functional.log_softmax(logits)\n",
    "        \n",
    "        if is_training:\n",
    "            loss = nn.functional.cross_entropy(input=logits, target=label, reduce=False).to(DEVICE)\n",
    "#             print(batch_idx, ':', loss, loss_weight)  \n",
    "            loss = loss*loss_weight.reshape(-1)\n",
    "            loss = loss.mean()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss)  \n",
    "\n",
    "#             print(*zip(probabs.argmax(-1), label))\n",
    "\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        pred += probabs.argmax(-1).detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    return np.array(pred), np.array(true), total_loss\n",
    "\n",
    "pred, true, total_loss = epoch(model, optimizer, criterion, is_training=True, loader=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3565, 5952)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == true).sum(), pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sazan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.5524193548387096 0.3686746987951807\n",
      "1 : 0.5549395161290323 0.5843373493975904\n",
      "2 : 0.586861559139785 0.5831325301204819\n",
      "3 : 0.5841733870967742 0.5156626506024097\n",
      "4 : 0.5591397849462365 0.5240963855421686\n",
      "5 : 0.5525873655913979 0.5807228915662651\n",
      "6 : 0.6009744623655914 0.5903614457831325\n",
      "7 : 0.5892137096774194 0.572289156626506\n",
      "8 : 0.5919018817204301 0.5650602409638554\n",
      "9 : 0.5877016129032258 0.5614457831325301\n",
      "10 : 0.5895497311827957 0.5927710843373494\n",
      "11 : 0.5971102150537635 0.608433734939759\n",
      "12 : 0.6065188172043011 0.5879518072289157\n",
      "13 : 0.5856854838709677 0.5108433734939759\n",
      "14 : 0.5853494623655914 0.5795180722891566\n",
      "15 : 0.5967741935483871 0.5301204819277109\n",
      "16 : 0.5885416666666666 0.5831325301204819\n",
      "17 : 0.5745967741935484 0.4783132530120482\n",
      "18 : 0.5979502688172043 0.5566265060240964\n",
      "19 : 0.5843413978494624 0.5108433734939759\n",
      "20 : 0.5893817204301075 0.5843373493975904\n",
      "21 : 0.6061827956989247 0.5409638554216868\n",
      "22 : 0.5542674731182796 0.5891566265060241\n",
      "24 : 0.5772849462365591 0.5542168674698795\n",
      "25 : 0.5818212365591398 0.5578313253012048\n",
      "26 : 0.587869623655914 0.5084337349397591\n",
      "27 : 0.6004704301075269 0.5855421686746988\n",
      "28 : 0.5643481182795699 0.5542168674698795\n",
      "29 : 0.6053427419354839 0.6445783132530121\n",
      "30 : 0.5977822580645161 0.5975903614457831\n",
      "31 : 0.5987903225806451 0.5819277108433735\n",
      "32 : 0.5932459677419355 0.591566265060241\n",
      "33 : 0.614247311827957 0.5590361445783133\n",
      "34 : 0.5897177419354839 0.6277108433734939\n",
      "35 : 0.6107190860215054 0.619277108433735\n",
      "36 : 0.5871975806451613 0.5698795180722892\n",
      "37 : 0.5989583333333334 0.5771084337349398\n",
      "38 : 0.6053427419354839 0.43373493975903615\n",
      "39 : 0.5858534946236559 0.5204819277108433\n",
      "40 : 0.6021505376344086 0.5650602409638554\n",
      "41 : 0.6031586021505376 0.5277108433734939\n",
      "42 : 0.5966061827956989 0.6\n",
      "43 : 0.5803091397849462 0.5397590361445783\n",
      "44 : 0.6060147849462365 0.572289156626506\n",
      "45 : 0.5858534946236559 0.5795180722891566\n",
      "46 : 0.5939180107526881 0.5987951807228916\n",
      "47 : 0.6063508064516129 0.5542168674698795\n",
      "48 : 0.5989583333333334 0.5867469879518072\n",
      "49 : 0.5950940860215054 0.563855421686747\n",
      "50 : 0.5972782258064516 0.5698795180722892\n",
      "51 : 0.5940860215053764 0.5506024096385542\n",
      "52 : 0.6018145161290323 0.5240963855421686\n",
      "53 : 0.5969422043010753 0.5662650602409639\n",
      "54 : 0.5974462365591398 0.5819277108433735\n",
      "55 : 0.5964381720430108 0.6662650602409639\n",
      "56 : 0.6003024193548387 0.5855421686746988\n",
      "57 : 0.5907258064516129 0.5879518072289157\n",
      "58 : 0.6110551075268817 0.43253012048192774\n",
      "59 : 0.5766129032258065 0.5855421686746988\n",
      "60 : 0.5667002688172043 0.42530120481927713\n",
      "61 : 0.5719086021505376 0.5951807228915663\n",
      "62 : 0.5887096774193549 0.5481927710843374\n",
      "63 : 0.5258736559139785 0.6289156626506024\n",
      "64 : 0.5633400537634409 0.5686746987951807\n",
      "65 : 0.5653561827956989 0.6759036144578313\n",
      "66 : 0.5700604838709677 0.5566265060240964\n",
      "70 : 0.5149529569892473 0.5939759036144578\n",
      "71 : 0.5082325268817204 0.5939759036144578\n",
      "72 : 0.5184811827956989 0.5373493975903615\n",
      "73 : 0.5120967741935484 0.5289156626506024\n",
      "74 : 0.49008736559139787 0.4530120481927711\n",
      "75 : 0.5021841397849462 0.4469879518072289\n",
      "76 : 0.5136088709677419 0.43253012048192774\n",
      "77 : 0.5455309139784946 0.5084337349397591\n",
      "78 : 0.5062163978494624 0.4879518072289157\n",
      "79 : 0.5650201612903226 0.6120481927710844\n",
      "80 : 0.5418346774193549 0.5481927710843374\n",
      "81 : 0.5267137096774194 0.5325301204819277\n",
      "82 : 0.5601478494623656 0.5301204819277109\n",
      "83 : 0.5505712365591398 0.5024096385542168\n",
      "84 : 0.555611559139785 0.3963855421686747\n",
      "85 : 0.5594758064516129 0.5421686746987951\n",
      "86 : 0.5777889784946236 0.5518072289156627\n",
      "87 : 0.5480510752688172 0.5096385542168674\n",
      "88 : 0.553763440860215 0.555421686746988\n",
      "89 : 0.577116935483871 0.5518072289156627\n",
      "90 : 0.594758064516129 0.5337349397590362\n",
      "91 : 0.5846774193548387 0.5686746987951807\n",
      "92 : 0.5745967741935484 0.5493975903614458\n",
      "93 : 0.5646841397849462 0.5144578313253012\n",
      "94 : 0.578125 0.5891566265060241\n",
      "95 : 0.6013104838709677 0.5674698795180723\n",
      "96 : 0.5856854838709677 0.5445783132530121\n",
      "97 : 0.577116935483871 0.5409638554216868\n",
      "98 : 0.571236559139785 0.43253012048192774\n",
      "99 : 0.5823252688172043 0.4746987951807229\n",
      "100 : 0.5992943548387096 0.5301204819277109\n",
      "101 : 0.5752688172043011 0.5445783132530121\n",
      "102 : 0.6018145161290323 0.5325301204819277\n",
      "103 : 0.5858534946236559 0.553012048192771\n",
      "104 : 0.5873655913978495 0.5891566265060241\n",
      "105 : 0.6065188172043011 0.5493975903614458\n",
      "106 : 0.587869623655914 0.5614457831325301\n",
      "107 : 0.6134072580645161 0.5084337349397591\n",
      "108 : 0.5651881720430108 0.553012048192771\n",
      "109 : 0.5818212365591398 0.5626506024096386\n",
      "110 : 0.6058467741935484 0.5457831325301205\n",
      "111 : 0.5705645161290323 0.5457831325301205\n",
      "112 : 0.5777889784946236 0.6024096385542169\n",
      "113 : 0.6129032258064516 0.5578313253012048\n",
      "114 : 0.5831653225806451 0.5686746987951807\n",
      "115 : 0.6001344086021505 0.553012048192771\n",
      "116 : 0.5737567204301075 0.5626506024096386\n",
      "117 : 0.5917338709677419 0.4602409638554217\n",
      "118 : 0.5806451612903226 0.5614457831325301\n",
      "119 : 0.6019825268817204 0.5614457831325301\n",
      "120 : 0.5845094086021505 0.5542168674698795\n",
      "121 : 0.5880376344086021 0.563855421686747\n",
      "122 : 0.5893817204301075 0.5289156626506024\n",
      "123 : 0.5972782258064516 0.5120481927710844\n",
      "124 : 0.5824932795698925 0.5650602409638554\n",
      "125 : 0.597614247311828 0.5710843373493976\n",
      "126 : 0.5831653225806451 0.5807228915662651\n",
      "127 : 0.6039986559139785 0.5253012048192771\n",
      "128 : 0.5769489247311828 0.5481927710843374\n",
      "129 : 0.5999663978494624 0.5662650602409639\n",
      "130 : 0.5853494623655914 0.4987951807228916\n",
      "131 : 0.5944220430107527 0.563855421686747\n",
      "132 : 0.5845094086021505 0.5216867469879518\n",
      "133 : 0.6068548387096774 0.6060240963855422\n",
      "134 : 0.5813172043010753 0.5650602409638554\n",
      "135 : 0.5972782258064516 0.5493975903614458\n",
      "136 : 0.6184475806451613 0.555421686746988\n",
      "137 : 0.579133064516129 0.5590361445783133\n",
      "138 : 0.5961021505376344 0.5397590361445783\n",
      "139 : 0.5848454301075269 0.553012048192771\n",
      "140 : 0.5892137096774194 0.5518072289156627\n",
      "141 : 0.5954301075268817 0.5614457831325301\n",
      "142 : 0.5991263440860215 0.5349397590361445\n",
      "143 : 0.6036626344086021 0.5602409638554217\n",
      "144 : 0.5934139784946236 0.5831325301204819\n",
      "145 : 0.5865255376344086 0.5951807228915663\n",
      "146 : 0.5929099462365591 0.5457831325301205\n",
      "147 : 0.5861895161290323 0.5518072289156627\n",
      "148 : 0.600638440860215 0.5614457831325301\n",
      "149 : 0.5887096774193549 0.572289156626506\n",
      "150 : 0.6004704301075269 0.5590361445783133\n",
      "151 : 0.5966061827956989 0.5686746987951807\n",
      "152 : 0.6117271505376344 0.6060240963855422\n",
      "153 : 0.594758064516129 0.5614457831325301\n",
      "154 : 0.6009744623655914 0.591566265060241\n",
      "155 : 0.5900537634408602 0.5493975903614458\n",
      "156 : 0.5994623655913979 0.5626506024096386\n",
      "157 : 0.5915658602150538 0.5795180722891566\n",
      "158 : 0.5934139784946236 0.5674698795180723\n",
      "159 : 0.5954301075268817 0.5686746987951807\n",
      "160 : 0.6058467741935484 0.553012048192771\n",
      "161 : 0.6009744623655914 0.5578313253012048\n",
      "162 : 0.5873655913978495 0.5566265060240964\n",
      "163 : 0.5950940860215054 0.5578313253012048\n",
      "164 : 0.5908938172043011 0.5710843373493976\n",
      "165 : 0.6004704301075269 0.5506024096385542\n",
      "166 : 0.6066868279569892 0.5120481927710844\n",
      "167 : 0.600638440860215 0.5277108433734939\n",
      "168 : 0.594758064516129 0.5590361445783133\n",
      "169 : 0.6122311827956989 0.5578313253012048\n",
      "170 : 0.5841733870967742 0.6048192771084338\n",
      "171 : 0.605510752688172 0.5759036144578313\n",
      "172 : 0.586861559139785 0.5506024096385542\n",
      "173 : 0.6036626344086021 0.6048192771084338\n",
      "174 : 0.5945900537634409 0.5674698795180723\n",
      "175 : 0.6011424731182796 0.5939759036144578\n",
      "176 : 0.5919018817204301 0.5963855421686747\n",
      "177 : 0.5994623655913979 0.4530120481927711\n",
      "178 : 0.5685483870967742 0.5963855421686747\n",
      "179 : 0.5752688172043011 0.5421686746987951\n",
      "180 : 0.6149193548387096 0.6120481927710844\n",
      "181 : 0.5653561827956989 0.536144578313253\n",
      "182 : 0.5964381720430108 0.5819277108433735\n",
      "183 : 0.5883736559139785 0.5987951807228916\n",
      "184 : 0.5561155913978495 0.5831325301204819\n",
      "185 : 0.5241935483870968 0.5831325301204819\n",
      "186 : 0.5136088709677419 0.3650602409638554\n",
      "187 : 0.5020161290322581 0.4855421686746988\n",
      "188 : 0.459005376344086 0.6132530120481928\n",
      "189 : 0.5367943548387096 0.5048192771084338\n",
      "190 : 0.49260752688172044 0.4975903614457831\n",
      "191 : 0.5384744623655914 0.5469879518072289\n",
      "192 : 0.5005040322580645 0.4578313253012048\n",
      "193 : 0.5367943548387096 0.5373493975903615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 : 0.5299059139784946 0.463855421686747\n",
      "195 : 0.5470430107526881 0.5626506024096386\n",
      "196 : 0.5411626344086021 0.5397590361445783\n",
      "197 : 0.5623319892473119 0.5277108433734939\n",
      "198 : 0.5702284946236559 0.6168674698795181\n",
      "199 : 0.588877688172043 0.5048192771084338\n",
      "200 : 0.525369623655914 0.572289156626506\n",
      "201 : 0.5497311827956989 0.5349397590361445\n",
      "202 : 0.5692204301075269 0.6289156626506024\n",
      "203 : 0.5685483870967742 0.4939759036144578\n",
      "204 : 0.5379704301075269 0.42771084337349397\n",
      "205 : 0.5283938172043011 0.4939759036144578\n",
      "206 : 0.5554435483870968 0.5746987951807229\n",
      "207 : 0.5774529569892473 0.37349397590361444\n",
      "208 : 0.5552755376344086 0.6036144578313253\n",
      "209 : 0.5347782258064516 0.38072289156626504\n",
      "210 : 0.5120967741935484 0.5445783132530121\n",
      "211 : 0.5423387096774194 0.5831325301204819\n",
      "212 : 0.572244623655914 0.572289156626506\n",
      "213 : 0.573252688172043 0.5240963855421686\n",
      "214 : 0.5596438172043011 0.4783132530120482\n",
      "215 : 0.515625 0.5120481927710844\n",
      "216 : 0.5646841397849462 0.5433734939759036\n",
      "217 : 0.5418346774193549 0.5409638554216868\n",
      "218 : 0.5784610215053764 0.6686746987951807\n",
      "219 : 0.5401545698924731 0.5734939759036145\n",
      "220 : 0.5 0.5265060240963856\n",
      "221 : 0.5608198924731183 0.5373493975903615\n",
      "222 : 0.5757728494623656 0.6024096385542169\n",
      "223 : 0.5922379032258065 0.5819277108433735\n",
      "224 : 0.5826612903225806 0.5156626506024097\n",
      "225 : 0.5756048387096774 0.5144578313253012\n",
      "226 : 0.5789650537634409 0.6277108433734939\n",
      "227 : 0.566364247311828 0.5542168674698795\n",
      "228 : 0.5584677419354839 0.4493975903614458\n",
      "229 : 0.5836693548387096 0.619277108433735\n",
      "230 : 0.5789650537634409 0.4975903614457831\n",
      "231 : 0.5540994623655914 0.5493975903614458\n",
      "232 : 0.5672043010752689 0.4951807228915663\n",
      "233 : 0.5754368279569892 0.5072289156626506\n",
      "234 : 0.5688844086021505 0.5614457831325301\n",
      "235 : 0.6065188172043011 0.5048192771084338\n",
      "236 : 0.6026545698924731 0.591566265060241\n",
      "237 : 0.558635752688172 0.5289156626506024\n",
      "238 : 0.573252688172043 0.5084337349397591\n",
      "239 : 0.5782930107526881 0.6313253012048192\n",
      "240 : 0.6053427419354839 0.6036144578313253\n",
      "241 : 0.5524193548387096 0.5578313253012048\n",
      "242 : 0.5971102150537635 0.5698795180722892\n",
      "243 : 0.610383064516129 0.5626506024096386\n",
      "244 : 0.553763440860215 0.5518072289156627\n",
      "245 : 0.6004704301075269 0.4759036144578313\n",
      "246 : 0.5831653225806451 0.5867469879518072\n",
      "247 : 0.6075268817204301 0.5421686746987951\n",
      "248 : 0.6150873655913979 0.5975903614457831\n",
      "249 : 0.5752688172043011 0.4963855421686747\n",
      "250 : 0.6088709677419355 0.5385542168674698\n",
      "251 : 0.5813172043010753 0.5590361445783133\n",
      "252 : 0.6048387096774194 0.5409638554216868\n",
      "253 : 0.6098790322580645 0.5566265060240964\n",
      "254 : 0.5665322580645161 0.5578313253012048\n",
      "255 : 0.5784610215053764 0.5867469879518072\n",
      "256 : 0.6053427419354839 0.5746987951807229\n",
      "257 : 0.6046706989247311 0.5879518072289157\n",
      "258 : 0.5628360215053764 0.5674698795180723\n",
      "259 : 0.5924059139784946 0.5566265060240964\n",
      "260 : 0.5949260752688172 0.5843373493975904\n",
      "261 : 0.6108870967741935 0.619277108433735\n",
      "262 : 0.6065188172043011 0.553012048192771\n",
      "263 : 0.5919018817204301 0.5409638554216868\n",
      "264 : 0.581989247311828 0.5662650602409639\n",
      "265 : 0.5818212365591398 0.5578313253012048\n",
      "266 : 0.5861895161290323 0.5518072289156627\n",
      "267 : 0.5982862903225806 0.555421686746988\n",
      "268 : 0.5954301075268817 0.5614457831325301\n",
      "269 : 0.6046706989247311 0.6132530120481928\n",
      "270 : 0.597614247311828 0.5698795180722892\n",
      "271 : 0.5944220430107527 0.5807228915662651\n",
      "272 : 0.5994623655913979 0.6072289156626506\n",
      "273 : 0.5950940860215054 0.4530120481927711\n",
      "274 : 0.5762768817204301 0.5927710843373494\n",
      "275 : 0.6043346774193549 0.5831325301204819\n",
      "276 : 0.5787970430107527 0.5698795180722892\n",
      "277 : 0.6113911290322581 0.5795180722891566\n",
      "278 : 0.5848454301075269 0.5734939759036145\n",
      "279 : 0.6122311827956989 0.5542168674698795\n",
      "280 : 0.5944220430107527 0.5542168674698795\n",
      "281 : 0.6026545698924731 0.5469879518072289\n",
      "282 : 0.6118951612903226 0.5421686746987951\n",
      "283 : 0.5903897849462365 0.5759036144578313\n",
      "284 : 0.5934139784946236 0.591566265060241\n",
      "285 : 0.6123991935483871 0.572289156626506\n",
      "286 : 0.6073588709677419 0.5409638554216868\n",
      "287 : 0.6102150537634409 0.536144578313253\n",
      "288 : 0.5999663978494624 0.5349397590361445\n",
      "289 : 0.5961021505376344 0.5771084337349398\n",
      "290 : 0.6115591397849462 0.5397590361445783\n",
      "291 : 0.594758064516129 0.5626506024096386\n",
      "292 : 0.6130712365591398 0.5433734939759036\n",
      "293 : 0.6031586021505376 0.5602409638554217\n",
      "294 : 0.621135752688172 0.5987951807228916\n",
      "295 : 0.5826612903225806 0.5819277108433735\n",
      "296 : 0.6081989247311828 0.5746987951807229\n",
      "297 : 0.6028225806451613 0.5481927710843374\n",
      "298 : 0.5944220430107527 0.5674698795180723\n",
      "299 : 0.5991263440860215 0.6132530120481928\n",
      "300 : 0.5981182795698925 0.5650602409638554\n",
      "301 : 0.6016465053763441 0.5602409638554217\n",
      "302 : 0.615255376344086 0.5746987951807229\n",
      "303 : 0.5924059139784946 0.6277108433734939\n",
      "304 : 0.6051747311827957 0.5987951807228916\n",
      "305 : 0.600638440860215 0.5891566265060241\n",
      "306 : 0.6068548387096774 0.6036144578313253\n",
      "307 : 0.604502688172043 0.5566265060240964\n",
      "308 : 0.6105510752688172 0.5542168674698795\n",
      "309 : 0.5981182795698925 0.6060240963855422\n",
      "310 : 0.6172715053763441 0.5734939759036145\n",
      "311 : 0.6147513440860215 0.5759036144578313\n",
      "312 : 0.6009744623655914 0.5746987951807229\n",
      "313 : 0.6100470430107527 0.5746987951807229\n",
      "314 : 0.6023185483870968 0.5674698795180723\n",
      "315 : 0.602486559139785 0.5819277108433735\n",
      "316 : 0.6090389784946236 0.5566265060240964\n",
      "317 : 0.6066868279569892 0.5843373493975904\n",
      "318 : 0.6029905913978495 0.5734939759036145\n",
      "319 : 0.5964381720430108 0.5831325301204819\n",
      "320 : 0.6038306451612904 0.5590361445783133\n",
      "321 : 0.610383064516129 0.6072289156626506\n",
      "322 : 0.6023185483870968 0.5795180722891566\n",
      "323 : 0.6097110215053764 0.5879518072289157\n",
      "324 : 0.6023185483870968 0.5602409638554217\n",
      "325 : 0.5917338709677419 0.5674698795180723\n",
      "326 : 0.6056787634408602 0.6012048192771084\n",
      "327 : 0.6090389784946236 0.5951807228915663\n",
      "328 : 0.5908938172043011 0.553012048192771\n",
      "329 : 0.5939180107526881 0.5939759036144578\n",
      "330 : 0.5314180107526881 0.4108433734939759\n",
      "331 : 0.5173051075268817 0.5421686746987951\n",
      "332 : 0.5090725806451613 0.5626506024096386\n",
      "333 : 0.5292338709677419 0.555421686746988\n",
      "334 : 0.5031922043010753 0.4493975903614458\n",
      "335 : 0.5184811827956989 0.38674698795180723\n",
      "336 : 0.5588037634408602 0.463855421686747\n",
      "337 : 0.4932795698924731 0.40481927710843374\n",
      "338 : 0.5364583333333334 0.46987951807228917\n",
      "339 : 0.5374663978494624 0.6240963855421687\n",
      "340 : 0.538138440860215 0.4783132530120482\n",
      "341 : 0.567372311827957 0.5132530120481927\n",
      "342 : 0.5529233870967742 0.5301204819277109\n",
      "343 : 0.5525873655913979 0.4602409638554217\n",
      "344 : 0.5285618279569892 0.4566265060240964\n",
      "345 : 0.5257056451612904 0.5228915662650603\n",
      "346 : 0.5656922043010753 0.5397590361445783\n",
      "347 : 0.5418346774193549 0.4710843373493976\n",
      "348 : 0.5477150537634409 0.5746987951807229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sazan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 : 0.5238575268817204 0.43373493975903615\n",
      "350 : 0.5277217741935484 0.5180722891566265\n",
      "351 : 0.5482190860215054 0.4734939759036145\n",
      "352 : 0.4741263440860215 0.4963855421686747\n",
      "353 : 0.5532594086021505 0.5349397590361445\n",
      "354 : 0.5653561827956989 0.5216867469879518\n",
      "355 : 0.5630040322580645 0.5204819277108433\n",
      "356 : 0.46774193548387094 0.5048192771084338\n",
      "357 : 0.5199932795698925 0.5493975903614458\n",
      "358 : 0.5645161290322581 0.555421686746988\n",
      "359 : 0.5655241935483871 0.5445783132530121\n",
      "360 : 0.5552755376344086 0.6132530120481928\n",
      "361 : 0.4986559139784946 0.4963855421686747\n",
      "362 : 0.5697244623655914 0.5240963855421686\n",
      "363 : 0.5658602150537635 0.572289156626506\n",
      "364 : 0.5769489247311828 0.5783132530120482\n",
      "365 : 0.5761088709677419 0.5578313253012048\n",
      "366 : 0.571236559139785 0.4530120481927711\n",
      "367 : 0.5425067204301075 0.35180722891566263\n",
      "368 : 0.5285618279569892 0.5036144578313253\n",
      "369 : 0.5505712365591398 0.5879518072289157\n",
      "370 : 0.5816532258064516 0.4783132530120482\n",
      "371 : 0.5848454301075269 0.6265060240963856\n",
      "372 : 0.6021505376344086 0.5132530120481927\n",
      "373 : 0.5403225806451613 0.4867469879518072\n",
      "374 : 0.5359543010752689 0.5313253012048192\n",
      "375 : 0.5935819892473119 0.5349397590361445\n",
      "376 : 0.5789650537634409 0.4963855421686747\n",
      "377 : 0.5357862903225806 0.5518072289156627\n",
      "378 : 0.5623319892473119 0.5132530120481927\n",
      "379 : 0.5915658602150538 0.5518072289156627\n",
      "380 : 0.6071908602150538 0.636144578313253\n",
      "381 : 0.5697244623655914 0.5698795180722892\n",
      "382 : 0.6056787634408602 0.5481927710843374\n",
      "383 : 0.6014784946236559 0.6397590361445783\n",
      "384 : 0.5561155913978495 0.5301204819277109\n",
      "385 : 0.5757728494623656 0.5867469879518072\n",
      "386 : 0.5908938172043011 0.6168674698795181\n",
      "387 : 0.5789650537634409 0.5578313253012048\n",
      "388 : 0.5515793010752689 0.5783132530120482\n",
      "389 : 0.5828293010752689 0.5734939759036145\n",
      "390 : 0.5964381720430108 0.6216867469879518\n",
      "391 : 0.5798051075268817 0.5349397590361445\n",
      "392 : 0.6053427419354839 0.5626506024096386\n",
      "393 : 0.6150873655913979 0.5626506024096386\n",
      "394 : 0.5887096774193549 0.5506024096385542\n",
      "395 : 0.5935819892473119 0.555421686746988\n",
      "396 : 0.5809811827956989 0.5518072289156627\n",
      "397 : 0.59375 0.5469879518072289\n",
      "398 : 0.582997311827957 0.5807228915662651\n",
      "399 : 0.6050067204301075 0.5650602409638554\n",
      "400 : 0.5651881720430108 0.5698795180722892\n",
      "401 : 0.5940860215053764 0.5951807228915663\n",
      "402 : 0.610383064516129 0.6\n",
      "403 : 0.5752688172043011 0.5253012048192771\n",
      "404 : 0.5804771505376344 0.6024096385542169\n",
      "405 : 0.5999663978494624 0.563855421686747\n",
      "406 : 0.5997983870967742 0.6\n",
      "407 : 0.5962701612903226 0.4891566265060241\n",
      "408 : 0.5796370967741935 0.5578313253012048\n",
      "409 : 0.6018145161290323 0.6301204819277109\n",
      "410 : 0.604502688172043 0.563855421686747\n",
      "411 : 0.5974462365591398 0.5891566265060241\n",
      "412 : 0.5851814516129032 0.5746987951807229\n",
      "413 : 0.5987903225806451 0.572289156626506\n",
      "414 : 0.5987903225806451 0.5253012048192771\n",
      "415 : 0.5905577956989247 0.5156626506024097\n",
      "416 : 0.5897177419354839 0.5614457831325301\n",
      "417 : 0.5939180107526881 0.6349397590361445\n",
      "418 : 0.5786290322580645 0.5771084337349398\n",
      "419 : 0.6164314516129032 0.6072289156626506\n",
      "420 : 0.5917338709677419 0.5349397590361445\n",
      "421 : 0.6095430107526881 0.5746987951807229\n",
      "422 : 0.605510752688172 0.5421686746987951\n",
      "423 : 0.5932459677419355 0.6036144578313253\n",
      "424 : 0.5992943548387096 0.5469879518072289\n",
      "425 : 0.6087029569892473 0.563855421686747\n",
      "426 : 0.5645161290322581 0.563855421686747\n",
      "427 : 0.5908938172043011 0.5975903614457831\n",
      "428 : 0.5892137096774194 0.5807228915662651\n",
      "429 : 0.5924059139784946 0.5048192771084338\n",
      "430 : 0.5929099462365591 0.5795180722891566\n",
      "431 : 0.569388440860215 0.5120481927710844\n",
      "432 : 0.6046706989247311 0.6325301204819277\n",
      "433 : 0.5690524193548387 0.5987951807228916\n",
      "434 : 0.5907258064516129 0.516867469879518\n",
      "435 : 0.4897513440860215 0.41927710843373495\n",
      "436 : 0.5383064516129032 0.5012048192771085\n",
      "437 : 0.5240255376344086 0.4614457831325301\n",
      "438 : 0.5171370967741935 0.555421686746988\n",
      "439 : 0.5394825268817204 0.5024096385542168\n",
      "440 : 0.5265456989247311 0.5421686746987951\n",
      "441 : 0.5349462365591398 0.4795180722891566\n",
      "442 : 0.49210349462365593 0.5578313253012048\n",
      "443 : 0.5492271505376344 0.5493975903614458\n",
      "444 : 0.5754368279569892 0.4867469879518072\n",
      "445 : 0.551747311827957 0.6036144578313253\n",
      "446 : 0.5539314516129032 0.5253012048192771\n",
      "447 : 0.5112567204301075 0.5108433734939759\n",
      "448 : 0.5804771505376344 0.45180722891566266\n",
      "449 : 0.5695564516129032 0.5469879518072289\n",
      "450 : 0.5787970430107527 0.5349397590361445\n",
      "451 : 0.573252688172043 0.5662650602409639\n",
      "452 : 0.5697244623655914 0.6036144578313253\n",
      "453 : 0.5700604838709677 0.5831325301204819\n",
      "454 : 0.5362903225806451 0.5506024096385542\n",
      "455 : 0.5403225806451613 0.4819277108433735\n",
      "456 : 0.555611559139785 0.6228915662650603\n",
      "457 : 0.5727486559139785 0.41566265060240964\n",
      "458 : 0.550739247311828 0.4987951807228916\n",
      "459 : 0.5483870967741935 0.5710843373493976\n",
      "460 : 0.5577956989247311 0.6108433734939759\n",
      "461 : 0.5863575268817204 0.519277108433735\n",
      "462 : 0.5804771505376344 0.5048192771084338\n",
      "463 : 0.5912298387096774 0.5831325301204819\n",
      "464 : 0.5714045698924731 0.5132530120481927\n",
      "465 : 0.5347782258064516 0.5542168674698795\n",
      "466 : 0.5465389784946236 0.5614457831325301\n",
      "467 : 0.584005376344086 0.5614457831325301\n",
      "468 : 0.5801411290322581 0.5457831325301205\n",
      "469 : 0.5816532258064516 0.536144578313253\n",
      "470 : 0.5756048387096774 0.5795180722891566\n",
      "471 : 0.5582997311827957 0.5409638554216868\n",
      "472 : 0.558635752688172 0.5385542168674698\n",
      "473 : 0.5823252688172043 0.519277108433735\n",
      "474 : 0.5897177419354839 0.5891566265060241\n",
      "475 : 0.5695564516129032 0.5734939759036145\n",
      "476 : 0.5994623655913979 0.5144578313253012\n",
      "477 : 0.5310819892473119 0.5855421686746988\n",
      "478 : 0.5999663978494624 0.5132530120481927\n",
      "479 : 0.6056787634408602 0.563855421686747\n",
      "480 : 0.586861559139785 0.5975903614457831\n",
      "481 : 0.5955981182795699 0.5373493975903615\n",
      "482 : 0.5670362903225806 0.5626506024096386\n",
      "483 : 0.5734206989247311 0.6012048192771084\n",
      "484 : 0.5861895161290323 0.553012048192771\n",
      "485 : 0.5648521505376344 0.5373493975903615\n",
      "486 : 0.5912298387096774 0.6\n",
      "487 : 0.6038306451612904 0.6156626506024097\n",
      "488 : 0.5912298387096774 0.5903614457831325\n",
      "489 : 0.5922379032258065 0.6048192771084338\n",
      "490 : 0.5759408602150538 0.5204819277108433\n",
      "491 : 0.602486559139785 0.5879518072289157\n",
      "492 : 0.5680443548387096 0.5\n",
      "493 : 0.6018145161290323 0.5421686746987951\n",
      "494 : 0.577116935483871 0.516867469879518\n",
      "495 : 0.5915658602150538 0.5759036144578313\n",
      "496 : 0.5804771505376344 0.653012048192771\n",
      "497 : 0.5912298387096774 0.5144578313253012\n",
      "498 : 0.5702284946236559 0.5433734939759036\n",
      "499 : 0.5979502688172043 0.5698795180722892\n",
      "500 : 0.5851814516129032 0.5759036144578313\n",
      "501 : 0.5966061827956989 0.553012048192771\n",
      "502 : 0.5761088709677419 0.5650602409638554\n",
      "503 : 0.6031586021505376 0.5518072289156627\n",
      "504 : 0.5907258064516129 0.5734939759036145\n",
      "505 : 0.6087029569892473 0.5674698795180723\n",
      "506 : 0.6100470430107527 0.5469879518072289\n",
      "507 : 0.5719086021505376 0.5759036144578313\n",
      "508 : 0.5972782258064516 0.563855421686747\n",
      "509 : 0.5955981182795699 0.536144578313253\n",
      "510 : 0.5892137096774194 0.5650602409638554\n",
      "511 : 0.6061827956989247 0.5710843373493976\n",
      "512 : 0.6033266129032258 0.5855421686746988\n",
      "513 : 0.5806451612903226 0.5590361445783133\n",
      "514 : 0.5897177419354839 0.5457831325301205\n",
      "515 : 0.6080309139784946 0.6204819277108434\n",
      "516 : 0.6004704301075269 0.5951807228915663\n",
      "517 : 0.6122311827956989 0.6012048192771084\n",
      "518 : 0.5826612903225806 0.5385542168674698\n",
      "519 : 0.5903897849462365 0.5433734939759036\n",
      "520 : 0.5436827956989247 0.5060240963855421\n",
      "521 : 0.5843413978494624 0.5843373493975904\n",
      "522 : 0.5720766129032258 0.4795180722891566\n",
      "523 : 0.5305779569892473 0.5542168674698795\n",
      "524 : 0.5789650537634409 0.5975903614457831\n",
      "525 : 0.5972782258064516 0.5843373493975904\n",
      "526 : 0.5754368279569892 0.6843373493975904\n",
      "527 : 0.5561155913978495 0.5903614457831325\n",
      "528 : 0.5601478494623656 0.4530120481927711\n",
      "529 : 0.532258064516129 0.5313253012048192\n",
      "530 : 0.5336021505376344 0.40602409638554215\n",
      "531 : 0.5682123655913979 0.491566265060241\n",
      "532 : 0.5374663978494624 0.555421686746988\n",
      "533 : 0.5851814516129032 0.5867469879518072\n",
      "534 : 0.5924059139784946 0.6036144578313253\n",
      "535 : 0.5729166666666666 0.6156626506024097\n",
      "536 : 0.5845094086021505 0.5674698795180723\n",
      "537 : 0.5658602150537635 0.6325301204819277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538 : 0.5974462365591398 0.5843373493975904\n",
      "539 : 0.5816532258064516 0.5710843373493976\n",
      "540 : 0.556619623655914 0.5783132530120482\n",
      "541 : 0.6038306451612904 0.5771084337349398\n",
      "542 : 0.5913978494623656 0.6072289156626506\n",
      "543 : 0.5987903225806451 0.591566265060241\n",
      "544 : 0.6113911290322581 0.5759036144578313\n",
      "545 : 0.5631720430107527 0.5469879518072289\n",
      "546 : 0.5863575268817204 0.5927710843373494\n",
      "547 : 0.6050067204301075 0.5289156626506024\n",
      "548 : 0.600638440860215 0.5578313253012048\n",
      "549 : 0.5917338709677419 0.5662650602409639\n",
      "550 : 0.603494623655914 0.555421686746988\n",
      "551 : 0.6013104838709677 0.563855421686747\n",
      "552 : 0.5954301075268817 0.5301204819277109\n",
      "553 : 0.6058467741935484 0.5602409638554217\n",
      "554 : 0.5971102150537635 0.5698795180722892\n",
      "555 : 0.5991263440860215 0.5807228915662651\n",
      "556 : 0.6090389784946236 0.5397590361445783\n",
      "557 : 0.5814852150537635 0.563855421686747\n",
      "558 : 0.6058467741935484 0.5602409638554217\n",
      "559 : 0.613239247311828 0.5843373493975904\n",
      "560 : 0.5991263440860215 0.6168674698795181\n",
      "561 : 0.5935819892473119 0.553012048192771\n",
      "562 : 0.6039986559139785 0.5421686746987951\n",
      "563 : 0.6038306451612904 0.5662650602409639\n",
      "564 : 0.6056787634408602 0.5481927710843374\n",
      "565 : 0.59375 0.5734939759036145\n",
      "566 : 0.6009744623655914 0.5048192771084338\n",
      "567 : 0.600638440860215 0.5759036144578313\n",
      "568 : 0.5925739247311828 0.5987951807228916\n",
      "569 : 0.610383064516129 0.553012048192771\n",
      "570 : 0.584005376344086 0.5807228915662651\n",
      "571 : 0.5999663978494624 0.5698795180722892\n",
      "572 : 0.5979502688172043 0.5903614457831325\n",
      "573 : 0.6135752688172043 0.5253012048192771\n",
      "574 : 0.6135752688172043 0.5228915662650603\n",
      "575 : 0.6001344086021505 0.5698795180722892\n",
      "576 : 0.5979502688172043 0.5867469879518072\n",
      "577 : 0.6038306451612904 0.5542168674698795\n",
      "578 : 0.5949260752688172 0.5433734939759036\n",
      "579 : 0.602486559139785 0.5325301204819277\n",
      "580 : 0.6068548387096774 0.5819277108433735\n",
      "581 : 0.5853494623655914 0.5481927710843374\n",
      "582 : 0.5935819892473119 0.5662650602409639\n",
      "583 : 0.608366935483871 0.5542168674698795\n",
      "584 : 0.592741935483871 0.563855421686747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "EPOCH_NUM = 1000\n",
    "\n",
    "for e in range(EPOCH_NUM):\n",
    "    train_pred, train_true, train_loss = epoch(model, optimizer, criterion, \n",
    "                                   is_training=True, loader=train_loader)\n",
    "    val_pred, val_true, val_loss = epoch(model, optimizer, criterion, \n",
    "                               is_training=False, loader=val_loader)\n",
    "    \n",
    "    p, r, f, s = precision_recall_fscore_support(y_true=val_true, y_pred=val_pred)\n",
    "    train_acc = (train_pred==train_true).sum() / train_true.shape[0]\n",
    "    val_acc = (val_pred==val_true).sum() / val_true.shape[0]\n",
    "    print(e, ':', train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
