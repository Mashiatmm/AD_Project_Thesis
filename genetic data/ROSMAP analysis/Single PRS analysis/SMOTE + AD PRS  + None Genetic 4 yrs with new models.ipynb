{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the input dataset :\n",
    "\n",
    "1. usable_samples_ADNI.json : stores the IID (index) for each row of PRS_feature_matrix.npy\n",
    "2. PRS_feature_matrix.npy : PR Score for different features\n",
    "3. Covar_FILE_bigger_dataset : for reading covar such as age, gender\n",
    "4. Final_Samples.json : contains ID and output for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tpot\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install skfeature-chappers\n",
    "# !pip install mlxtend\n",
    "# !pip --version\n",
    "# !pip install imblearn\n",
    "# !conda install -c conda-forge imbalanced-learn\n",
    "# conda install -c conda-forge tqdm\n",
    "# conda install mlxtend\n",
    "\n",
    "# conda config --add channels conda-forge\n",
    "# conda install hyperopt\n",
    "\n",
    "# conda config --add channels conda-forge\n",
    "# conda config --set channel_priority strict\n",
    "# conda install auto-sklearn\n",
    "# conda install -c conda-forge tpot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the installed packages with their versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/bayzid/anaconda3/envs/ad_venv_2:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                  2_kmp_llvm    conda-forge\n",
      "_py-xgboost-mutex         2.0                       cpu_0    conda-forge\n",
      "_r-mutex                  1.0.1               anacondar_1    conda-forge\n",
      "abseil-cpp                20211102.0           h93e1e8c_2    conda-forge\n",
      "absl-py                   1.4.0              pyhd8ed1ab_0    conda-forge\n",
      "aiohttp                   3.8.4           py310h1fa729e_0    conda-forge\n",
      "aiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge\n",
      "alsa-lib                  1.2.6.1              h7f98852_0    conda-forge\n",
      "anyio                     3.6.2              pyhd8ed1ab_0    conda-forge\n",
      "aom                       3.5.0                h27087fc_0    conda-forge\n",
      "argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge\n",
      "argon2-cffi-bindings      21.2.0          py310h5764c6d_3    conda-forge\n",
      "asttokens                 2.2.0              pyhd8ed1ab_0    conda-forge\n",
      "astunparse                1.6.3              pyhd8ed1ab_0    conda-forge\n",
      "async-timeout             4.0.2              pyhd8ed1ab_0    conda-forge\n",
      "attr                      2.5.1                h166bdaf_1    conda-forge\n",
      "attrs                     22.1.0             pyh71513ae_1    conda-forge\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\n",
      "backports                 1.0                pyhd8ed1ab_3    conda-forge\n",
      "backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge\n",
      "beautifulsoup4            4.11.1             pyha770c72_0    conda-forge\n",
      "binutils_impl_linux-64    2.39                 he00db2b_1    conda-forge\n",
      "binutils_linux-64         2.39                h5fc0e48_11    conda-forge\n",
      "blas                      2.116                  openblas    conda-forge\n",
      "blas-devel                3.9.0           16_linux64_openblas    conda-forge\n",
      "bleach                    5.0.1              pyhd8ed1ab_0    conda-forge\n",
      "blinker                   1.5                pyhd8ed1ab_0    conda-forge\n",
      "blosc                     1.21.3               hafa529b_0    conda-forge\n",
      "bottleneck                1.3.5           py310hde88566_1    conda-forge\n",
      "brotli                    1.0.9                h166bdaf_8    conda-forge\n",
      "brotli-bin                1.0.9                h166bdaf_8    conda-forge\n",
      "brotlipy                  0.7.0           py310h5764c6d_1005    conda-forge\n",
      "brunsli                   0.1                  h9c3ff4c_0    conda-forge\n",
      "bwidget                   1.9.14               ha770c72_1    conda-forge\n",
      "bzip2                     1.0.8                h7f98852_4    conda-forge\n",
      "c-ares                    1.18.1               h7f98852_0    conda-forge\n",
      "c-blosc2                  2.6.1                hf91038e_0    conda-forge\n",
      "ca-certificates           2022.12.7            ha878542_0    conda-forge\n",
      "cached-property           1.5.2                hd8ed1ab_1    conda-forge\n",
      "cached_property           1.5.2              pyha770c72_1    conda-forge\n",
      "cachetools                5.3.0              pyhd8ed1ab_0    conda-forge\n",
      "cairo                     1.16.0            ha61ee94_1012    conda-forge\n",
      "certifi                   2022.12.7          pyhd8ed1ab_0    conda-forge\n",
      "cffi                      1.15.1          py310h255011f_2    conda-forge\n",
      "cfitsio                   4.1.0                hd9d235c_0    conda-forge\n",
      "charls                    2.3.4                h9c3ff4c_0    conda-forge\n",
      "charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge\n",
      "click                     8.1.3           unix_pyhd8ed1ab_2    conda-forge\n",
      "cloudpickle               2.2.0              pyhd8ed1ab_0    conda-forge\n",
      "colorama                  0.4.6              pyhd8ed1ab_0    conda-forge\n",
      "contourpy                 1.0.6           py310hbf28c38_0    conda-forge\n",
      "cryptography              38.0.4          py310h597c629_0    conda-forge\n",
      "cudatoolkit               10.2.89             h713d32c_11    conda-forge\n",
      "cudnn                     7.6.5.32             h01f27c4_1    conda-forge\n",
      "curl                      7.86.0               h7bff187_1    conda-forge\n",
      "cycler                    0.11.0             pyhd8ed1ab_0    conda-forge\n",
      "cytoolz                   0.12.0          py310h5764c6d_1    conda-forge\n",
      "dask-core                 2023.2.0           pyhd8ed1ab_0    conda-forge\n",
      "dav1d                     1.0.0                h166bdaf_1    conda-forge\n",
      "dbus                      1.13.6               h5008d03_3    conda-forge\n",
      "deap                      1.3.3           py310h769672d_1    conda-forge\n",
      "debugpy                   1.6.4           py310hd8f1fbe_0    conda-forge\n",
      "decorator                 5.1.1              pyhd8ed1ab_0    conda-forge\n",
      "defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge\n",
      "deprecated                1.2.13                   pypi_0    pypi\n",
      "entrypoints               0.4                pyhd8ed1ab_0    conda-forge\n",
      "et_xmlfile                1.0.1                   py_1001    conda-forge\n",
      "exceptiongroup            1.1.0                    pypi_0    pypi\n",
      "executing                 1.2.0              pyhd8ed1ab_0    conda-forge\n",
      "expat                     2.5.0                h27087fc_0    conda-forge\n",
      "ffmpeg                    4.4.2           gpl_hc51e5dc_110    conda-forge\n",
      "fftw                      3.3.10          nompi_hf0379b8_105    conda-forge\n",
      "flit-core                 3.8.0              pyhd8ed1ab_0    conda-forge\n",
      "font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge\n",
      "font-ttf-inconsolata      3.000                h77eed37_0    conda-forge\n",
      "font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge\n",
      "font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge\n",
      "fontconfig                2.14.0               h8e229c2_0    conda-forge\n",
      "fonts-conda-ecosystem     1                             0    conda-forge\n",
      "fonts-conda-forge         1                             0    conda-forge\n",
      "fonttools                 4.38.0          py310h5764c6d_1    conda-forge\n",
      "freeglut                  3.2.2                h9c3ff4c_1    conda-forge\n",
      "freetype                  2.12.1               hca18f0e_1    conda-forge\n",
      "fribidi                   1.0.10               h36c2ea0_0    conda-forge\n",
      "frozenlist                1.3.3           py310h5764c6d_0    conda-forge\n",
      "fsspec                    2023.1.0           pyhd8ed1ab_0    conda-forge\n",
      "gast                      0.4.0              pyh9f0ad1d_0    conda-forge\n",
      "gcc_impl_linux-64         12.2.0              hcc96c02_19    conda-forge\n",
      "gcc_linux-64              12.2.0              h4798a0e_11    conda-forge\n",
      "gettext                   0.21.1               h27087fc_0    conda-forge\n",
      "gfortran_impl_linux-64    12.2.0              h55be85b_19    conda-forge\n",
      "gfortran_linux-64         12.2.0              h307d370_11    conda-forge\n",
      "giflib                    5.2.1                h36c2ea0_2    conda-forge\n",
      "glib                      2.74.1               h6239696_0    conda-forge\n",
      "glib-tools                2.74.1               h6239696_0    conda-forge\n",
      "gmp                       6.2.1                h58526e2_0    conda-forge\n",
      "gnutls                    3.7.8                hf3e180e_0    conda-forge\n",
      "google-auth               2.16.1             pyh1a96a4e_0    conda-forge\n",
      "google-auth-oauthlib      0.4.6              pyhd8ed1ab_0    conda-forge\n",
      "google-pasta              0.2.0              pyh8c360ce_0    conda-forge\n",
      "graphite2                 1.3.13            h58526e2_1001    conda-forge\n",
      "grpc-cpp                  1.46.4               h6fc47f4_3    conda-forge\n",
      "grpcio                    1.46.4          py310he36eabb_3    conda-forge\n",
      "gsl                       2.7                  he838d99_0    conda-forge\n",
      "gst-plugins-base          1.20.3               hf6a322e_0    conda-forge\n",
      "gstreamer                 1.20.3               hd4edc92_2    conda-forge\n",
      "gxx_impl_linux-64         12.2.0              hcc96c02_19    conda-forge\n",
      "gxx_linux-64              12.2.0              hb41e900_11    conda-forge\n",
      "h5py                      3.8.0           nompi_py310h0311031_100    conda-forge\n",
      "harfbuzz                  5.1.0                hf9f4e7c_0    conda-forge\n",
      "hdf5                      1.12.2          nompi_h2386368_100    conda-forge\n",
      "icu                       70.1                 h27087fc_0    conda-forge\n",
      "idna                      3.4                pyhd8ed1ab_0    conda-forge\n",
      "imagecodecs               2022.7.27       py310h1281eb2_0    conda-forge\n",
      "imageio                   2.25.1             pyh24c5eb1_0    conda-forge\n",
      "imbalanced-learn          0.9.1              pyhd8ed1ab_1    conda-forge\n",
      "importlib-metadata        5.1.0              pyha770c72_0    conda-forge\n",
      "importlib_resources       5.10.0             pyhd8ed1ab_0    conda-forge\n",
      "iniconfig                 2.0.0                    pypi_0    pypi\n",
      "intel-openmp              2022.1.0          h9e868ea_3769  \n",
      "ipykernel                 6.17.1             pyh210e3f2_0    conda-forge\n",
      "ipython                   8.6.0              pyh41d4057_1    conda-forge\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\n",
      "ipywidgets                8.0.2              pyhd8ed1ab_1    conda-forge\n",
      "jack                      1.9.18            h8c3723f_1002    conda-forge\n",
      "jasper                    2.0.33               h0ff4b12_1    conda-forge\n",
      "jedi                      0.18.2             pyhd8ed1ab_0    conda-forge\n",
      "jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge\n",
      "joblib                    1.2.0              pyhd8ed1ab_0    conda-forge\n",
      "jpeg                      9e                   h166bdaf_2    conda-forge\n",
      "jsonschema                4.17.3             pyhd8ed1ab_0    conda-forge\n",
      "jupyter                   1.0.0           py310hff52083_7    conda-forge\n",
      "jupyter_client            7.4.7              pyhd8ed1ab_0    conda-forge\n",
      "jupyter_console           6.4.4              pyhd8ed1ab_0    conda-forge\n",
      "jupyter_core              5.1.0           py310hff52083_0    conda-forge\n",
      "jupyter_server            1.23.3             pyhd8ed1ab_0    conda-forge\n",
      "jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge\n",
      "jupyterlab_widgets        3.0.3              pyhd8ed1ab_0    conda-forge\n",
      "jxrlib                    1.1                  h7f98852_2    conda-forge\n",
      "keras                     2.9.0              pyhd8ed1ab_0    conda-forge\n",
      "keras-preprocessing       1.1.2              pyhd8ed1ab_0    conda-forge\n",
      "kernel-headers_linux-64   2.6.32              he073ed8_15    conda-forge\n",
      "keyutils                  1.6.1                h166bdaf_0    conda-forge\n",
      "kiwisolver                1.4.4           py310hbf28c38_1    conda-forge\n",
      "krb5                      1.19.3               h3790be6_0    conda-forge\n",
      "lame                      3.100             h166bdaf_1003    conda-forge\n",
      "lcms2                     2.14                 h6ed2654_0    conda-forge\n",
      "ld_impl_linux-64          2.39                 hcc3a1bd_1    conda-forge\n",
      "lerc                      3.0                  h9c3ff4c_0    conda-forge\n",
      "libabseil                 20211102.0      cxx17_h48a1fff_2    conda-forge\n",
      "libaec                    1.0.6                hcb278e6_1    conda-forge\n",
      "libavif                   0.10.1               h5cdd6b5_2    conda-forge\n",
      "libblas                   3.9.0           16_linux64_openblas    conda-forge\n",
      "libbrotlicommon           1.0.9                h166bdaf_8    conda-forge\n",
      "libbrotlidec              1.0.9                h166bdaf_8    conda-forge\n",
      "libbrotlienc              1.0.9                h166bdaf_8    conda-forge\n",
      "libcap                    2.64                 ha37c62d_0    conda-forge\n",
      "libcblas                  3.9.0           16_linux64_openblas    conda-forge\n",
      "libclang                  14.0.6          default_h2e3cab8_0    conda-forge\n",
      "libclang13                14.0.6          default_h3a83d3e_0    conda-forge\n",
      "libcups                   2.3.3                h3e49a29_2    conda-forge\n",
      "libcurl                   7.86.0               h7bff187_1    conda-forge\n",
      "libdb                     6.2.32               h9c3ff4c_0    conda-forge\n",
      "libdeflate                1.12                 h166bdaf_0    conda-forge\n",
      "libdrm                    2.4.114              h166bdaf_0    conda-forge\n",
      "libedit                   3.1.20191231         he28a2e2_2    conda-forge\n",
      "libev                     4.33                 h516909a_1    conda-forge\n",
      "libevent                  2.1.10               h9b69904_4    conda-forge\n",
      "libffi                    3.4.2                h7f98852_5    conda-forge\n",
      "libflac                   1.3.4                h27087fc_0    conda-forge\n",
      "libgcc                    5.2.0                         0    conda-forge\n",
      "libgcc-devel_linux-64     12.2.0              h3b97bd3_19    conda-forge\n",
      "libgcc-ng                 12.2.0              h65d4601_19    conda-forge\n",
      "libgfortran-ng            12.2.0              h69a702a_19    conda-forge\n",
      "libgfortran5              12.2.0              h337968e_19    conda-forge\n",
      "libglib                   2.74.1               h7a41b64_0    conda-forge\n",
      "libglu                    9.0.0             he1b5a44_1001    conda-forge\n",
      "libgomp                   12.2.0              h65d4601_19    conda-forge\n",
      "libiconv                  1.17                 h166bdaf_0    conda-forge\n",
      "libidn2                   2.3.4                h166bdaf_0    conda-forge\n",
      "liblapack                 3.9.0           16_linux64_openblas    conda-forge\n",
      "liblapacke                3.9.0           16_linux64_openblas    conda-forge\n",
      "libllvm10                 10.0.1               he513fc3_3    conda-forge\n",
      "libllvm11                 11.1.0               he0ac6c6_5    conda-forge\n",
      "libllvm14                 14.0.6               he0ac6c6_1    conda-forge\n",
      "libnghttp2                1.47.0               hdcd2b5c_1    conda-forge\n",
      "libnsl                    2.0.0                h7f98852_0    conda-forge\n",
      "libogg                    1.3.4                h7f98852_1    conda-forge\n",
      "libopenblas               0.3.21          pthreads_h78a6416_3    conda-forge\n",
      "libopencv                 4.6.0           py310hedba25b_3    conda-forge\n",
      "libopus                   1.3.1                h7f98852_1    conda-forge\n",
      "libpciaccess              0.17                 h166bdaf_0    conda-forge\n",
      "libpng                    1.6.39               h753d276_0    conda-forge\n",
      "libpq                     14.5                 hd77ab85_1    conda-forge\n",
      "libprotobuf               3.20.2               h6239696_0    conda-forge\n",
      "libsanitizer              12.2.0              h46fd767_19    conda-forge\n",
      "libsndfile                1.0.31               h9c3ff4c_1    conda-forge\n",
      "libsodium                 1.0.18               h36c2ea0_1    conda-forge\n",
      "libsqlite                 3.40.0               h753d276_0    conda-forge\n",
      "libssh2                   1.10.0               ha56f1ee_2    conda-forge\n",
      "libstdcxx-devel_linux-64  12.2.0              h3b97bd3_19    conda-forge\n",
      "libstdcxx-ng              12.2.0              h46fd767_19    conda-forge\n",
      "libtasn1                  4.19.0               h166bdaf_0    conda-forge\n",
      "libtiff                   4.4.0                hc85c160_1    conda-forge\n",
      "libtool                   2.4.6             h9c3ff4c_1008    conda-forge\n",
      "libudev1                  252                  h166bdaf_0    conda-forge\n",
      "libunistring              0.9.10               h7f98852_0    conda-forge\n",
      "libuuid                   2.32.1            h7f98852_1000    conda-forge\n",
      "libva                     2.17.0               h0b41bf4_0    conda-forge\n",
      "libvorbis                 1.3.7                h9c3ff4c_0    conda-forge\n",
      "libvpx                    1.11.0               h9c3ff4c_3    conda-forge\n",
      "libwebp                   1.2.4                h522a892_0    conda-forge\n",
      "libwebp-base              1.2.4                h166bdaf_0    conda-forge\n",
      "libxcb                    1.13              h7f98852_1004    conda-forge\n",
      "libxgboost                1.7.1            cpu_ha3b9936_0    conda-forge\n",
      "libxkbcommon              1.0.3                he3ba5ed_0    conda-forge\n",
      "libxml2                   2.10.3               h7463322_0    conda-forge\n",
      "libxslt                   1.1.37               h873f0b0_0    conda-forge\n",
      "libzlib                   1.2.13               h166bdaf_4    conda-forge\n",
      "libzopfli                 1.0.3                h9c3ff4c_0    conda-forge\n",
      "llvm-openmp               15.0.6               he0ac6c6_0    conda-forge\n",
      "llvmlite                  0.39.1          py310h58363a5_1    conda-forge\n",
      "locket                    1.0.0              pyhd8ed1ab_0    conda-forge\n",
      "lz4-c                     1.9.3                h9c3ff4c_1    conda-forge\n",
      "magma                     2.5.4                hfead8bd_4    conda-forge\n",
      "make                      4.3                  hd18ef5c_1    conda-forge\n",
      "markdown                  3.4.1              pyhd8ed1ab_0    conda-forge\n",
      "markupsafe                2.1.1           py310h5764c6d_2    conda-forge\n",
      "matplotlib                3.6.2           py310hff52083_0    conda-forge\n",
      "matplotlib-base           3.6.2           py310h8d5ebf3_0    conda-forge\n",
      "matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge\n",
      "mistune                   2.0.4              pyhd8ed1ab_0    conda-forge\n",
      "mkl                       2022.2.1         h84fe81f_16996    conda-forge\n",
      "mlxtend                   0.21.0             pyhd8ed1ab_0    conda-forge\n",
      "multidict                 6.0.4           py310h1fa729e_0    conda-forge\n",
      "munkres                   1.1.4              pyh9f0ad1d_0    conda-forge\n",
      "mysql-common              8.0.31               haf5c9bc_0    conda-forge\n",
      "mysql-libs                8.0.31               h28c427c_0    conda-forge\n",
      "nbclassic                 0.4.8              pyhd8ed1ab_0    conda-forge\n",
      "nbclient                  0.7.2              pyhd8ed1ab_0    conda-forge\n",
      "nbconvert                 7.2.5              pyhd8ed1ab_0    conda-forge\n",
      "nbconvert-core            7.2.5              pyhd8ed1ab_0    conda-forge\n",
      "nbconvert-pandoc          7.2.5              pyhd8ed1ab_0    conda-forge\n",
      "nbformat                  5.7.0              pyhd8ed1ab_0    conda-forge\n",
      "nccl                      2.14.3.1             h1a5f58c_0    conda-forge\n",
      "ncurses                   6.3                  h27087fc_1    conda-forge\n",
      "nest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge\n",
      "nettle                    3.8.1                hc379101_1    conda-forge\n",
      "networkx                  3.0                pyhd8ed1ab_0    conda-forge\n",
      "ninja                     1.11.0               h924138e_0    conda-forge\n",
      "notebook                  6.5.2              pyha770c72_1    conda-forge\n",
      "notebook-shim             0.2.2              pyhd8ed1ab_0    conda-forge\n",
      "nspr                      4.35                 h27087fc_0    conda-forge\n",
      "nss                       3.82                 he02c5a1_0    conda-forge\n",
      "numba                     0.56.4          py310ha5257ce_0    conda-forge\n",
      "numexpr                   2.7.3           py310hb5077e9_1    conda-forge\n",
      "numpy                     1.23.5          py310h53a5b5f_0    conda-forge\n",
      "oauthlib                  3.2.2              pyhd8ed1ab_0    conda-forge\n",
      "openblas                  0.3.21          pthreads_h320a7e8_3    conda-forge\n",
      "opencv                    4.6.0           py310hff52083_3    conda-forge\n",
      "openh264                  2.3.1                h27087fc_1    conda-forge\n",
      "openjpeg                  2.5.0                h7d73246_1    conda-forge\n",
      "openpyxl                  3.0.9              pyhd8ed1ab_0    conda-forge\n",
      "openssl                   1.1.1t               h0b41bf4_0    conda-forge\n",
      "opt_einsum                3.3.0              pyhd8ed1ab_1    conda-forge\n",
      "p11-kit                   0.24.1               hc5aa10d_0    conda-forge\n",
      "packaging                 21.3               pyhd8ed1ab_0    conda-forge\n",
      "pandas                    1.5.2           py310h769672d_0    conda-forge\n",
      "pandas-plink              2.2.9                    pypi_0    pypi\n",
      "pandoc                    2.19.2               ha770c72_0    conda-forge\n",
      "pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge\n",
      "pango                     1.50.9               hc4f8a73_0    conda-forge\n",
      "parso                     0.8.3              pyhd8ed1ab_0    conda-forge\n",
      "partd                     1.3.0              pyhd8ed1ab_0    conda-forge\n",
      "patsy                     0.5.3              pyhd8ed1ab_0    conda-forge\n",
      "pcre                      8.45                 h9c3ff4c_0    conda-forge\n",
      "pcre2                     10.37                h032f7d1_0    conda-forge\n",
      "pexpect                   4.8.0              pyh1a96a4e_2    conda-forge\n",
      "pickleshare               0.7.5                   py_1003    conda-forge\n",
      "pillow                    9.2.0           py310hbd86126_2    conda-forge\n",
      "pip                       22.3.1             pyhd8ed1ab_0    conda-forge\n",
      "pixman                    0.40.0               h36c2ea0_0    conda-forge\n",
      "pkgutil-resolve-name      1.3.10             pyhd8ed1ab_0    conda-forge\n",
      "platformdirs              2.5.2              pyhd8ed1ab_1    conda-forge\n",
      "pluggy                    1.0.0                    pypi_0    pypi\n",
      "ply                       3.11                       py_1    conda-forge\n",
      "portaudio                 19.6.0               h57a0ea0_5    conda-forge\n",
      "prometheus_client         0.15.0             pyhd8ed1ab_0    conda-forge\n",
      "prompt-toolkit            3.0.33             pyha770c72_0    conda-forge\n",
      "prompt_toolkit            3.0.33               hd8ed1ab_0    conda-forge\n",
      "protobuf                  3.20.2          py310hd8f1fbe_1    conda-forge\n",
      "psutil                    5.9.4           py310h5764c6d_0    conda-forge\n",
      "pthread-stubs             0.4               h36c2ea0_1001    conda-forge\n",
      "ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\n",
      "pulseaudio                14.0                 h7f54b18_8    conda-forge\n",
      "pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge\n",
      "py-opencv                 4.6.0           py310hfdc917e_3    conda-forge\n",
      "py-xgboost                1.7.1           cpu_py310hd1aba9c_0    conda-forge\n",
      "pyasn1                    0.4.8                      py_0    conda-forge\n",
      "pyasn1-modules            0.2.7                      py_0    conda-forge\n",
      "pycparser                 2.21               pyhd8ed1ab_0    conda-forge\n",
      "pygments                  2.13.0             pyhd8ed1ab_0    conda-forge\n",
      "pyjwt                     2.6.0              pyhd8ed1ab_0    conda-forge\n",
      "pyopenssl                 22.1.0             pyhd8ed1ab_0    conda-forge\n",
      "pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge\n",
      "pyqt                      5.15.7          py310h29803b5_0    conda-forge\n",
      "pyqt5-sip                 12.11.0         py310hd8f1fbe_0    conda-forge\n",
      "pyrsistent                0.19.2          py310h5764c6d_0    conda-forge\n",
      "pysocks                   1.7.1              pyha2e5f31_6    conda-forge\n",
      "pytest                    7.2.1                    pypi_0    pypi\n",
      "python                    3.10.8          h257c98d_0_cpython    conda-forge\n",
      "python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\n",
      "python-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge\n",
      "python-flatbuffers        1.12               pyhd8ed1ab_1    conda-forge\n",
      "python_abi                3.10                    2_cp310    conda-forge\n",
      "pytorch                   1.12.1          cuda102py310hdf4a2db_200    conda-forge\n",
      "pytz                      2022.6             pyhd8ed1ab_0    conda-forge\n",
      "pyu2f                     0.1.5              pyhd8ed1ab_0    conda-forge\n",
      "pywavelets                1.4.1           py310h0a54255_0    conda-forge\n",
      "pyyaml                    6.0             py310h5764c6d_5    conda-forge\n",
      "pyzmq                     24.0.1          py310h330234f_1    conda-forge\n",
      "qt-main                   5.15.4               ha5833f6_2    conda-forge\n",
      "qt-webengine              5.15.4               hcbadb6c_3    conda-forge\n",
      "qtconsole                 5.4.0              pyhd8ed1ab_0    conda-forge\n",
      "qtconsole-base            5.4.0              pyha770c72_0    conda-forge\n",
      "qtpy                      2.3.0              pyhd8ed1ab_0    conda-forge\n",
      "qtwebkit                  5.212                h3383a02_6    conda-forge\n",
      "r                         4.2             r42hd8ed1ab_1006    conda-forge\n",
      "r-base                    4.2.1                ha8c3e7c_1    conda-forge\n",
      "r-boot                    1.3_28.1          r42hc72bb7e_0    conda-forge\n",
      "r-class                   7.3_20            r42h06615bd_1    conda-forge\n",
      "r-cluster                 2.1.4             r42h8da6f51_0    conda-forge\n",
      "r-codetools               0.2_18            r42hc72bb7e_1    conda-forge\n",
      "r-foreign                 0.8_83            r42h06615bd_1    conda-forge\n",
      "r-kernsmooth              2.23_20           r42hd009a43_1    conda-forge\n",
      "r-lattice                 0.20_45           r42h06615bd_1    conda-forge\n",
      "r-mass                    7.3_58.1          r42h06615bd_1    conda-forge\n",
      "r-matrix                  1.5_3             r42h5f7b363_0    conda-forge\n",
      "r-mgcv                    1.8_41            r42h5f7b363_0    conda-forge\n",
      "r-nlme                    3.1_160           r42h8da6f51_0    conda-forge\n",
      "r-nnet                    7.3_18            r42h06615bd_1    conda-forge\n",
      "r-recommended             4.2             r42hd8ed1ab_1005    conda-forge\n",
      "r-rpart                   4.1.19            r42h06615bd_0    conda-forge\n",
      "r-spatial                 7.3_15            r42h06615bd_1    conda-forge\n",
      "r-survival                3.4_0             r42h06615bd_1    conda-forge\n",
      "re2                       2022.06.01           h27087fc_1    conda-forge\n",
      "readline                  8.1.2                h0f457ee_0    conda-forge\n",
      "requests                  2.28.1             pyhd8ed1ab_1    conda-forge\n",
      "requests-oauthlib         1.3.1              pyhd8ed1ab_0    conda-forge\n",
      "rsa                       4.9                pyhd8ed1ab_0    conda-forge\n",
      "scikit-image              0.19.3          py310h769672d_2    conda-forge\n",
      "scikit-learn              1.1.3           py310h0c3af53_1    conda-forge\n",
      "scipy                     1.9.3           py310hdfbd76f_2    conda-forge\n",
      "seaborn                   0.12.1               hd8ed1ab_0    conda-forge\n",
      "seaborn-base              0.12.1             pyhd8ed1ab_0    conda-forge\n",
      "sed                       4.8                  he412f7d_0    conda-forge\n",
      "send2trash                1.8.0              pyhd8ed1ab_0    conda-forge\n",
      "setuptools                65.5.1             pyhd8ed1ab_0    conda-forge\n",
      "shap                      0.39.0          py310hb5077e9_1    conda-forge\n",
      "sip                       6.6.2           py310hd8f1fbe_0    conda-forge\n",
      "six                       1.16.0             pyh6c4a22f_0    conda-forge\n",
      "sleef                     3.5.1                h9b69904_2    conda-forge\n",
      "slicer                    0.0.7              pyhd8ed1ab_0    conda-forge\n",
      "snappy                    1.1.9                hbd366e4_2    conda-forge\n",
      "sniffio                   1.3.0              pyhd8ed1ab_0    conda-forge\n",
      "soupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge\n",
      "sqlite                    3.40.0               h4ff8645_0    conda-forge\n",
      "stack_data                0.6.2              pyhd8ed1ab_0    conda-forge\n",
      "statsmodels               0.13.5          py310hde88566_2    conda-forge\n",
      "stopit                    1.1.2                      py_0    conda-forge\n",
      "svt-av1                   1.3.0                h27087fc_0    conda-forge\n",
      "sysroot_linux-64          2.12                he073ed8_15    conda-forge\n",
      "tbb                       2021.7.0             h924138e_0    conda-forge\n",
      "tensorboard               2.9.0              pyhd8ed1ab_0    conda-forge\n",
      "tensorboard-data-server   0.6.1           py310h597c629_4    conda-forge\n",
      "tensorboard-plugin-wit    1.8.1              pyhd8ed1ab_0    conda-forge\n",
      "tensorflow                2.9.1           cuda102py310hcf4adbc_0    conda-forge\n",
      "tensorflow-base           2.9.1           cuda102py310h282d6da_0    conda-forge\n",
      "tensorflow-estimator      2.9.1           cuda102py310hac962ef_0    conda-forge\n",
      "termcolor                 2.2.0              pyhd8ed1ab_0    conda-forge\n",
      "terminado                 0.17.0             pyh41d4057_0    conda-forge\n",
      "threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge\n",
      "tifffile                  2022.10.10         pyhd8ed1ab_0    conda-forge\n",
      "tinycss2                  1.2.1              pyhd8ed1ab_0    conda-forge\n",
      "tk                        8.6.12               h27826a3_0    conda-forge\n",
      "tktable                   2.10                 hb7b940f_3    conda-forge\n",
      "toml                      0.10.2             pyhd8ed1ab_0    conda-forge\n",
      "tomli                     2.0.1                    pypi_0    pypi\n",
      "toolz                     0.12.0             pyhd8ed1ab_0    conda-forge\n",
      "torchvision               0.13.0          cuda102py310h4532de0_0    conda-forge\n",
      "tornado                   6.2             py310h5764c6d_1    conda-forge\n",
      "tpot                      0.11.7             pyhd8ed1ab_1    conda-forge\n",
      "tqdm                      4.64.1             pyhd8ed1ab_0    conda-forge\n",
      "traitlets                 5.6.0              pyhd8ed1ab_0    conda-forge\n",
      "typing-extensions         4.4.0                hd8ed1ab_0    conda-forge\n",
      "typing_extensions         4.4.0              pyha770c72_0    conda-forge\n",
      "tzdata                    2022g                h191b570_0    conda-forge\n",
      "unicodedata2              15.0.0          py310h5764c6d_0    conda-forge\n",
      "update_checker            0.18.0             pyh9f0ad1d_0    conda-forge\n",
      "urllib3                   1.26.13            pyhd8ed1ab_0    conda-forge\n",
      "wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\n",
      "webencodings              0.5.1                      py_1    conda-forge\n",
      "websocket-client          1.4.2              pyhd8ed1ab_0    conda-forge\n",
      "werkzeug                  2.2.3              pyhd8ed1ab_0    conda-forge\n",
      "wheel                     0.38.4             pyhd8ed1ab_0    conda-forge\n",
      "widgetsnbextension        4.0.3              pyhd8ed1ab_0    conda-forge\n",
      "wrapt                     1.14.1          py310h5764c6d_1    conda-forge\n",
      "x264                      1!164.3095           h166bdaf_2    conda-forge\n",
      "x265                      3.5                  h924138e_3    conda-forge\n",
      "xarray                    2023.2.0                 pypi_0    pypi\n",
      "xcb-util                  0.4.0                h166bdaf_0    conda-forge\n",
      "xcb-util-image            0.4.0                h166bdaf_0    conda-forge\n",
      "xcb-util-keysyms          0.4.0                h166bdaf_0    conda-forge\n",
      "xcb-util-renderutil       0.3.9                h166bdaf_0    conda-forge\n",
      "xcb-util-wm               0.4.1                h166bdaf_0    conda-forge\n",
      "xorg-fixesproto           5.0               h7f98852_1002    conda-forge\n",
      "xorg-inputproto           2.3.2             h7f98852_1002    conda-forge\n",
      "xorg-kbproto              1.0.7             h7f98852_1002    conda-forge\n",
      "xorg-libice               1.0.10               h7f98852_0    conda-forge\n",
      "xorg-libsm                1.2.3             hd9c2040_1000    conda-forge\n",
      "xorg-libx11               1.7.2                h7f98852_0    conda-forge\n",
      "xorg-libxau               1.0.9                h7f98852_0    conda-forge\n",
      "xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge\n",
      "xorg-libxext              1.3.4                h7f98852_1    conda-forge\n",
      "xorg-libxfixes            5.0.3             h7f98852_1004    conda-forge\n",
      "xorg-libxi                1.7.10               h7f98852_0    conda-forge\n",
      "xorg-libxrender           0.9.10            h7f98852_1003    conda-forge\n",
      "xorg-libxt                1.2.1                h7f98852_2    conda-forge\n",
      "xorg-renderproto          0.11.1            h7f98852_1002    conda-forge\n",
      "xorg-xextproto            7.3.0             h7f98852_1002    conda-forge\n",
      "xorg-xproto               7.0.31            h7f98852_1007    conda-forge\n",
      "xz                        5.2.6                h166bdaf_0    conda-forge\n",
      "yaml                      0.2.5                h7f98852_2    conda-forge\n",
      "yarl                      1.8.2           py310h5764c6d_0    conda-forge\n",
      "zeromq                    4.3.4                h9c3ff4c_1    conda-forge\n",
      "zfp                       0.5.5                h9c3ff4c_8    conda-forge\n",
      "zipp                      3.11.0             pyhd8ed1ab_0    conda-forge\n",
      "zlib                      1.2.13               h166bdaf_4    conda-forge\n",
      "zlib-ng                   2.0.6                h166bdaf_0    conda-forge\n",
      "zstandard                 0.19.0                   pypi_0    pypi\n",
      "zstd                      1.5.2                h6239696_4    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Final_Samples_4yrs.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m Final_Samples_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal_Samples_4yrs.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m Final_Samples \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFinal_Samples_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \n\u001b[1;32m      3\u001b[0m usable_samples_ADNI \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./usable_samples_ADNI.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m PRS_orig_feature_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PRS_feature_matrix_only_ad.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/anaconda3/envs/ad_venv_2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Final_Samples_4yrs.json'"
     ]
    }
   ],
   "source": [
    "Final_Samples_path = 'Final_Samples_4yrs.json'\n",
    "Final_Samples = json.load(open(Final_Samples_path, 'r')) \n",
    "usable_samples_ADNI = json.load(open('./usable_samples_ADNI.json'))\n",
    "PRS_orig_feature_matrix = np.load('./PRS_feature_matrix_only_ad.npy').astype(np.float32)\n",
    "# normalize feature matrix\n",
    "PRS_orig_feature_matrix = (PRS_orig_feature_matrix - PRS_orig_feature_matrix.mean(0))/PRS_orig_feature_matrix.std(0)\n",
    "# PRS_orig_feature_matrix.shape[1], len(usable_samples_ADNI), usable_samples_ADNI\n",
    "print(PRS_orig_feature_matrix.shape)\n",
    "# num_features=PRS_orig_feature_matrix.shape[1]\n",
    "num_features = 1\n",
    "print(len( usable_samples_ADNI ) )\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covar_df = pd.read_csv('./COVAR_FILE_bigger_dataset.txt', ' ') \n",
    "# covar_df = pd.read_csv('./COVAR_FILE_bigger_dataset.txt', ' ') \n",
    "# print(\"shape\",covar_df.shape)\n",
    "# print( covar_df[['AGE', 'PTGENDER']].shape, covar_df[['AGE', 'PTGENDER']].dropna().shape ) \n",
    "# # PC - Principal Component\n",
    "\n",
    "# # trying to normalize AGE with having max age of 100\n",
    "# covar_df['AGE'] = covar_df['AGE'] / 100.0\n",
    "# print( covar_df.head() )\n",
    "\n",
    "# need to collect from dataset_1225_basic.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter parameters :\n",
    "    1. Number of features\n",
    "    2. Number of Hidden Layers \n",
    "    3. Dimension of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1\n",
    "hidden = 4\n",
    "hidden_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first num_features column from PRS_feature_matrix\n",
    "PRS_feature_matrix = PRS_orig_feature_matrix\n",
    "# PRS_feature_matrix = PRS_feature_matrix[:, :num_features]\n",
    "print(PRS_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Samples has two columns of data :\n",
    "    1. ID \n",
    "    2. output - true / false\n",
    "    \n",
    "Get the length of positive and negative samples of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive samples - output true\n",
    "# negative samples - output false\n",
    "len_positive_samples = 0\n",
    "len_negative_samples = 0\n",
    "for x in Final_Samples:\n",
    "    if x[1] == 1 :\n",
    "        len_positive_samples += 1\n",
    "    else :\n",
    "        len_negative_samples += 1\n",
    "        \n",
    "print(len(Final_Samples))\n",
    "print(len_positive_samples)\n",
    "print(len_negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining covar data with PRS Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRS_feature_matrix = np.reshape(PRS_feature_matrix, (-1, 1))\n",
    "print( PRS_feature_matrix.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnt = number of missing IDs for which covar data doesn't exist\n",
    "cnt = 0\n",
    "print(\"Before adding covar_df columns, shape : \",PRS_feature_matrix.shape)\n",
    "# adding ( total columns - 2 ) of covar_df , excluding FID, IID\n",
    "FEATURE_MATRIX = np.concatenate([PRS_feature_matrix, np.zeros([PRS_feature_matrix.shape[0], covar_df.shape[1] - 2 ])], 1).astype(np.float32)\n",
    "print(\"Before adding covar_df columns, shape : \",FEATURE_MATRIX.shape)\n",
    "for sample in usable_samples_ADNI:\n",
    "    # taking from the PCs, skipping the first two columns of IID, FID\n",
    "    covar = covar_df[covar_df['IID'] == sample].to_numpy()[:, 2:].astype(np.float32) \n",
    "    # shape[0] = 1 means a row is found in covar for the following sample ID\n",
    "    # if not, that means no covar data exists for the sample in usable_samples_ADNI\n",
    "    if covar.shape[0] != 1:\n",
    "#         print(covar.shape)\n",
    "        print(sample)\n",
    "        cnt += 1\n",
    "        continue\n",
    "    # Adding the covar values to the feature matrix\n",
    "    FEATURE_MATRIX[usable_samples_ADNI[sample], num_features:] = covar\n",
    "\n",
    "\n",
    "print(\"Count of missing samples for covar data : \", cnt)\n",
    "#     FEATURE_MATRIX[usable_samples_ADNI[sample], num_features:] = covar # naeem's modification\n",
    "# cnt/FEATURE_MATRIX.shape[0], FEATURE_MATRIX[:2], PRS_feature_matrix[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directory for saving shap figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./shap/\" + str(num_features)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the index with the Age = 0\n",
    "\n",
    "age is zero for the rows that the covar data was not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_zero = 0\n",
    "age_zero_idx = []\n",
    "for i in range( len(FEATURE_MATRIX) ):\n",
    "    if FEATURE_MATRIX[i, -1] == 0.00:\n",
    "        age_zero += 1\n",
    "        age_zero_idx.append(i)\n",
    "        \n",
    "print(age_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding info of CDRSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adnimerge = pd.read_csv(\"../larger_dataset/ADNIMERGE.csv\")\n",
    "# adnimerge = adnimerge[['PTID', 'CDRSB', 'Years_bl']]\n",
    "# print( adnimerge['CDRSB'].value_counts() )\n",
    "# adnimerge.loc[ adnimerge['PTID'] == '116_S_4167' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adnimerge = pd.read_csv(\"../larger_dataset/ADNIMERGE.csv\")\n",
    "# adnimerge = adnimerge[['PTID', 'CDRSB', 'Years_bl']]\n",
    "# # adnimerge.sort_values(by = ['PTID', 'Years_bl'], ascending = [True, True], na_position = 'first')\n",
    "# # print(adnimerge.head(20))\n",
    "# # print( adnimerge.loc[ adnimerge['PTID'] == '116_S_4167' ] )\n",
    "# # adnimerge.drop_duplicates(subset = ['PTID'], keep = 'first', inplace = True) \n",
    "# adnimerge = adnimerge.loc[adnimerge['Years_bl'] == 0]\n",
    "# # print( adnimerge['CDRSB'].value_counts() )\n",
    "\n",
    "# print( adnimerge.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adnimerge.loc[ adnimerge['PTID'] == '116_S_4167' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW_FEATURE_MATRIX = np.concatenate([FEATURE_MATRIX, np.zeros([PRS_feature_matrix.shape[0], 1 ])], 1).astype(np.float32)\n",
    "\n",
    "# for sample in usable_samples_ADNI:\n",
    "#     PTID = \"_\".join( sample.split(\"_\")[1:] )\n",
    "# #     print(PTID)\n",
    "#     cdrsb = adnimerge.loc[adnimerge['PTID'] == PTID]['CDRSB']\n",
    "# #     year = adnimerge.loc[adnimerge['PTID'] == PTID]['Years_bl']\n",
    "# #     print( PTID, cdrsb.values[0], year.values[0] )\n",
    "#     NEW_FEATURE_MATRIX[usable_samples_ADNI[sample], -1] = cdrsb.values[0]\n",
    "    \n",
    "# print(NEW_FEATURE_MATRIX)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE_MATRIX = NEW_FEATURE_MATRIX\n",
    "# FEATURE_MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indices of features to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# feature_indices_to_consider = list(range(23))#list(range(35)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "# naeem modified\n",
    "print(FEATURE_MATRIX.shape[1])\n",
    "last_idx = FEATURE_MATRIX.shape[1] - 1\n",
    "# feature_indices_to_consider = list(range(num_features))  + [last_idx - 1, last_idx] #list(range(35)) #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 20, 22]\n",
    "# remove_indices = [1, 2, 3]\n",
    "# for i in remove_indices:\n",
    "#     feature_indices_to_consider.remove(i)\n",
    "\n",
    "# feature_indices_to_consider = list(range(23, 36))\n",
    "\n",
    "# feature_indices_to_consider = [ 4, 11, 14, 21, 23, 26, 32, 34, 46]\n",
    "\n",
    "# feature_indices_to_consider = [ 9, 10, 11, 14, 21, 23, 26, 28, 32, 34, 46]\n",
    "# feature_indices_to_consider = [9, 10, 28, 34, 46]\n",
    "feature_indices_to_consider = list( range(last_idx + 1) )  \n",
    "print(feature_indices_to_consider)\n",
    "# feature_indices_to_consider = [1, 2, 3, 11, 14, 21, 23, 26, 32, 45]\n",
    "# feature_indices_to_consider = [2, 26, 32, 45]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_in_file: saves model accuracy in a text file\n",
    "#     args : model_name : name of model with layers and dimensions\n",
    "#            accuracy : accuracy  score\n",
    "def save_in_file(model_name, accuracy):\n",
    "    model_file = open(\"model_details.txt\",\"a\")\n",
    "    model_file.write(model_name + \" -> accuracy : \" + str(accuracy) + \"\\n\" )\n",
    "    model_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications**\n",
    "1. Added relu in the hidden layers and sigmoid in the output layer as activation functions\n",
    "2. Added dropout in the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_features=FEATURE_MATRIX.shape[1], hidden_dim= hidden_dimension, drop_probab=.5):\n",
    "        super(simple_model, self).__init__()\n",
    "        \n",
    "        ####\n",
    "        num_hidden = hidden\n",
    "        hidden_dim = hidden_dimension\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.fc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, 8)\n",
    "        self.outLayer = nn.Linear(8, 1)\n",
    "#         self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.drop_probab = drop_probab\n",
    "        self.dropout = nn.functional.dropout\n",
    "        ####\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        for i in range(self.num_hidden):\n",
    "            features = self.fc_hidden[i](features)\n",
    "            # added by Mashiat\n",
    "            features = self.dropout(features, p=self.drop_probab)\n",
    "            features = self.relu( features )\n",
    "            ####################\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features, p=self.drop_probab)\n",
    "        logit = self.outLayer(features)\n",
    "#         print(features.shape, features)\n",
    "        probab = self.sigmoid(logit)\n",
    "        return probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model(num_features=len(feature_indices_to_consider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataSet class \n",
    "combines usable_samples_ADNI, Final_Samples, feature_matrix to one dataset with features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataSet(data.Dataset):\n",
    "    def __init__(self, Final_Samples, feature_matrix, usable_samples_ADNI, feature_indices_to_consider=feature_indices_to_consider):\n",
    "        super(dataSet, self).__init__()  \n",
    "        self.data_len = len(Final_Samples)\n",
    "        self.usable_samples_ADNI = usable_samples_ADNI\n",
    "        self.Final_Samples = Final_Samples\n",
    "        self.feature_indices_to_consider = feature_indices_to_consider\n",
    "        self.feature_matrix = feature_matrix[:, self.feature_indices_to_consider]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        features = torch.from_numpy(self.feature_matrix[self.usable_samples_ADNI[self.Final_Samples[index][0]]]).float()\n",
    "        label = torch.tensor([float(self.Final_Samples[index][1])]).float()\n",
    "        return features, label\n",
    "    \n",
    "    def update_prs_features(self, mean, std):\n",
    "        self.feature_matrix = (self.feature_matrix - mean) / std\n",
    "        \n",
    "    def get_mean_std(self):\n",
    "        mean = self.feature_matrix.mean(0)\n",
    "        std = self.feature_matrix.std(0)\n",
    "        return mean, std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Pandas Dataframe to Dataset class\n",
    "\n",
    "overriding the constructor, getitem, len function of the original class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class df_dataSet(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.features = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y.values, dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# random samples : sample test dataset taking equal number of positive & negative outputs\n",
    "#                 args : \n",
    "#                         total_folds : fold number for splitting\n",
    "#                         random_seed : seed value for randomization\n",
    "#                 return :\n",
    "#                         N_splits : test dataset splitted according to fold numbers\n",
    "                        \n",
    "def random_samples(total_folds, random_seed=None):\n",
    "    Final_Samples = json.load(open(Final_Samples_path, 'r')) \n",
    "    positive_samples = Final_Samples[:len_positive_samples]\n",
    "    negative_samples = Final_Samples[len_positive_samples:]\n",
    "    min_len = min( len(positive_samples), len(negative_samples))\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 2)\n",
    "    random.shuffle(positive_samples)\n",
    "    random.shuffle(negative_samples)\n",
    "    print(\"min length : \", min_len)\n",
    "    Final_Samples = positive_samples[:min_len] + negative_samples[:min_len]\n",
    "    random.shuffle(Final_Samples)\n",
    "    print(len([x[1] for x in Final_Samples if x[1] == 1]), len([x[1] for x in Final_Samples if x[1] == 0]))\n",
    "    Final_Samples = np.array(Final_Samples)\n",
    "\n",
    "#   -----------------------------------------------------------------------\n",
    "#     positive_samples = Final_Samples[:654]\n",
    "#     negative_samples = Final_Samples[654:]\n",
    "#     if random_seed is not None: \n",
    "#         random.seed(random_seed * 2)\n",
    "#     random.shuffle(positive_samples)\n",
    "#     random.shuffle(negative_samples)\n",
    "#     Final_Samples = positive_samples[:500] + negative_samples[:500]\n",
    "#     if random_seed is not None: \n",
    "#         random.seed(random_seed)\n",
    "#     random.shuffle(Final_Samples)\n",
    "#     Final_Samples = np.array(Final_Samples)\n",
    "# --------------------------------------------------------------------------\n",
    "    N_splits = Final_Samples.reshape(total_folds, -1, 2)\n",
    "    return N_splits\n",
    "\n",
    "# generate_datasets : get train, validation & test datasets\n",
    "#                 args : \n",
    "#                         N_splits : data splitted according to folds; output of random samples\n",
    "#                         fold_num : fold_num for test dataset\n",
    "#                         random_seed : seed value for randomization\n",
    "#                 return :\n",
    "#                         train_set, test_set, val_set : datasets\n",
    "def generate_datasets(N_splits, fold_num, random_seed):\n",
    "    test_samples = N_splits[fold_num:fold_num+1].reshape([-1, 2])\n",
    "    train_samples = np.concatenate([N_splits[0:fold_num],N_splits[fold_num+1:]], 0).reshape([-1, 2]).tolist()\n",
    "    if random_seed is not None: \n",
    "        random.seed(random_seed * 3)\n",
    "    random.shuffle(train_samples)\n",
    "    train_samples = np.array(train_samples)\n",
    "    # take all as training dataset, leaves nothing for validation - multiply shpae by 1\n",
    "    split_pos = int(train_samples.shape[0] * 1.) \n",
    "    #split_pos = int(train_samples.shape[0] * .8) \n",
    "#     print(train_samples.shape, split_pos, train_samples.shape[0])\n",
    "    train_samples, val_samples = train_samples[:split_pos], train_samples[split_pos:]\n",
    "    train_set = dataSet(Final_Samples=train_samples, \n",
    "                        feature_matrix=FEATURE_MATRIX, \n",
    "                        usable_samples_ADNI=usable_samples_ADNI)\n",
    "    val_set = dataSet(Final_Samples=val_samples, \n",
    "                      feature_matrix=FEATURE_MATRIX, \n",
    "                      usable_samples_ADNI=usable_samples_ADNI)\n",
    "    test_set = dataSet(Final_Samples=test_samples, \n",
    "                      feature_matrix=FEATURE_MATRIX, \n",
    "                      usable_samples_ADNI=usable_samples_ADNI)\n",
    "    mean, std = train_set.get_mean_std()\n",
    "    # normalize dataset\n",
    "    train_set.update_prs_features(mean, std)\n",
    "    val_set.update_prs_features(mean, std)\n",
    "    test_set.update_prs_features(mean, std)\n",
    "#     print(len(train_set))\n",
    "#     print(len(val_set))\n",
    "#     print(len(test_set))\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "\n",
    "# generate_loader : get train, validation & test as torch dataset\n",
    "#                 args : \n",
    "#                         train_set, test_set, val_set : datasets\n",
    "#                 returns :\n",
    "#                         train, val & test torch datasets\n",
    "def generate_loader(train_set, val_set, test_set, num_workers):\n",
    "    train_batch_size = train_set.__len__()\n",
    "    val_batch_size = val_set.__len__()\n",
    "    test_batch_size = test_set.__len__()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                              batch_size=train_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              batch_size=test_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()),\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# train_set, val_set, test_set = generate_datasets(N_splits=random_samples(total_folds=10, random_seed=0), fold_num=0, random_seed=0)\n",
    "# val_set.feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch function : runs an epoch of a model\n",
    "#                 args :\n",
    "#                         model : neural network model\n",
    "#                         optimizer :\n",
    "#                         criterion :\n",
    "#                         is_training : train - true or test - false\n",
    "#                         loader : torch dataset\n",
    "#                 returns :\n",
    "#                         different accuracy score for the dataset of per epoch\n",
    "def epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "#     print(loader)\n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "        label = torch.reshape(label, (label.shape[0], 1))\n",
    "        probab = model(features)\n",
    "        if is_training:  \n",
    "#             print(probab.shape, label.shape)\n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in range(len(pred_binary)):\n",
    "        if pred_binary[i]==1:\n",
    "            if true[i]==1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        else :\n",
    "            if true[i]==0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "    \n",
    "    return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true,tp,tn,fp,fn\n",
    "#     return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**epoch function for LOOCV**\n",
    "\n",
    "Without precision, recall, ROC, AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_epoch(model, optimizer, criterion, is_training, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0.\n",
    "#     print(loader)\n",
    "    for batch_idx, (features, label) in enumerate(loader):\n",
    "        features = torch.autograd.Variable(features.to(DEVICE).float())\n",
    "        label = torch.autograd.Variable(label.to(DEVICE).float())\n",
    "        label = torch.reshape(label, (label.shape[0], 1))\n",
    "        probab = model(features)\n",
    "        if is_training:  \n",
    "#             print(probab.shape, label.shape)\n",
    "            loss = criterion(probab, label)\n",
    "            ## compute gradient and do SGD step \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#             print(batch_idx, ':', loss) \n",
    "        pred += probab.detach().cpu().numpy().tolist()\n",
    "        true += label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    pred, true, total_loss = np.array(pred).reshape([-1]), np.array(true).reshape([-1]), total_loss\n",
    "    pred_binary = (pred > .5).astype(float)\n",
    "#     precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "#     auroc = roc_auc_score(true, pred)\n",
    "#     p, r, thresholds = precision_recall_curve(true, pred)\n",
    "#     auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "    \n",
    "#     return precision[1], recall[1], fscore[1], support, auroc, auprc, acc, total_loss, pred, pred_binary, true\n",
    "    return None, None, None, None, None, None, acc, total_loss, pred, pred_binary, true\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usable_samples_ADNI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable_indices : take indices from usable_samples_ADNI which are in Final_Samples\n",
    "# usable_features : take the part of feature matrix with usable_indices( rows )\n",
    "#                   and feature_indices_to_consider ( columns )\n",
    "\n",
    "print(len(Final_Samples))\n",
    "usable_indices = [( usable_samples_ADNI[Final_Samples[i][0]] if ( Final_Samples[i][0] in usable_samples_ADNI.keys() ) else None ) for i in range(len(Final_Samples))]\n",
    "print(len(usable_indices))\n",
    "# print(usable_indices)\n",
    "usable_features = FEATURE_MATRIX[usable_indices][:, feature_indices_to_consider]\n",
    "print(\"Shape of usable features : \", usable_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing age values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing age with value 0\n",
    "#--------------------------------------------------------------------------------\n",
    "age_zero = 0\n",
    "age_zero_idx = []\n",
    "for i in range(len(usable_features)):\n",
    "    if usable_features[i, -1] == 0.00:\n",
    "        age_zero += 1\n",
    "        age_zero_idx.append(i)\n",
    "print(len(age_zero_idx))\n",
    "print(usable_features.shape, len( Final_Samples ) )\n",
    "usable_features = np.delete(usable_features, age_zero_idx, axis = 0)\n",
    "Final_Samples = np.delete(Final_Samples, age_zero_idx, axis = 0)\n",
    "print(usable_features.shape, len( Final_Samples ) )\n",
    "print(Final_Samples[0])\n",
    "#--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usable_labels : output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_labels = np.array([float(Final_Samples[i][1]) for i in range(len(Final_Samples))])\n",
    "print(\"Length of usable labels : \", len(usable_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all traits from json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traits = json.load(open('traits_map.json', 'r'))\n",
    "# print(all_traits)\n",
    "GWAS_IDS = list(all_traits)\n",
    "# print(GWAS_IDS)\n",
    "traits = [all_traits[x] for x in all_traits]\n",
    "print(traits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get traits corresponding to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(PRS_orig_feature_matrix.shape[1])\n",
    "# # print(feature_indices_to_consider)\n",
    "\n",
    "# print(\"traits length: \", len(traits))\n",
    "# print(feature_indices_to_consider)\n",
    "# print( len( feature_indices_to_consider ) )\n",
    "\n",
    "# # check if age and gender is included\n",
    "# # if they are included, their index will be larger than the original feature matrix\n",
    "# if any(y >PRS_orig_feature_matrix.shape[1] for y in feature_indices_to_consider):\n",
    "#     features = feature_indices_to_consider.copy()\n",
    "#     age_include = False\n",
    "#     gender_include = False\n",
    "#     print(features[-2])\n",
    "#     if(features[-2] == PRS_orig_feature_matrix.shape[1] + 10):\n",
    "#         print(\"gender_include\")\n",
    "#         features.pop(-2)\n",
    "#         gender_include = True\n",
    "#     print(features[-1])\n",
    "#     if(features[-1] == PRS_orig_feature_matrix.shape[1] + 11):\n",
    "#         print(\"age include\")\n",
    "#         features.pop(-1)\n",
    "#         age_include = True\n",
    "    \n",
    "#     print(features)\n",
    "#     traits = [ traits[i] for i in features]\n",
    "#     if gender_include == True:\n",
    "#         traits.append(\"gender\")\n",
    "#     if age_include == True:\n",
    "#         traits.append(\"age\")\n",
    "    \n",
    "# #     traits.append(\"output prediction\")\n",
    "# #     traits.append(\"age\")\n",
    "# #     traits.append(\"gender\")\n",
    "# else:\n",
    "#     # not included - age, gender\n",
    "#     traits = [ traits[i] for i in feature_indices_to_consider]\n",
    "# print(traits)\n",
    "traits = ['AD']\n",
    "for i in range(1, 11):\n",
    "    traits.append(\"PC\" + str(i) )\n",
    "traits.append(\"gender\")\n",
    "traits.append(\"age\")\n",
    "print( traits )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda dataframe conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = np.c_[ usable_features, usable_labels ]\n",
    "# print(total_features.shape)\n",
    "# print(num_features)\n",
    "# column_list = list(range(num_features)) + [last_idx - 1, last_idx - 2, num_features ] \n",
    "column_list = feature_indices_to_consider + [num_features]\n",
    "# print( column_list )\n",
    "\n",
    "df = pd.DataFrame(total_features, columns = column_list )\n",
    "\n",
    "# turn column names to strings\n",
    "# df.columns = df.columns.astype(str)\n",
    "\n",
    "# assign traits as column names\n",
    "column_names = traits.copy()\n",
    "column_names.append('output')\n",
    "df.columns = column_names\n",
    "# print( df.columns )\n",
    "\n",
    "# dropping last / output column in df\n",
    "df_X = df.iloc[: , :-1]\n",
    "# taking the output column of df\n",
    "df_Y = df.iloc[: , -1]\n",
    "\n",
    "print( df.head() )\n",
    "# print( df_X.head() )\n",
    "# print( df_Y.head() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get equal amount of positive & negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df['output'].value_counts() )\n",
    "\n",
    "# ones = df[df['output'] == 1]\n",
    "# zeros = df[df['output'] == 0]\n",
    "# min_len = min( len(ones), len(zeros) ) \n",
    "\n",
    "# ones = ones.iloc[:min_len, :]\n",
    "# zeros = zeros.iloc[:min_len, :]\n",
    "\n",
    "# df = ones.append(zeros, ignore_index=True)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Analysis to oversample data\n",
    "\n",
    "https://towardsdatascience.com/smote-synthetic-data-augmentation-for-tabular-data-1ce28090debc#:~:text=SMOTE%20is%20an%20over%2Dsampling,its%20%E2%80%9Ck%E2%80%9D%20nearest%20neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smote(x, y):\n",
    "    # Synthetic Minority Over-samping Technique\n",
    "    # \n",
    "    # sampling_strategy: determines the portion of samples to \n",
    "    #                    generate with respect to the majority class\n",
    "    # k_neighbors : number of neighbors to be considered for each sample\n",
    "    \n",
    "    # For this example, only 1% of minoirty samples are considered\n",
    "    k_neighbors = math.ceil(sum(y) * 0.01)\n",
    "      \n",
    "    smote = SMOTE(sampling_strategy=1, \n",
    "                  k_neighbors=k_neighbors)\n",
    "    x, y = smote.fit_resample(x, y)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "def bordersmote(x, y):\n",
    "    # Borderline-SMOTE\n",
    "    # \n",
    "    # sampling_strategy: determines the portion of samples to \n",
    "    #                    generate with respect to the majority class\n",
    "    # k_neighbors : number of neighbors to be considered for each sample\n",
    "    # m_neighbors : number of neighbors to consider to determine if a sample is danger\n",
    "    \n",
    "    # For this example, only 1% of minoirty samples are considered\n",
    "    k_neighbors = math.ceil(sum(y) * 0.01)\n",
    "    m_neighbors = math.ceil(sum(y) * 0.01)\n",
    "    \n",
    "    bordersmote = BorderlineSMOTE(sampling_strategy=1, \n",
    "                                  k_neighbors=k_neighbors, \n",
    "                                  m_neighbors=m_neighbors)\n",
    "    \n",
    "    x, y = bordersmote.fit_resample(x, y)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "def adasyn(x, y):\n",
    "    # Adaptive Synthetic\n",
    "    # \n",
    "    # sampling_strategy: determines the portion of samples to \n",
    "    #                    generate with respect to the majority class\n",
    "    # n_neighbors : number of neighbors to be considered for each sample\n",
    "    \n",
    "    # For this example, only 1% of minoirty samples are considered\n",
    "    n_neighbors = math.ceil(sum(y) * 0.01)\n",
    "    \n",
    "    adasyn = ADASYN(sampling_strategy=1,\n",
    "                   n_neighbors=n_neighbors)\n",
    "    x, y = adasyn.fit_resample(x, y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropping last / output column in df\n",
    "# shuffled_X = shuffled.iloc[: , :-1]\n",
    "# # taking the output column of df\n",
    "# shuffled_Y = shuffled.iloc[: , -1]\n",
    "# print(df.shape)\n",
    "# df_X_new, df_Y_new = bordersmote(df_X, df_Y)\n",
    "# print( df_X_new.shape, df_Y_new.shape )\n",
    "\n",
    "# df = df_X_new\n",
    "# df['output'] = df_Y_new\n",
    "# print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df['output'].value_counts() )\n",
    "print(df.head())\n",
    "\n",
    "# ones = df[df['output'] == 1]\n",
    "# zeros = df[df['output'] == 0]\n",
    "# min_len = min( len(ones), len(zeros) ) \n",
    "\n",
    "# ones = ones.iloc[:min_len, :]\n",
    "# zeros = zeros.iloc[:min_len, :]\n",
    "\n",
    "# df = ones.append(zeros, ignore_index=True)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling dataframe\n",
    "\n",
    "**Dropping alcohol recommended columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # keep the index same\n",
    "# shuffled = df.sample(frac=1)\n",
    "# print( shuffled.head() )\n",
    "\n",
    "# reset the index\n",
    "shuffled = df.sample(frac=1, random_state = 1).reset_index()\n",
    "# print(shuffled.columns)\n",
    "shuffled = shuffled.drop( ['index'], axis = 1 )\n",
    "\n",
    "\n",
    "# #  dropping alcohol recommended columns\n",
    "# shuffled = shuffled.drop(['Ever had known person concerned about, or recommend reduction of, alcohol consumption: No', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, but not in the last year', 'Ever had known person concerned about, or recommend reduction of, alcohol consumption: Yes, during the last year'], axis = 1)\n",
    "# shuffled = shuffled.drop(['Non-cancer illness code, self-reported: anxiety/panic attacks', 'Sleeplessness / insomnia', 'Non-cancer illness code, self-reported: type 2 diabetes', \"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\", \"Alcohol intake frequency\", 'Other meat intake' ], axis = 1)\n",
    "\n",
    "print( shuffled.head() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_features = torch.autograd.Variable(torch.from_numpy(usable_features)).to(DEVICE).float()\n",
    "\n",
    "print(\"Usable Features : \", usable_features[:2])\n",
    "\n",
    "print(usable_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Illness of Father & Mother having Alzheimer\n",
    "\n",
    "Previously data showed a negative correlation with both of these factors, which shouldn't be. We will now try to combine both risk scores by averaging it and use is as a new feature, descarding the previous two. This new feature will represent the risk score of a parent having Alzheimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled['Parent having Alzheimer'] = shuffled[[\"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\"]].max(axis=1)\n",
    "# shuffled[[\"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\", \"Parent having Alzheimer\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled = shuffled.drop(['Illnesses of mother: Alzheimer\\'s disease/dementia', 'Illnesses of father: Alzheimer\\'s disease/dementia'], axis = 1 )\n",
    "# print( shuffled.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index=shuffled.columns.get_loc(\"output\")\n",
    "# temp_cols = shuffled.columns.tolist()\n",
    "# print(index)\n",
    "# new_cols= temp_cols[0 : index] + temp_cols[index+1 : ] + temp_cols[index : index + 1 ]\n",
    "# shuffled = shuffled[new_cols]\n",
    "# print( shuffled.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X, Y differentiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping last / output column in df\n",
    "shuffled_X = shuffled.iloc[: , :-1]\n",
    "# taking the output column of df\n",
    "shuffled_Y = shuffled.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.head())\n",
    "column_list = shuffled.columns\n",
    "for i in column_list:\n",
    "    print(i)\n",
    "    shuffled[i].plot(kind=\"hist\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check distribution for positive and negative output with parents' Alzheimer PRS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_check = [\"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\"]\n",
    "# shuffled_neg = shuffled[shuffled['output'] == 0]\n",
    "\n",
    "\n",
    "# print('negative PRS values for negative output among ', shuffled_neg.shape[0], \" samples\")\n",
    "# print( shuffled_neg[cols_to_check].lt(0).sum() )\n",
    "\n",
    "# for i in cols_to_check:\n",
    "#     print(i)\n",
    "#     shuffled_neg[i].plot(kind=\"hist\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_check = [\"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\"]\n",
    "# shuffled_pos = shuffled[shuffled['output'] == 1]\n",
    "\n",
    "\n",
    "# print('negative PRS values for positive output among ', shuffled_pos.shape[0], \" samples\")\n",
    "# print( shuffled_pos[cols_to_check].lt(0).sum() )\n",
    "\n",
    "# for i in cols_to_check:\n",
    "#     print(i)\n",
    "#     shuffled_pos[i].plot(kind=\"hist\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter methods\n",
    "Filter methods pick up the intrinsic properties of the features measured via univariate statistics instead of cross-validation performance. These methods are faster and less computationally expensive than wrapper methods. When dealing with high-dimensional data, it is computationally cheaper to use filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain**\n",
    "\n",
    "Information gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from matplotlib.pyplot import figure\n",
    "\n",
    "# figure(figsize=(20, 20), dpi=100)\n",
    "\n",
    "# importances = mutual_info_classif(shuffled_X, shuffled_Y)\n",
    "# feat_importances = pd.Series(importances, shuffled.columns[ 0:len(shuffled.columns) - 1 ] )\n",
    "# important_features = []\n",
    "# count = 0\n",
    "# for elem in feat_importances:\n",
    "#     if elem > 0.05:\n",
    "#         important_features.append(shuffled.columns[count])\n",
    "#     count += 1\n",
    "# print(important_features)\n",
    "\n",
    "# selected_features += important_features\n",
    "# # print(selected_features)\n",
    "\n",
    "# feat_importances.plot( kind = 'barh', color = 'teal' )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Coefficient**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20,20))         # Sample figsize in inches\n",
    "# corr = shuffled.corr()\n",
    "# sns.heatmap( corr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Still Alzheimer of parents show negative correlation:**\n",
    "    1. Lack of Diagnosis for parents.\n",
    "    2. Less patients overall earlier.\n",
    "    3. Lack of such patients in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corr.iloc[0][0])\n",
    "# features_selected = []\n",
    "# column_len = len( shuffled.columns )\n",
    "# output_idx = column_len - 1\n",
    "\n",
    "# for i in range( column_len ): \n",
    "#     if (corr.iloc[i][output_idx] > 0.1 ) and i != output_idx :\n",
    "#         features_selected.append(shuffled.columns[i])\n",
    "# print(features_selected)\n",
    "\n",
    "# selected_features += features_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Threshold**\n",
    "\n",
    "The variance threshold is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples. We assume that features with a higher variance may contain more useful information, but note that we are not taking the relationship between feature variables or feature and target variables into account, which is one of the drawbacks of filter methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# # remove features with variance less than 0.1\n",
    "# v_threshold = VarianceThreshold( threshold = 0.1 )\n",
    "# v_threshold.fit(shuffled_X)\n",
    "# print( v_threshold.get_support() )\n",
    "# rejected_col = [x for x, y in zip(shuffled_X.columns, v_threshold.get_support() ) if y == False]\n",
    "# print(rejected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense as most of the age is very high for AD patients. No need to reject the \"age\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Difference (MAD)**\n",
    "\n",
    "The mean absolute difference (MAD) computes the absolute difference from the mean value. The main difference between the variance and MAD measures is the absence of the square in the latter. The MAD, like the variance, is also a scale variant.’ This means that higher the MAD, higher the discriminatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_abs_diff = np.sum( np.abs( shuffled_X - np.mean(shuffled_X, axis = 0 ) ), axis = 0 ) / shuffled_X.shape[0]\n",
    "\n",
    "# plt.bar( np.arange( shuffled_X.shape[1] ), mean_abs_diff, color = 'teal' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Methods\n",
    "\n",
    "Wrappers require some method to search the space of all possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. The feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The wrapper methods usually result in better predictive accuracy than filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Feature Selection**\n",
    "\n",
    "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html\n",
    "\n",
    "This is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rfc = RandomForestClassifier()\n",
    "# rfc.fit(shuffled_X, shuffled_Y)\n",
    "\n",
    "# ffs = SequentialFeatureSelector( rfc, k_features = 'best', forward = True, n_jobs = -1 )\n",
    "# ffs.fit(shuffled_X, shuffled_Y)\n",
    "# features = list( ffs.k_feature_names_ )\n",
    "# print(features)\n",
    "# print( len( features ) )\n",
    "\n",
    "# selected_features += features\n",
    "# # print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Feature Elimination**\n",
    "\n",
    "This method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we remove the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# lr = LogisticRegression( class_weight = 'balanced', solver = 'lbfgs', random_state = 42, n_jobs = -1, max_iter = 500 )\n",
    "# lr.fit(shuffled_X, shuffled_Y)\n",
    "\n",
    "# bfs = SequentialFeatureSelector( lr, k_features = 'best', forward = False, n_jobs = -1 )\n",
    "# bfs.fit(shuffled_X, shuffled_Y)\n",
    "# features = list( bfs.k_feature_names_ )\n",
    "# print(features)\n",
    "# print( len( features ) )\n",
    "\n",
    "# selected_features += features\n",
    "# # print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exhaustive Feature Selection**\n",
    "\n",
    "This is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# efs = ExhaustiveFeatureSelector( RandomForestClassifier(), min_features = 8, max_features = 10, scoring = 'roc_auc', cv = 2)\n",
    "# efs.fit(shuffled_X, shuffled_Y)\n",
    "\n",
    "# selected_features = shuffled_X.columns[list(efs.best_idx_)]\n",
    "# print(selected_features)\n",
    "\n",
    "# print(efs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination**\n",
    "\n",
    "‘Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute.\n",
    "\n",
    "Then, the least important features are pruned from the current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# # here lr denotes the logistic regression model trained on backward elimination\n",
    "# rfe = RFE(lr, n_features_to_select = 10)\n",
    "# rfe.fit(shuffled_X, shuffled_Y)\n",
    "# selected_col = [ x for x, y in zip(shuffled_X.columns, rfe.support_ )  if y == True ]\n",
    "# print(selected_col)\n",
    "# print( len (selected_col ) )\n",
    "# print(rfe.ranking_)\n",
    "\n",
    "# selected_features += selected_col \n",
    "# # print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Methods\n",
    "\n",
    "These methods encompass the benefits of both the wrapper and filter methods, by including interactions of features but also maintaining reasonable computational cost. Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extracts those features which contribute the most to the training for a particular iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO Regularization (L1)**\n",
    "\n",
    "Regularization consists of adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model, i.e. to avoid over-fitting. In linear model regularization, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularization, Lasso or L1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# logistic = LogisticRegression( C = 1, penalty = \"l1\", solver = 'liblinear', random_state = 7 ).fit(shuffled_X, shuffled_Y )\n",
    "\n",
    "# model = SelectFromModel(logistic, prefit = True )\n",
    "\n",
    "# X_new = model.transform(shuffled_X)\n",
    "\n",
    "# selected_feat = shuffled_X.columns[(model.get_support())]\n",
    "# print(selected_feat)\n",
    "# print( len( selected_feat ) )\n",
    "# # selected_columns = shuffled_X.columns[X_new.var() != 0 ]\n",
    "# # print(selected_columns)\n",
    "# # print( len(selected_columns[0] ) )\n",
    "# # print(type(selected_feat))\n",
    "# # print( selected_feat.tolist() )\n",
    "# selected_features += selected_feat.tolist()\n",
    "\n",
    "# # print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Importance**\n",
    "\n",
    "Random Forests is a kind of a Bagging Algorithm that aggregates a specified number of decision trees. The tree-based strategies used by random forests naturally rank by how well they improve the purity of the node, or in other words a decrease in the impurity (Gini impurity) over all trees. Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = RandomForestClassifier( n_estimators = 340 )\n",
    "\n",
    "# model.fit(shuffled_X, shuffled_Y)\n",
    "\n",
    "# importances = model.feature_importances_\n",
    "\n",
    "# final_df = pd.DataFrame({\"Features\" : pd.DataFrame(shuffled_X).columns, \"Importances\" : importances } )\n",
    "# final_df.set_index('Importances')\n",
    "\n",
    "# final_df = final_df.sort_values('Importances')\n",
    "# final_df.plot.bar(color = 'teal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(type( final_df ) )\n",
    "# # print(final_df.columns)\n",
    "# # print(final_df['Features'].iloc[-10:])\n",
    "# important_cols = []\n",
    "# important_cols = final_df['Features'].iloc[-10:].values.tolist()\n",
    "# print(important_cols)\n",
    "\n",
    "# selected_features += important_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features based on all methods\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/python-counter-python-collections-counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # print(selected_features)\n",
    "# c = Counter( selected_features )\n",
    "# # print( c )\n",
    "# # print( c.most_common(20) )\n",
    "# most_common = c.most_common(15)\n",
    "# print( most_common )\n",
    "# selected_col = list( list(zip(*most_common))[0] )\n",
    "# print(selected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop extra features from Shuffled Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review select features\n",
    "\n",
    "Features related to : \n",
    "**Alcohol Consumption,\n",
    "Hearing Problem,\n",
    "Smoking / Cigarettes,\n",
    "Cholesterol,\n",
    "Blood Pressure,\n",
    "Meat,\n",
    "Depression,\n",
    "Insomnia /  Sleep Schedule,\n",
    "Education,\n",
    "Hypertension,\n",
    "Physical Inactivity,\n",
    "Brain Injury,\n",
    "Father / Mother,\n",
    "Obesity,\n",
    "Diabetes,\n",
    "Age**\n",
    "\n",
    "\n",
    "'Non-cancer illness code, self-reported: type 2 diabetes'\n",
    "'Total cholesterol', 'HDL cholesterol', 'LDL cholesterol'\n",
    "'Cigarettes per Day', 'systolic blood pressure', 'diastolic blood pressure'\n",
    "'Hearing difficulty/problems: Yes', 'Non-cancer illness code, self-reported: depression’\n",
    "'Hearing difficulty/problems with background noise'\n",
    "'Sleeplessness / insomnia', 'Sleep duration', 'Age completed full time education'\n",
    "'Types of physical activity in last 4 weeks: Strenuous sports', 'Other meat intake', 'Loneliness, isolation', \"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\", 'Mood swings'\n",
    "'Non-cancer illness code, self-reported: hypertension'\n",
    "'Diagnoses - secondary ICD10: I10 Essential (primary) hypertension'\n",
    "'Non-cancer illness code, self-reported: head injury', 'Alcohol intake frequency', 'Diagnoses - secondary ICD10: E66.9 Obesity, unspecified’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_col = ['Non-cancer illness code, self-reported: type 2 diabetes', 'Total cholesterol', 'HDL cholesterol', 'LDL cholesterol', \n",
    "# 'Cigarettes per Day', 'systolic blood pressure', 'diastolic blood pressure',\n",
    "# 'Hearing difficulty/problems: Yes', 'Non-cancer illness code, self-reported: depression',\n",
    "# 'Hearing difficulty/problems with background noise',\n",
    "# 'Sleeplessness / insomnia', 'Sleep duration', 'Age completed full time education',\n",
    "# 'Types of physical activity in last 4 weeks: Strenuous sports', 'Other meat intake', 'Loneliness, isolation', \"Illnesses of father: Alzheimer's disease/dementia\", \"Illnesses of mother: Alzheimer's disease/dementia\", 'Mood swings',\n",
    "# 'Non-cancer illness code, self-reported: hypertension',\n",
    "# 'Diagnoses - secondary ICD10: I10 Essential (primary) hypertension',\n",
    "# 'Non-cancer illness code, self-reported: head injury', 'Alcohol intake frequency', 'Diagnoses - secondary ICD10: E66.9 Obesity, unspecified', 'age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add AD PRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shuffled.columns)\n",
    "# selected_col += [\"Alzheimer's disease\"]\n",
    "# print(selected_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_col += ['output']\n",
    "# shuffled = shuffled[selected_col]\n",
    "\n",
    "# print( shuffled.shape )\n",
    "# print( shuffled.head() )\n",
    "\n",
    "# # dropping last / output column in df\n",
    "# shuffled_X = shuffled.iloc[: , :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier with LOOCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import LeaveOneOut\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # print(shuffled_X.shape)\n",
    "# # create loocv procedure\n",
    "# cv = LeaveOneOut()\n",
    "# # enumerate splits\n",
    "# y_true, y_pred = list(), list()\n",
    "# for train_ix, test_ix in cv.split(shuffled_X):\n",
    "#     # split data\n",
    "# #     print(train_ix, test_ix)\n",
    "#     X_train, X_test = shuffled_X.iloc[train_ix], shuffled_X.iloc[test_ix]\n",
    "#     y_train, y_test = shuffled_Y.iloc[train_ix], shuffled_Y.iloc[test_ix]\n",
    "#     # fit model\n",
    "#     model = RandomForestClassifier(random_state=1)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     # evaluate model\n",
    "#     yhat = model.predict(X_test)\n",
    "#     # store\n",
    "# #     print(y_test[0], yhat[0])\n",
    "# #  print(len( y_test ), len( yhat ) )\n",
    "#     y_true.append(y_test)\n",
    "#     y_pred.append(yhat)\n",
    "# # calculate accuracy\n",
    "# acc = accuracy_score(y_true, y_pred)\n",
    "# print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Model with LOOCV**\n",
    "\n",
    "Epoch number is reduced to 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import LeaveOneOut\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # print(shuffled_X.shape)\n",
    "\n",
    "# avg_val_acc = []\n",
    "# accuracies = []\n",
    "# accuracies_val = []\n",
    "# acc_score = []\n",
    "# total_epochs = 250 #250(ideal)\n",
    "\n",
    "\n",
    "# # create loocv procedure\n",
    "# cv = LeaveOneOut()\n",
    "# # enumerate splits\n",
    "# y_true, y_pred = list(), list()\n",
    "# for train_ix, test_ix in cv.split(shuffled_X):\n",
    "#     # split data\n",
    "# #     print(train_ix, test_ix)\n",
    "#     X_train, X_test = shuffled_X.iloc[train_ix], shuffled_X.iloc[test_ix]\n",
    "#     y_train, y_test = shuffled_Y.iloc[train_ix], shuffled_Y.iloc[test_ix]\n",
    "    \n",
    "#     train_dataset = df_dataSet( X_train, y_train )\n",
    "#     valid_dataset = df_dataSet( X_test, y_test )\n",
    "                \n",
    "#     train_batch_size = train_dataset.__len__()\n",
    "#     val_batch_size = valid_dataset.__len__()\n",
    "                \n",
    "\n",
    "                \n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, num_workers = 0)\n",
    "#     valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = val_batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "#     model = simple_model(num_features = shuffled_X.shape[1], hidden_dim = hidden_dimension)\n",
    "#     model = model.to(DEVICE)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = torch.nn.BCEWithLogitsLoss() \n",
    "#     best_acc_val = 0.\n",
    "#     model_best = None\n",
    "                \n",
    "#     for epoch_num in range(total_epochs):\n",
    "# #       print(epoch_num)\n",
    "#         model.train()\n",
    "# #       model.drop_probab=.8\n",
    "# #       print(\"model trained\")\n",
    "#         precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = loocv_epoch(model=model, optimizer=optimizer, \n",
    "#                                                                                  criterion=criterion, is_training=True, \n",
    "#                                                                                loader=train_loader)\n",
    "# #                     print(\"model validated\")\n",
    "#         model.eval()\n",
    "# #       model.drop_probab=.0\n",
    "#         precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = loocv_epoch(model=model, \n",
    "#                                                                                  optimizer=optimizer, \n",
    "#                                                                                  criterion=criterion, is_training=False, \n",
    "#                                                                                 loader=valid_loader)\n",
    "# #       print(\"model kahini done\")\n",
    "#         if acc_val >= best_acc_val:\n",
    "#             best_acc_val = acc_val\n",
    "#             model_best = model.to(DEVICE)\n",
    "\n",
    "#     model_best.eval()\n",
    "#     precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true = loocv_epoch(model=model_best, \n",
    "#                                                                              optimizer=optimizer, \n",
    "#                                                                              criterion=criterion, is_training=False, \n",
    "#                                                                              loader=valid_loader)\n",
    "#     accuracies += [acc_test]\n",
    "#     accuracies_val += [best_acc_val]\n",
    "    \n",
    "#     if test_ix % 50 == 0:\n",
    "#         print(test_ix, ':', acc_test)\n",
    "   \n",
    "\n",
    "\n",
    "   # print( 'Accuracy', np.sum( accuracies ) / len( accuracies ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_test_file(X_train, y_train, X_test, y_test, loop):\n",
    "#     print( X_train.head(), type(X_train), y_train.head(), type(y_train) )\n",
    "    train_df = X_train.join( y_train )\n",
    "#     print( train_df.columns, train_df.shape )\n",
    "#     print( train_df.head() )\n",
    "    test_df = X_test.join( y_test )\n",
    "    train_df.to_csv( \"fold_dataset/train_\" + str( loop ) + \".csv\" )\n",
    "    test_df.to_csv( \"fold_dataset/test_\" + str( loop ) + \".csv\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/09/a-comprehensive-guide-on-neural-networks-performance-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor_X = torch.tensor(shuffled_X.values, dtype=torch.float32).to(DEVICE)\n",
    "print( tensor_X[0].shape )\n",
    "\n",
    "GENERATE_SHAP = True\n",
    "total_epochs = 500 #250(ideal)\n",
    "num_features_list = [shuffled_X.shape[1]]\n",
    "# random_integers = [2, 6, 108, 90, 5]\n",
    "random_integers = [90]#, 92, 0, 87, 73, 82, 54]\n",
    "\n",
    "folds_list = [10]#[37*2]\n",
    "\n",
    "avg_val_acc = []\n",
    "\n",
    "shap_values_list = []\n",
    "for num_features in num_features_list:\n",
    "    print(f'NF:{num_features}')\n",
    "    global_best_acc_val = 0.\n",
    "    precision_avg = 0\n",
    "    recall_avg = 0\n",
    "    auprc_avg = 0\n",
    "    auroc_avg = 0\n",
    "    fscore_avg = 0\n",
    "    for total_folds in folds_list:\n",
    "        print(f'\\n#F{total_folds}')\n",
    "        for random_seed in random_integers:\n",
    "            accuracies = []\n",
    "            accuracies_val = []\n",
    "            temp_shap_values = np.zeros(shuffled_X.shape)\n",
    "            \n",
    "            kf = KFold(n_splits = total_folds, random_state = None)\n",
    "            acc_score = []\n",
    "            loop = 0\n",
    "            for train_index , test_index in kf.split(shuffled):\n",
    "                X_train , X_test = shuffled_X.iloc[train_index,:], shuffled_X.iloc[test_index,:]\n",
    "                y_train , y_test = shuffled_Y[train_index] , shuffled_Y[test_index]\n",
    "                \n",
    "                X_train, y_train = bordersmote( X_train, y_train )\n",
    "                \n",
    "                                               \n",
    "                save_train_test_file(X_train, y_train, X_test, y_test, loop)                    \n",
    "                loop += 1\n",
    "                \n",
    "                print(\"train size: \", X_train.shape, y_train.shape)\n",
    "                print(\"test size: \", X_test.shape, y_test.shape)\n",
    "                \n",
    "                train_dataset = df_dataSet( X_train, y_train )\n",
    "                valid_dataset = df_dataSet( X_test, y_test )\n",
    "                \n",
    "                train_batch_size = train_dataset.__len__()\n",
    "                val_batch_size = valid_dataset.__len__()\n",
    "                \n",
    "#                 print( train_batch_size, val_batch_size )\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "#                 train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, num_workers = 0)\n",
    "                valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = val_batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "                model = simple_model(num_features = shuffled_X.shape[1], hidden_dim = hidden_dimension)\n",
    "                model = model.to(DEVICE)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                criterion = torch.nn.BCEWithLogitsLoss() \n",
    "                best_acc_val = 0.\n",
    "                model_best = None\n",
    "                \n",
    "                for epoch_num in range(total_epochs):\n",
    "#                     print(epoch_num)\n",
    "                    model.train()\n",
    "#                     model.drop_probab=.8\n",
    "#                     print(\"model trained\")\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true,tp,tn,fp,fn = epoch(model=model, optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=True, \n",
    "                                                                                           loader=train_loader)\n",
    "#                     print(\"model validated\")\n",
    "                    model.eval()\n",
    "#                     model.drop_probab=.0\n",
    "                    precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true,tp,tn,fp,fn  = epoch(model=model, \n",
    "                                                                                             optimizer=optimizer, \n",
    "                                                                                             criterion=criterion, is_training=False, \n",
    "                                                                                            loader=valid_loader)\n",
    "#                     print(\"model kahini done\")\n",
    "                    \n",
    "                    if acc_val > best_acc_val:\n",
    "                        best_acc_val = acc_val\n",
    "                        if acc_val > global_best_acc_val:\n",
    "                            global_best_acc_val = acc_val\n",
    "    #                         print('global updated!')\n",
    "                        torch.save(model.state_dict(), 'PRS_model.pt')\n",
    "    #                     print(f'#F:{total_folds}| seed:{random_seed}, fold:{fold_num}, epoch:{epoch_num} -> local:{best_acc_val}, global:{global_best_acc_val}')  \n",
    "#                     if epoch_num + 1 == total_epochs:\n",
    "#     #                     print(f'LAST_Epoch:{epoch_num}, train_acc:{acc_train}, val_acc:{acc_val}, local_best:{best_acc_val}, global_best:{global_best_acc_val}')\n",
    "#                         pass\n",
    "                model_best = simple_model(num_features= shuffled_X.shape[1], hidden_dim = hidden_dimension, drop_probab=.0)\n",
    "                model_best.load_state_dict(torch.load('PRS_model.pt'))\n",
    "                model_best = model_best.to(DEVICE)\n",
    "                model_best.eval()\n",
    "                precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true,tp,tn,fp,fn  = epoch(model=model_best, \n",
    "                                                                                         optimizer=optimizer, \n",
    "                                                                                         criterion=criterion, is_training=False, \n",
    "                                                                                         loader=valid_loader)\n",
    "                accuracies += [acc_test]\n",
    "                accuracies_val += [best_acc_val]\n",
    "#                 print(\"precision : \", precision, \" ; recall : \", recall)\n",
    "                precision_avg += precision\n",
    "                recall_avg += recall\n",
    "                auprc_avg += auprc\n",
    "                auroc_avg += auroc\n",
    "                fscore_avg += fscore\n",
    "                print(\"true positive:\",tp)\n",
    "                print(\"true negative:\",tn)\n",
    "                print(\"false positive:\",fp)\n",
    "                print(\"false negative:\",fn)\n",
    "                \n",
    "#                 print(precision, recall, fscore, support, auroc, auprc, acc_test, total_loss)\n",
    "#                 print(\"pred\")\n",
    "#                 print(pred)\n",
    "#                 print(\"pred binary\")\n",
    "#                 print(type(pred_binary))\n",
    "#                 print(pred_binary)\n",
    "                \n",
    "                print(total_folds, ':')\n",
    "                if GENERATE_SHAP:\n",
    "                    explainer = shap.GradientExplainer(model_best.to(DEVICE), tensor_X,\n",
    "                                                       batch_size=shuffled_X.shape[0]) #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "#                     print(\"usable features : \", usable_features.shape[0])\n",
    "#                     print(usable_features.shape)\n",
    "#                     print(usable_features)\n",
    "                    shap_values = explainer.shap_values(tensor_X, nsamples=500)\n",
    "#                     print(\"shap values shape : \", shap_values.shape)\n",
    "#                     print(\"Shap values : \", shap_values)\n",
    "#                     print(\"shap values of 0 index\", shap_values[0, :])\n",
    "                    \n",
    "                    temp_shap_values += shap_values \n",
    "            if GENERATE_SHAP:\n",
    "                temp_shap_values /= total_folds\n",
    "                shap_values_list += [temp_shap_values] \n",
    "            print(f'random_seed:{random_seed}:', np.mean(accuracies), np.std(accuracies), \n",
    "                  np.mean(accuracies_val), np.std(accuracies_val), 'train acc:', acc_train)\n",
    "            avg_val_acc += [np.mean(accuracies_val)]\n",
    "    print(\"accuraacies of validation: \", accuracies_val)\n",
    "    print(\"average val accuracy: \", avg_val_acc)\n",
    "    print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "    precision_avg = precision_avg * 1.0 / total_folds\n",
    "    recall_avg = recall_avg * 1.0 / total_folds\n",
    "    auprc_avg = auprc_avg * 1.0 / total_folds\n",
    "    auroc_avg = auroc_avg * 1.0 / total_folds\n",
    "    fscore_avg = fscore_avg * 1.0 / total_folds\n",
    "    print( \"precision avg : \", precision_avg )\n",
    "    print( \"recall avg : \", recall_avg )\n",
    "    print( \"AUPRC avg : \", auprc_avg )\n",
    "    print( \"AUROC avg : \", auroc_avg )\n",
    "    print( \"FScore avg : \", fscore_avg )\n",
    "# usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n",
    "avg_val_acc = np.array(avg_val_acc)\n",
    "print(\"average val accuracy: \", avg_val_acc.max(), avg_val_acc.min(), avg_val_acc.mean(), avg_val_acc.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GENERATE_SHAP = True\n",
    "# total_epochs = 500 #250(ideal)\n",
    "# num_features_list = [usable_features.shape[1]]\n",
    "# # random_integers = [2, 6, 108, 90, 5]\n",
    "# random_integers = [90]#, 92, 0, 87, 73, 82, 54]\n",
    "\n",
    "# folds_list = [10]#[37*2]\n",
    "\n",
    "# avg_val_acc = []\n",
    "\n",
    "# shap_values_list = []\n",
    "# for num_features in num_features_list:\n",
    "#     print(f'NF:{num_features}')\n",
    "#     global_best_acc_val = 0.\n",
    "#     precision_avg = 0\n",
    "#     recall_avg = 0\n",
    "#     auprc_avg = 0\n",
    "#     auroc_avg = 0\n",
    "#     fscore_avg = 0\n",
    "#     for total_folds in folds_list:\n",
    "#         print(f'\\n#F{total_folds}')\n",
    "#         for random_seed in random_integers:\n",
    "#             N_splits = random_samples(total_folds=total_folds, random_seed=random_seed)\n",
    "#             accuracies = []\n",
    "#             accuracies_val = []\n",
    "#             temp_shap_values = np.zeros(usable_features.shape)\n",
    "#             for fold_num in tqdm(range(total_folds)):\n",
    "#     #             print(f'fold-{fold_num}:')\n",
    "# #                 train_set, val_set, test_set = generate_datasets(N_splits=N_splits, fold_num=fold_num, random_seed=random_seed)\n",
    "#                 train_set, _, test_set = generate_datasets(N_splits=N_splits, fold_num=fold_num, random_seed=random_seed)\n",
    "#                 val_set = test_set        \n",
    "#                 train_loader, val_loader, test_loader = generate_loader(train_set=train_set, val_set=val_set, \n",
    "#                                                                         test_set=test_set, num_workers=0)\n",
    "#                 model = simple_model(num_features=usable_features.shape[1], hidden_dim= hidden_dimension)\n",
    "#                 model = model.to(DEVICE)\n",
    "#                 optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#                 criterion = torch.nn.BCEWithLogitsLoss() \n",
    "#                 best_acc_val = 0.\n",
    "#                 model_best = None\n",
    "#                 for epoch_num in range(total_epochs):\n",
    "#                     model.train()\n",
    "# #                     model.drop_probab=.8\n",
    "#                     precision, recall, fscore, support, auroc, auprc, acc_train, total_loss, pred, pred_binary, true = epoch(model=model, optimizer=optimizer, \n",
    "#                                                                                              criterion=criterion, is_training=True, \n",
    "#                                                                                              loader=train_loader)\n",
    "#                     model.eval()\n",
    "# #                     model.drop_probab=.0\n",
    "#                     precision, recall, fscore, support, auroc, auprc, acc_val, total_loss, pred, pred_binary, true = epoch(model=model, \n",
    "#                                                                                              optimizer=optimizer, \n",
    "#                                                                                              criterion=criterion, is_training=False, \n",
    "#                                                                                             loader=val_loader)\n",
    "#                     if acc_val > best_acc_val:\n",
    "#                         best_acc_val = acc_val\n",
    "#                         if acc_val > global_best_acc_val:\n",
    "#                             global_best_acc_val = acc_val\n",
    "#     #                         print('global updated!')\n",
    "#                         torch.save(model.state_dict(), 'PRS_model.pt')\n",
    "#     #                     print(f'#F:{total_folds}| seed:{random_seed}, fold:{fold_num}, epoch:{epoch_num} -> local:{best_acc_val}, global:{global_best_acc_val}')  \n",
    "# #                     if epoch_num + 1 == total_epochs:\n",
    "# #     #                     print(f'LAST_Epoch:{epoch_num}, train_acc:{acc_train}, val_acc:{acc_val}, local_best:{best_acc_val}, global_best:{global_best_acc_val}')\n",
    "# #                         pass\n",
    "#                 model_best = simple_model(num_features=usable_features.shape[1], hidden_dim= hidden_dimension, drop_probab=.0)\n",
    "#                 model_best.load_state_dict(torch.load('PRS_model.pt'))\n",
    "#                 model_best = model_best.to(DEVICE)\n",
    "#                 model_best.eval()\n",
    "#                 precision, recall, fscore, support, auroc, auprc, acc_test, total_loss, pred, pred_binary, true = epoch(model=model_best, \n",
    "#                                                                                          optimizer=optimizer, \n",
    "#                                                                                          criterion=criterion, is_training=False, \n",
    "#                                                                                          loader=val_loader)\n",
    "#                 accuracies += [acc_test]\n",
    "#                 accuracies_val += [best_acc_val]\n",
    "# #                 print(\"precision : \", precision, \" ; recall : \", recall)\n",
    "#                 precision_avg += precision\n",
    "#                 recall_avg += recall\n",
    "#                 auprc_avg += auprc\n",
    "#                 auroc_avg += auroc\n",
    "#                 fscore_avg += fscore\n",
    "                \n",
    "# #                 print(precision, recall, fscore, support, auroc, auprc, acc_test, total_loss)\n",
    "# #                 print(\"pred\")\n",
    "# #                 print(pred)\n",
    "# #                 print(\"pred binary\")\n",
    "# #                 print(type(pred_binary))\n",
    "# #                 print(pred_binary)\n",
    "                \n",
    "# #                 print(fold_num, ':', accuracies)\n",
    "#                 if GENERATE_SHAP:\n",
    "#                     explainer = shap.GradientExplainer(model_best.to(DEVICE), usable_features,\n",
    "#                                                        batch_size=usable_features.shape[0]) #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "# #                     print(\"usable features : \", usable_features.shape[0])\n",
    "# #                     print(usable_features.shape)\n",
    "# #                     print(usable_features)\n",
    "#                     shap_values = explainer.shap_values(usable_features, nsamples=500)\n",
    "# #                     print(\"shap values shape : \", shap_values.shape)\n",
    "# #                     print(\"Shap values : \", shap_values)\n",
    "# #                     print(\"shap values of 0 index\", shap_values[0, :])\n",
    "                    \n",
    "#                     temp_shap_values += shap_values \n",
    "#             if GENERATE_SHAP:\n",
    "#                 temp_shap_values /= total_folds\n",
    "#                 shap_values_list += [temp_shap_values] \n",
    "#             print(f'random_seed:{random_seed}:', np.mean(accuracies), np.std(accuracies), \n",
    "#                   np.mean(accuracies_val), np.std(accuracies_val), 'train acc:', acc_train)\n",
    "#             avg_val_acc += [np.mean(accuracies_val)]\n",
    "    \n",
    "#     print(f'global_best_acc_val:{global_best_acc_val}')\n",
    "#     precision_avg = precision_avg * 1.0 / total_folds\n",
    "#     recall_avg = recall_avg * 1.0 / total_folds\n",
    "#     auprc_avg = auprc_avg * 1.0 / total_folds\n",
    "#     auroc_avg = auroc_avg * 1.0 / total_folds\n",
    "#     fscore_avg = fscore_avg * 1.0 / total_folds\n",
    "#     print( \"precision avg : \", precision_avg )\n",
    "#     print( \"recall avg : \", recall_avg )\n",
    "#     print( \"AUPRC avg : \", auprc_avg )\n",
    "#     print( \"AUROC avg : \", auroc_avg )\n",
    "#     print( \"FScore avg : \", fscore_avg )\n",
    "# # usable_features = usable_features.cpu().detach().numpy().astype(np.float64)\n",
    "# avg_val_acc = np.array(avg_val_acc)\n",
    "# print(avg_val_acc.max(), avg_val_acc.min(), avg_val_acc.mean(), avg_val_acc.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model accuracy in model_details.txt\n",
    "save_in_file(\"Neural Network with \" + str(hidden)  + \" layers_4yrs_\" + str(len(feature_indices_to_consider)), global_best_acc_val)\n",
    "# save_in_file(\"Neural Network with \" + str(hidden) + \" layers\", global_best_acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_features = usable_features.cpu().detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap Analysis\n",
    "\n",
    "https://medium.com/dataman-in-ai/explain-your-model-with-the-shap-values-bc36aac4de3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( shap_values_list )\n",
    "# write shap_values_list to pkl file\n",
    "pickle.dump(shap_values_list, open('shap_values_list.pkl', 'wb'))\n",
    "shap_values = np.mean(shap_values_list, axis=0)\n",
    "print( shap_values.shape )\n",
    "# print(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force plot :\n",
    "\n",
    "\n",
    "The shap.force_plot() takes three values: \n",
    "\n",
    "(i) the base value (explainerModel.expected_value[0]),\n",
    "\n",
    "(ii) the SHAP values (shap_values_Model[j][0]) and \n",
    "\n",
    "(iii) the matrix of feature values (S.iloc[[j]]). The base value or the expected value is the average of the model output over the training data X_train. It is the base value used in the following plot.\n",
    "\n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195\n",
    "\n",
    "the bold 0.80 is the model’s score for this observation. Higher scores lead the model to predict 1 and lower scores lead the model to predict 0. The features that were important to making the prediction for this observation are shown in red and blue, with red representing features that pushed the model score higher, and blue representing features that pushed the score lower. Features that had more of an impact on the score are located closer to the dividing boundary between red and blue, and the size of that impact is represented by the size of the bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print( shap_values_list )\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "\n",
    "# print(shap_values[0, :])\n",
    "# print(usable_features[0, :])\n",
    "# shap.summary_plot(shap_values[:, :], usable_features[:, :])\n",
    "\n",
    "# shap.force_plot(.5, shap_values[0,:], usable_features[0, :], link = \"logit\", matplotlib = True)  \n",
    "\n",
    "# using pandas dataframe\n",
    "shap.force_plot(.5, shap_values[0,:], shuffled_X.iloc[0, :], link = \"logit\", matplotlib = True  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shap.force_plot(.5, shap_values[:,:], usable_features[:, :], link=\"logit\")\n",
    "\n",
    "# using pandas dataframe\n",
    "print(shap_values.shape)\n",
    "print(shuffled_X.shape)\n",
    "shap.force_plot(.5, shap_values[:,:], shuffled_X.iloc[:, :], link=\"logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Plot\n",
    "This plot is made of all the dots in the train data. It delivers the following information:\n",
    "\n",
    "Feature importance: Variables are ranked in descending order.\n",
    "\n",
    "Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
    "\n",
    "Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n",
    "\n",
    "Correlation: A high level of the “alcohol” content has a high and positive impact on the quality rating. The “high” comes from the red color, and the “positive” impact is shown on the X-axis. Similarly, we will say the “volatile acidity” is negatively correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./shap/\" + str(num_features)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# for trait in traits:\n",
    "#     print(trait)\n",
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "# usable_features_std = (usable_features - usable_features.mean(0))/usable_features.std(0)\n",
    "# shap.summary_plot(shap_values, features=usable_features, feature_names=traits, plot_type='dot', max_display=len(traits), show = False)\n",
    "# plt.savefig('shap/' + str(num_features) + '/summary_plot_hidden_'+ str(hidden) + '_dim_' + str(hidden_dimension) + '.pdf',  bbox_inches='tight')\n",
    "# shap.summary_plot(shap_values, features=usable_features, feature_names=list(range(23)), plot_type='dot', max_display=len(traits))\n",
    "\n",
    "shap.summary_plot(shap_values, features = shuffled_X, feature_names = shuffled_X.columns, plot_type='dot', max_display=len(traits), show = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html?highlight=beeswarm#shap.summary_plot\n",
    "# shap.summary_plot(shap_values, features=usable_features, feature_names=list(range(23)), plot_type='bar', max_display=len(traits), show=False)\n",
    "# plt.savefig('shap/summary_plot_hidden_'+ str(hidden) + '_bar.png', bbox_inches='tight')\n",
    "# naeem modified\n",
    "# shap.summary_plot(shap_values, features=usable_features, feature_names=list(range(num_features)), plot_type='bar', max_display=len(traits), show=False)\n",
    "# shap.summary_plot(shap_values, features=usable_features, feature_names = traits, plot_type='bar', max_display=len(traits), show=False)\n",
    "# plt.savefig('shap/' + str(num_features) + 'summary_plot_hidden_'+ str(hidden) + '_bar.png', bbox_inches='tight')\n",
    "\n",
    "shap.summary_plot(shap_values, features = shuffled_X, feature_names = shuffled_X.columns, plot_type='bar', max_display=len(traits), show=False)\n",
    "plt.savefig('shap/' + str(num_features) + 'summary_plot_hidden_'+ str(hidden) + '_bar.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip(list(range(23)), abs(shap_values).mean(0))), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(shap.force_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision, recall, fscore, auroc, auprc,accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScores(true,pred_binary,pred):\n",
    "    print(true.shape,pred_binary.shape,pred.shape)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(true, pred_binary)\n",
    "    auroc = roc_auc_score(true, pred)\n",
    "    p, r, thresholds = precision_recall_curve(true, pred)\n",
    "    auprc = auc(r, p)\n",
    "    acc = (pred_binary==true).mean()\n",
    "\n",
    "\n",
    "    print(\"precision\",precision[1],\"recall\", recall[1], \"fscore\",fscore[1], \"auroc\", auroc,\"auprc\", auprc,\"accuracy\" ,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(shuffled_X, shuffled_Y, random_state=1)\n",
    "print(\"training_features\",training_features.shape)\n",
    "print(\"testing_features\",testing_features.shape)\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.6719885773624091\n",
    "exported_pipeline = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, max_features=0.45, min_samples_leaf=7, min_samples_split=6, n_estimators=100, subsample=0.45)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(exported_pipeline, 'random_state'):\n",
    "    setattr(exported_pipeline, 'random_state', 1)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "# save the model for further use\n",
    "with open(\"GradientBoostingClassifier\", 'wb') as file:\n",
    "    pickle.dump(exported_pipeline, file)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=testing_target\n",
    "pred_binary=results\n",
    "pred=exported_pipeline.predict_proba(testing_features).T\n",
    "pred=pred[1]\n",
    "GetScores(true,pred_binary,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(shuffled_X, shuffled_Y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Preprocess the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Create an SVM object\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear', random_state=0, probability=True)\n",
    "\n",
    "# Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# save the model for further use\n",
    "with open(\"SVM_model\", 'wb') as file:\n",
    "    pickle.dump(classifier, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=y_test\n",
    "pred_binary=y_pred\n",
    "pred=classifier.predict_proba(X_test).T\n",
    "pred=pred[1]\n",
    "GetScores(true,pred_binary,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred_bin = rfc.predict(X_test)\n",
    "y_pred_frac= rfc.predict_proba(X_test).T[1]\n",
    "GetScores(y_test,y_pred_bin,y_pred_frac)\n",
    "# save the model for further use\n",
    "with open(\"Random Forest Classifier\", 'wb') as file:\n",
    "    pickle.dump(rfc, file)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad_venv_2",
   "language": "python",
   "name": "ad_venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
